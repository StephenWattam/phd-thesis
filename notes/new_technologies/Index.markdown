New Technologies
================
Lit review notes & structure.

Structure
=========


New Sources of Text
-------------------
Cover the gradual digitisation of documents that would normally require heavy processing to acquire, and the implication of this.  Though many sources will be easier to integrate with a corpus, many more are not designed for automated processing.  Further to this, digitisation has led to a tighter, more explicit focus on intellectual property rights and re-use, including the use of DRM to block access in some instances.

### Documents of Digital Origin
With the rise of the paperless office (ha!), even documents typically accessed in physical form are authored and stored digitally.  This has now extended to almost every form of textual information *(with some notable exceptions, of course, like all my damn notes).

This has two main advantages: firstly there is greater coverage of conventional formats such as books.  Secondly there are entirely new opportunities to sample and meaningfully process things that have never been available before, like flyers, video with overlays, etc.

It's also worth linking this section to multi-modal corpora.



### Life-Logging
With the availability of portable devices (and Google Glass), life-logging techniques that have conventionally been restricted to only a few individuals worldwide are available for use by people in many contexts.  

These techniques offer much value in providing data about one's own life, however, they also may be used to provide a new perspective on sampling.  One's own logged history may be a useful source of data for systems that interact using NLP techniques (in order to mimic one's dialect and idiomatic language use more closely), something that is already done to a limited degree with speech recognition software.

In addition to this, techniques developed in life-logging may be applied to practical problems in sampling, such as the ethical and legal issues surrounding text transcription (in a similar way to Google's use of facial recognition algorithms to obscure identities in Maps images).

todo: stress further the ability to use stuff as auxiliary data for balancing, rather than comprising, a corpus.  link forward to personal corpus stuff.



### WaC
The item on this list for which most effort has been expended already, using the web as a source of text has been a routine part of corpus methods for a number of years now.

WaC offers a way to access a parallel world of documents covering almost all purposes and forms, and the technical implementation of the web means that most of this data is freely accessible (if only for private use).

The breadth of the data that may be found online has led to a number of efforts in creating tools for those building special-purpose corpora, as well as some attempts to create authoritative GP ones.

The vast size of the internet has also served to sidestep many of the practical issues with sampling large volumes of data, something that has fuelled the big data movement.  [mention Princeton?'s colossal web corpus that's entirely illegal :-)]




New Sampling Opportunities
--------------------------
Miniaturisation of technology, and increases in computing power, gradually unveils new opportunities to sample data.  At one level, this may make possible voice recognition and transcription of multiple subjects, or analysis of data in-place, without transcription.  At another, computationally-expensive techniques such as MCMC become increasingly applicable to samples large enough to be used for linguistic purposes.


### Population Access
The ubiquity of technology also offers opportunities in accessing a diverse population of language speakers with little concern for geographic limitations. Â Submissions of textual data may quickly be acquired via the internet, and populations of people otherwise unrepresented in corpora may be sampled this way.

[This has also had other effects, such as the tendency for a single conversation online to include speakers from many countries, cultures and demographics (often without even being aware of that fact).  Also, certain subcultures use various specialised language forms online that can squirrel off into their own little thing with little reference back to 'everyday' forms.]

[todo: mention AMT!]


### Indexing and Access
The improved power and utilisation of large-scale computing machinery opens up possibilities for more complex examination and extraction of data from existing lists.  A prime example of this is Google, which has become a source all of its own for many WaC researchers [sooo many citations..]

Once data is acquired, data warehousing techniques open possibilities for examination using many more covariates than has previously been possible, allowing very complex research questions to construct meaningful subcorpora with relatively little effort or time overhead.


### Big Data
Due in part to the above items [iew], 'big data' techniques allow us to extend sample sizes and significantly improve upon representations of smaller and more select sub-populations.  This may be used either to increase the specificity of findings, analyse existing features with increased confidence, or perform statistically-defensible analyses of increasingly complex linguistic features.

(mention biber's "only the weird bits are interesting" point, find some big data introductory refs)
As we have seen in previous chapters, conventional corpus building efforts centre around linguistic variables, and rely largely on expert opinion to balance their socioeconomic variables.  This approach was initially selected in order to avoid certain practical problems (many of them caused by technological limitations), though it has also caused others, most notably the difficulty in retrieving metadata about texts post-hoc.

Often, the only way to ensure demographic balance is to rely on sources of auxiliary data such as bestseller lists and library loan records that index textual variables by socioeconomic ones.  This process of using `proxy' variables is particularly opaque, and often limits the metadata available to that in the list used for selection.

The design used for this chapter's census is instead based on observing `linguistic events'---informally-demarked single uses of language---in an ad-hoc manner.  This allows the context to be recorded and metadata to be captured without retrospection.  In order to accomplish this, detailed logs were kept on everyday activities.

This approach is far closer to that used in some special purpose corpora, especially where the restricted domain allows use of automated recording tools or sampling from an existing rich database (such as from an online forum).  Sampling in this way offers guaranteed external validity for linguistic proportions, but may be considered just a single data point from the population of language users and cannot be formally generalised to other people.  The purpose of this case study is in part to explore the methods that may be used in creating such samples.

Of particular interest are variables that are particularly challenging to sample:

\begin{itemize}
    \item The age of a text when `used';
    \item Other temporal information, such as the times and days when texts are used;
    \item The social context of text use;
    \item Attention paid to a text, and which portions were read;
    \item Proportions of text types used, especially representation of types that may be missing from other corpora such as greetings, billboards, product labels.
\end{itemize}

The ad-hoc sampling approach, applied to \textsl{all} texts, also allows inspection of the proportions of language types used---something that is estimated for general purpose corpora.  Though the scope of this case study is necessarily limited, a wider `language use census' for many people would be a robust empirical method for validating claims of representativeness in general purpose corpora.


A further advantage of this method is that the population may be rigorously defined, as data on the participants is available to whatever standard deemed necessary.  This is notably at odds with the use of existing lists or repositories, which have been constructed with differing purposes and levels of documentation.






\subsection{Difficulties and Disadvantages}
Sampling language use ad-hoc involves changes in method that are practically challenging.  To build a true general-purpose corpus, one would have to take data from enough people to cover the population required, and each would have to undertake a fairly intrusive procedure to do so.

Difficulties encountered when trying to sample large populations are well documented in the social sciences, and many techniques exist to mitigate common biases (such as weighted sampling\cite{kalton1983introduction}), however, these are largely beyond the scope of this chapter.  It is notable that one solution in sociology has been to share data in a style similar to corpus linguistics, to minimise the costs involved in executing a high quality survey\cite{dataarchive2015}.

The increased `focus on the person' of a primarily social design also raises a number of ethical issues, as it is increasingly possible to derive information not just about a general group's language preferences, but about an individual's (or a comparison between groups).  This is the inverse of its value, but it is nonetheless worth considering as many study designs in linguistics and NLP need not work with such sensitive data.  This issue is addressed after the discussion of methods and findings in Section~\ref{sec:personal:discussion:ethics}.

Further, sampling text as it is used raises methodological challenges---how can we be sure that the language observed is still naturally occurring and valid?  All sampling is going to compromise on this, and the extent to which one values detail over interruption will vary by study design.  The aims of this chapter are, in part, to identify major challenges in this area, for example, which text types are most difficult to sample or require most time to record.



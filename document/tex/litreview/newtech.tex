

\subsection{New Sources of Text}
Cover the gradual digitisation of documents that would normally require heavy processing to acquire, and the implication of this.  Though many sources will be easier to integrate with a corpus, many more are not designed for automated processing.  Further to this, digitisation has led to a tighter, more explicit focus on intellectual property rights and re-use, including the use of DRM to block access in some instances.

\subsubsection{Documents of Digital Origin}
With the rise of the paperless office (ha!), even documents typically accessed in physical form are authored and stored digitally.  This has now extended to almost every form of textual information *(with some notable exceptions, of course, like all my damn notes).

This has two main advantages: firstly there is greater coverage of conventional formats such as books.  Secondly there are entirely new opportunities to sample and meaningfully process things that have never been available before, like flyers, video with overlays, etc.

It's also worth linking this section to multi-modal corpora.



\subsubsection{Life-Logging}
Life-logging is an activity that is focused around gathering, organising, and using a continuous record of the data encountered in everyday life.  It has been developed with two main focuses, both of which may lend value to the process of corpus building and sampling:

\begin{itemize}
    \item Entertainment---Many people have, since the mid 1990s, broadcasted significant portions of their life online, something that has risen in popularity to the point of spawning consumerised applications for the purpose (Justin.tv, ushare).  Methods used focus on audio-visual broadcasting, as the output is matched closely in format to reality TV.
    \item Information categorisation and extraction for personal use---This has been the main focus of the academic community (and, in one notable case, DARPA), and has spawned many projects that focus on digesting and operationalising lifelogging data.  Typically such efforts are less focused on audio-visual data, since it is prohibitively difficult to process.
\end{itemize}


With the availability of powerful portable devices such as tablets and phones (and especially Google Glass), life-logging techniques that have conventionally been restricted to only a few individuals worldwide due to technical requirements or practical limitations are becoming increasingly viable as sources of information for many people. \todo{ugly}

These techniques offer an approach to sampling that, whilst explored by many other fields, is typically seen as expensive and involved.  One's own logged history may be a useful source of data for systems that interact using NLP techniques (in order to mimic one's dialect and idiomatic language use more closely), or the language proportions of social groups may be more accurately determined for scientific study.  The value of such personalisation is already proven in many contexts with more limited interaction methods, for example speech recognition (personalised phonetic models) and web search (Google and others' personalised results).  Further to the benefits of being able to gather real data more easily, life-logging allows us to peer further into social contexts with less disruption, yielding higher quality data.

Where language metadata is needed (rather than verbatim text), many life-logging technologies support discarding of any identifiable information on-the-fly---there are techniques for storing only irreversably scrambled audio such that the characteristics of speakers may still be identified (Voice Activity Detection in Personal Audio Recordings, Lee+Ellis)% CITE
, or devices with sufficient power can simply store summaries of the events they observe, discarding the data itself.
The decrease in ethical sensitivity associated with such measures further reduces the boundaries to wider sampling of a population, something that may be used to improve and adapt existing languages resources.


\til{By analogy, Google maps obscuring people's faces using face recognition tech}


\til{stress further the ability to use stuff as auxiliary data for balancing, rather than comprising, a corpus.  link forward to personal corpus stuff.}
\til{ Perhaps a lit review belongs here, but the structure falls too deeply to do it in a structured, logical way...}


\subsubsection{WaC}
The item on this list for which most effort has been expended already, using the web as a source of text has been a routine part of corpus methods for a number of years now.

WaC offers a way to access a parallel world of documents covering almost all purposes and forms, and the technical implementation of the web means that most of this data is freely accessible (if only for private use).

The breadth of the data that may be found online has led to a number of efforts in creating tools for those building special-purpose corpora, as well as some attempts to create authoritative GP ones.

The vast size of the internet has also served to sidestep many of the practical issues with sampling large volumes of data, something that has fuelled the big data movement.  \td{mention Princeton?'s colossal web corpus that's entirely illegal :-)}




\subsection{New Sampling Opportunities}
Miniaturisation of technology, and increases in computing power, gradually unveils new opportunities to sample data.  At one level, this may make possible voice recognition and transcription of multiple subjects, or analysis of data in-place, without transcription.  At another, computationally-expensive techniques such as MCMC become increasingly applicable to samples large enough to be used for linguistic purposes.


\subsubsection{Population Access}
The ubiquity of technology also offers opportunities in accessing a diverse popuilation of languiage speakers with little concern for geographic limitations.  Submissions of textual data may quickly be acquired via the intetnet, and populations of people otherwise unrepresented in corpora may be sampled this way.

[This has also had other effects, such as the tendency for a single conversation online to include speakers from many countries, cultures and demographics (often without even being aware of that fact).  Also, certain subcultures use various specialised language forms online that can squirrel off into their own little thing with little reference back to 'everyday' forms.]

\til{mention AMT!}


\subsubsection{Indexing and Access}
The improved power and utilisation of large-scale computing machinery opens up possibilities for more complex examination and extraction of data from existing lists.  A prime example of this is Google, which has become a source all of its own for many WaC researchers [sooo many citations..]

Once data is acquired, data warehousing techniques open possibilities for examination using many more covariates than has previously been possible, allowing very complex research questions to construct meaningful subcorpora with relatively little effort or time overhead.


\subsubsection{Big Data}
Due in part to the above items [iew], 'big data' techniques allow us to extend sample sizes and significantly improve upon representations of smaller and more select sub-populations.  This may be used either to increase the specificity of findings, analyse existing features with increased confidence, or perform statistically-defensible analyses of increasingly complex linguistic features.

\til{mention biber's "only the weird bits are interesting" point, find some big data introductory refs}






% Old as of 02-02-13
% 
% \til{New technologies have "fixed" many issues surrounding the availability of data, but introduced their own intricacies.  Things such as text to speech, life corpora and the web will be introduced here, after which the web will be focused on.  The intent is to frame web corpora in terms of their emerging nature w.r.t. other sources of data, and to state that we wish to exceed, rather than emulate, the quality of existing corpora.}
% 
% 
% \subsection{Text Sources}
% \subsubsection{Web Corpora}
% \til{This section will cover, in general, web-related concerns.  The aim is to provide a comprehensive overview of the challenges to good scientific practice and representative samples.
% 
% I envisage this section having a subsection for each issue, for example, attrition, cleaning of data, identification of documents, etc.}
% 
% \subsubsection{Life-Logging}
% Cover the possibility of using low-impact audio recording or web proxies for corpus collection.
% 
% \subsubsection{Digital Documents}
% Documents that start off and remain digital.
% 
% 
% % ---------------------------------------------------------------------------------------------------------------------
% \subsection{Sampling Methods}
% 
% \subsubsection{Access}
% Explain how this greatly improves methods of sampling through access to people.  Systems such as AMT are a boon especially to special purpose corpus builders.
% 
% \subsubsection{Indexing and Retrieval}
% Systems such as search engines (private ones or google), data warehousing and noSQL all offer advantages in retrieval that may be used to resample and retrieve data from existing corpora in an efficient and practical manner.
% 
% %CITATION: "Googleology is bad science", kilgarriff

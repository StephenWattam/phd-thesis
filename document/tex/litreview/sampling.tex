% This will be a review of more formal sampling theory, comparing it to methods for acquiring language.


Corpus building methods are largely based on sampling methods from the social sciences.  These methods are well developed, and their formal frameworks specify a number of design choices that must be made whilst designing a sample.  
These decisions largely affect the suitability of a corpus for different forms of analysis, and the frameworks they are based on may be used to motivate design of the sampling process itself.
Re-examining the original principles of sample selection allows us to use some of the formalisms developed elsewhere to inform judgements on the properties of linguistic samples.

The goal of any sample is to present a scaled-down set, containing individuals that represent all variation within the population.  Before discussing the implication of various approaches, it's therefore important to draw a distinction between variables which are controlled by an experimenter (independent) and those that remain free to vary (dependent).

This distinction has a large impact on sample design, as it is impossible to draw conclusions about the population by inspecting independent variables.  Further, the selection of documents according to these controls often leads to systematic bias in dependent variables.  In order to come to conclusions about representativeness and sample quality, it is thus necessary to identify these variables ahead of time.
% It is this that leads me to stress the importance of corpus documentation

In the case of sampling documents, I will be presuming that users of corpora are primarily interested in inspecting `internal' text features, which are described in terms of `external' document metadata.  This guiding principle mirrors the metadata/data dichotomy seen in corpus tools, and should be uncontroversial\footnote{Note that many definitions of genre, text type, etc.\ make this circular, as they are defined by document content.}.  In this thesis, I will often label variables as internal or external based on these criteria, with the implication that external variables are independent, and vice-versa.

The ideal sampling scheme for a given population and selection of variables, then, contains variables as controlled specifically for the research question about which one wishes to infer, and maximises coverage of internal document content (and uncontrolled-for metadata values).

Taking this principle to extremes yields the maximally-representative census sample: 100\% of the population of interest.  At this point, any inference is mere observation, and the only potential pitfalls are ones related to whether or not the question itself is worth asking, rather than the validity of its answer.  A census still contains theory-laden assertions, however, in the form of its population definition\cite{atkins1992corpus}.


At the high level, samples may be classified into two main groups: \textsl{nonprobability}, and \textsl{probability} samples\cite{neyman1934representative}.  The former of these is primarily guided by a systematic or subjective choice, and the latter has a sampling frame defined by random selection.

% For statistical analysis, probability sampling is necessary, with the simplest case being simple random sampling (SRS).  In practice such a thing is seldom possible, and methods such as weighting and stratification may be used.
\subsection{Nonprobability Sampling}
The selection of data points in a nonprobability sample is performed either by an objective system, subjective reasoning, or some combination of the two.  Factors influencing selection are often situational or theoretical, meaning that samples require a greater understanding of the subject to avoid accidental biases.  

% Because a number of  there are often nonprobability elements that creep into larger samples.

Three main forms of nonprobability sampling are identified by Barnett\cite{barnett1991sample} and Teddlie\cite{Teddlie01012007}: \textsl{availability sampling}, \textsl{purposive sampling}, and \textsl{quota sampling}.

\subsubsection{Availability Sampling}
Also known as convenience sampling, this approach simply takes the most readily available data points.

This method has been widely used in the social sciences, with experimenters often using students from their affiliated institutions (an approach used for some very famous studies).

Convenience sampling is rightly regarded as extremely unrepresentative of wider populations, particularly in fields (such as psychology) where there are a great number of covariates.  If corpus linguists were to work with data sampled as conveniently as those in the Stanford prison experiment\footnote{``The participants were respondents to a newspaper advertisement, which asked for male volunteers to participate in a psychological study of ‘prison life’ in return for payment of \$15 per day.''}\cite{zimbardo1971stanford}, they would merely be reading books from their own bookshelves.

There are a number of methods that are designed to adjust for these biases.  Birnbaum \& Munroe\cite{birnbaum1950munroe} present a model of the bias resulting from unavailable population members after random selection.  This approach is difficult to operationalise in a linguistic context as they require enumeration of data points prior to determining whether or not they are available, rather than the more haphazard approach of true convenience sampling.

Farrokhi\cite{farrokhi2012rethinking} approaches the problem of constructing two groups: a control and treatment group, using a series of predetermined criteria to assign membership.  This approach mirrors the comparative nature of many corpus linguistic methods, but does not directly offer a way to produce a more representative corpus for `general use' aside from the principles of using heuristics to guide design.

Despite the drawbacks of convenience sampling, such an approach may be appropriate for populations where it is agreed that there is little variation between individuals for the variables of interest: for example, a study which seeks to establish the modal number of eyes humans have might be well served with a very small sample.  Smaller linguistic features such as unigram frequencies are likely to be similarly easy to represent.



\subsubsection{Purposive Sampling}
Purposive sampling describes the case where those constructing the sample make a deliberate and systematic choice of inclusion, based on expert opinion.

This approach is primarily effective against well-known sources of bias, and the validity of any sample built using it is entirely dependent on identification of these.  Where the expert design encompasses all variables of interest to a study, and where selection has been performed in such a way as to encompass individual data points from across the population, such an approach may result in a high-quality and defensible data set.

The main difficulty is in avoiding previously-unknown correlations between the theoretical selection criteria and the study variables.  As selection criteria are based on domain knowledge, and heavily theory-laden, it is often difficult to anticipate their interactions with other variables of interest.  As theories improve over time, this may also to lead to the effect of samples being considered more biased as knowledge of an area improves, gradually reducing the perceived validity of any results.

Criteria for selection generally accomplish differing goals, and will suit varying study aims\cite{advice2000study} (for example, some of these may be very useful for exploring rare features):

\begin{itemizeTitle}
    \item[Heterogeneous] An attempt to cover as much of the population as possible, by selecting data points that are unlike ones already in the sample.
    \item[Homogenous] Data points are selected to be as similar as possible according to given variables.  This is suited to in-depth qualitative analysis, being somewhat analogous to merely controlling for more variables (i.e.\ reducing the population).
    \item[Typical Case] Data points are selected according to a theoretical/rational idea of typicality.
        %This is distinct from statistical representativeness (which relies on the central limit theorem).
    \item[Extreme Case] Only those data points regarded as atypical are sampled.  May be used to explore reasons for atypicality.
    \item[Critical Case] Data points are selected based on previous theories, such that they explain certain hypotheses.
    \item[Total Population] An entire sub-population is sampled, due to its ease of access (for example, all members of an organisation, or all publications by a certain author).  If no other items are sampled, this merely becomes a census with a very tight population.
    \item[Expert] Expert opinion is used to determine inclusion in the sample.
\end{itemizeTitle}

From a corpus construction perspective, where the sampling party is often distinct from the analyst, the complexity of sample inclusion criteria brings with it the need for extensive documentation: without awareness of the expert's choices, it becomes very difficult to defend any use of the sample.  Due to the nuanced nature of their validity, purposive samples are valid only for qualitative analyses, where interactions with the study design are able to be rationalised and explained in context.  This is especially the case where a study's purposive design is borrowed for use in other contexts: for example, the Brown corpus explicitly states its aim to represent `standard' English, yet its sample design has been widely copied\cite{hundt1999manual,shastri1988kolhapur,mcenery2004lancaster}.

In fields which lack a cohesive, quantitative model of their population, it is often necessary to start from an expert-defined base.  Ideally, information from this sample may then yield methodological improvements, and, eventually, an unbiased mechanism for retrieving a statistically representative sample.
% cite: http://dissertation.laerd.com/purposive-sampling.php


\subsubsection{Quota Sampling}
This is a two-stage design, with a number of sub-populations being identified and then selected on a purposive/availability basis.  It is essentially a nonprobabilistic form of stratified sampling: the population is split into mutually exclusive groups, into which data points are placed until each group is `full'.

This approach is often used to control convenience sampling variation for some important variables, for example, controlling for sex and age whilst performing market research.  The BNC's spoken portion was `balanced' using this method, with a number of bins being allocated by context and speaker information.  The COCA corpus was also constructed using this method using data from the web~\cite[p. 163]{Davies20090601T0000001384-6655159}:

\begin{quote}
Using VB.NET (a programming interface and language), we created a script that would check our database to see what sources to query (a particular magazine, academic journal, newspaper, TV transcript, etc.) and how many words we needed from that source for a given year.
\end{quote}



\subsubsection{Summary of Nonprobability Methods}
Corpus sampling methods described above already closely resemble quota sampling, using a lot of expert opinion to define the quotas.  Nonprobability sampling, however, is particularly ill suited to statistical analysis and inference, which relies on random selection over uncontrolled variables.  Simply, nonprobability sampling techniques are very easy to bias~\cite[p.19]{barnett1991sample}:

\begin{quote}
\ldots{}there is no yardstick against which to measure `representativeness' or to assess the propriety or accuracy of estimators based on such a sampling principle.
\end{quote}

This opacity leads to limitations in corpus analysis, where the goal is to use large volumes of data as objectively as possible.  For any quantitative analysis to be scientifically defensible on empirical (rather than rational) grounds, probability sampling is \textsl{required}.




\subsection{Probability Sampling}
There are many probability-based designs available, and the choice of them is largely dependent on the methods of inquiry that are to be applied to the resulting data.  In all cases, the goal is to allow the dependent variables to vary randomly, ultimately allowing for statistical inference if sample sizes are sufficient.

Random selection of data points provides both a mechanism for unbiased estimation of parameters such as means, as well as knowledge of variation.  The latter of these is the key difference between a probability-based sample and a well-chosen nonprobability one, and is vital to basic hypothesis testing procedures.  Further, notions of sample size are entirely based on these measures: power analysis demands some knowledge of variance and effect size, both of which are only meaningful under conditions of random selection.

Random sampling methods identified by Lohr, Barnett, and Teddlie\cite{lohr2009sampling,barnett1991sample,Teddlie01012007} include: \textsl{simple random sampling}, \textsl{stratified sampling}, and \textsl{multi-stage sampling}.

\subsubsection{Simple Random Sampling}
This approach selects members of a population entirely at random.  Each member of the population has a probability of selection that is simply $\frac{sample~size}{population~size}$.

SRS is free of any errors that stem from classification, reducing the importance of potentially circular genre definitions and taxonomies.  The main disadvantage is that it requires a complete sampling frame: that is, all individuals in the population must be known in order to be randomly selected from.

Though corpora often contain randomly sampled portions, for example, where data is enumerated by a publisher's list or online directory, the lack of a central authoritative index that covers the whole population is usually an impediment to retrieval of a truly representative sample.  Essentially, simple random sampling is unsuitable for larger samples in linguistics due to this limitation.

Once a corpus has been constructed, it is often possible to use SRS in subsampling approaches.  Its unbiased properties also make it useful in bootstrapping, allowing methods to use the full distributional information contained within the corpus\cite{gries2006exploring}.  These techniques serve to avoid introduction of further bias, but don't sidestep any representativeness issues with the original sample.

% \til{examples from CL?}

\subsubsection{Stratified Sampling}
Stratification is the process of breaking a random sample into a set of bins, with sizes weighted according to some policy.  In the case that the strata are selected in an unbiased manner, this should yield the same sample as SRS.

This method can be used to ensure that subpopulations are sampled comprehensively by overestimating their proportions, or by using a different sampling method for that stratum.  (for example, selecting more people from areas with low population density).  Such practices damage representativeness but may be useful for qualitative analysis\cite{trost1986qualitative}.

Stratum selection should be along real-world subpopulation boundaries, and is usually selected in order to maximise either representativeness (by ensuring that strata are sized according to the population) or statistical power (by ensuring that strata are sized according to the amount of variance in the strata).  Stratification is most efficient when strata are homogeneous and well separated.

Stratification requires the ability to exhaustively separate the population into mutually exclusive categories, and sample from these categories in a random manner.  Both of these are a challenge in corpus linguistics, as removing the first level of `no indexing' often leads to another.  Nonetheless, where information silos exist (such as in academic publishing) then this approach is particularly suited\cite{rossi2013handbook}.

Where auxiliary data exists, statistical benchmarking may be performed by adjusting sample weights according to the proportions seen in this data.  This may be applied after sampling itself (otherwise said auxiliary data is simply used to determine the initial strata sizes), and so is often referred to as `post-stratification'.  This approach is applicable where ordinary stratification is impossible, for example, where the variables on which to stratify are unknowable at the time of sampling.


\subsubsection{Multi-stage Sampling}
Multi-stage sampling involves multiple rounds of random sampling with progressively diminishing sampling units.

Multi-stage sampling is particularly applicable in cases where \textsl{all} of the data points in the population are accessible through a hierarchical structure.  This is clearly the case for smaller linguistic features, but much less so for whole documents (or large extracts).

This approach often significantly speeds up the process of enumerating and selecting from a population, and reduces the need to classify things compared to stratified approaches.  The increased structure may also be of use to some models, allowing for multi-level modelling approaches during analysis.  Because the method relies on excluding large areas at once, it is less representative than SRS for the same sample size, and this may complicate analysis.

Cluster sampling, a form of multi-stage sampling that relies on existing organisation of data, has particular applicability to web sampling, where documents are often stored in academic repositories or simply under the same domain, or to sampling from different publishing houses.

Evert provides a library metaphor for sampling in which he explains random selection of individual words using a multi-stage approach\cite{evert2006random}, selecting progressively smaller units at random before returning a single word and repeating the process.  Though he focuses on randomness, it would be possible to use this approach with stratification at each level---this mechanism is well suited to situations where auxiliary data may be available, but not at the level of the desired sampling unit.
% http://www.tandfonline.com/doi/pdf/10.2167/eri421.0

\subsubsection{Probability Sampling Summary}
Simple random sampling is often seen as the ideal probability sampling design, in that it yields high quality results using statistical methods, without the need for weighting and adjustment of results.  It is also by far the hardest method to apply to text due to its incompatibility with the process of accessing the data.

Stratified sampling is arguably simpler, in that it allows for testing and definition of strata prior to retrieval of texts, and stratum sizes may be computed from existing corpora in a multi-stage design.  Another benefit of stratified approaches is the ability to examine and adjust the distribution of strata according to expert opinion, offering a hybrid design that is able to compensate for known practical issues.  This is often used to artificially boost the stratum sizes of minor groups within the population in order to ensure that they are over-represented in the final sample --- something that may be desirable if their influence is particularly important to a research question.



\subsection{Sample Size Estimation}
Quantitative sample size estimation is largely based on ensuring that a given hypothesis test can detect an effect of interest.  This varies according to a number of parameters, including the type of test used.  Any hypothesis test is defined by four parameters\cite{ellis2010essential}:

\begin{itemizeTitle}
    \item[Probability of Type I Error ($\alpha$)] The probability of rejecting the null hypothesis when no effect exists in the population.  Forms the threshold for the oft-quoted `p-value'.
    \item[Power ($1 - \beta$)] The probability of rejecting the null hypothesis when an effect exists in the population.
    \item[Effect Size ($d$)] The observed change in a parameter necessary to identify an effect with probability $1 - \beta$.
    \item[Sample Size ($n$)] The number of individual units in the sample that are free to vary.
\end{itemizeTitle}

Sample size must be estimated with knowledge of the power of the test to be done, and of the population distribution.  A simple binomial test may have its sample size estimated thus:

$$
n = \frac{Z^2p(1-p)}{d^2}
$$

Where $Z$ is selected according to the area under the population distribution according to $\alpha$, and $p$ is the parameter value within the population (often estimated as $\hat{p}$).  This implies that a reduction in any effect size we wish to detect increases the required sample size (and vice versa), as do increases in the confidence at which we seek to reject $H_0$ at.  This general trend applies to all sample types above.

In addition to sample size, the power of a test is also affected by the method used to perform the hypothesis test.  Tests which are excessively conservative further increase the probability of missing an effect, lowering power, as does violation of assumptions upon which parameter estimates are based.

Current methods for computing sample size are reliant on assumptions of independence, often phrased such that members of the sample must be `independent and identically distributed' (IID).  This assumption is violated massively by conventional corpus construction: corpora are sampled at an extract or document level, yet often analysed at a word or phrase level.

In the absence of IID samples, sample size calculations are reduced to what Cohen~\cite[p. 145]{cohen1977statistical} termed `non-rational bases' such as past experience or rules-of-thumb.  The process of building a corpus that is IID for a given research question is so theory-laden as to be inapplicable to those building general-purpose corpora using the current model (where many users share a large corpus for diverse tasks).

The reliance of corpus linguistics (and to some degree NLP) on qualitative methods and rational definitions of parameters such as population and sampling frame make application of existing power analysis to corpus construction challenging.

An alternative method, suited to qualitative and mixed-methods approaches (and thus less reliant on the randomness assumptions violated by document-level sampling) is presented by Glaser\cite{glaser1965qualitative}, who suggests that those performing experiments should be guided by `saturation': the point at which no (or negligible) new information is presented by adding new data points to the sample.  This mirrors the methods of Good-Turing frequency estimation\cite{GOOD01121953}, as used by Biber in his 1993 assessment of representativeness\cite{biber1993representativeness}.

Saturation-based measures offer a less-formal mechanism for assessing the adequacy of sample sizes, however, they still require definition of a study design prior to assessment of sample size sufficiency.  Such methods are also particularly lacking when sampling is nonrandom, as they rely on the discovery rate of new information being unbiased---if retrieval of documents is systematic the rate of new information will plateau, leading to premature conclusions of coverage.  Again, without the ability to enumerate the population, it is difficult to insure against such bias.




\subsection{Sample Design}
The design of a sample can be broken into a number of stages, each of which informs the next.  As we have seen, many of these require theoretical justification or analysis based on the final study design.  This forms the framework used elsewhere in the thesis as an ideal case.

% \til{This isn't cited because it's novel --- I consider it a summary.    }
% http://blog.reseapro.com/2012/11/sample-designing-steps/
% http://korbedpsych.com/R06Sample.html
%

\subsubsection{Research Question Definition}
The definition of a research question is an important first stage to selecting a sample.  This is in order to define any theoretical assumptions and the analysis design, both of which have significant impact on the size and form of any sample.

\subsubsection{Sample Unit Specification}
The unit of analysis should be in agreement with the aims of the research question above, and ideally should match the sampling unit.  Where disagreement occurs, this is likely to abrogate the quality of any analysis based on IID assumptions, including power analysis for sample size estimation below.

\subsubsection{Variable Selection}
Dependent and independent variables should be specified unambiguously, and any relationships between the two should be examined from past experience and existing literature, in order to reduce unanticipated correlations causing bias.  Where the research question and intended analysis are already rigorously designed, this task should be fairly transparent to any circularity, however, this becomes more challenging as qualitative elements are included.

\subsubsection{Population Definition}
The bounds of the sampling frame must be defined in terms of the external variables to be sampled, such that any retrieval efforts can unambiguously use these.  A population definition is, in effect, definition of an independent variable which is controlled prior to sampling, and as such its relationship to dependent variables and to the original research question is similar to that of any other variable.

\subsubsection{Sample Policy}
The manner of sample must be decided based on practical issues, and on the various properties specified above.

\subsubsection{Sample Size}
Depending on the design and analysis method proposed, this may take the form of a quantitative power analysis, or a plan to implement qualitative controls during construction of the sample.  Some a-priori objective criteria should be defined, particularly when a sample is based on qualitative assessment, in order to avoid data dredging (or `dredging' of variables correlated with those to be studied).

\paragraph{}
The existence of previous studies, literature, and datasets may guide all of the above stages, and the progression from expert-opinion-based sampling to a more quantitative understanding of population variance is made possible by iterating the above: something that happens naturally as a field progresses.

It is notable that traditional general-purpose corpus construction efforts are largely unable to specify research questions ahead of time as they are necessarily retrieving data prior to the study even being conceived.  It is also the case that most studies include a significant qualitative component, in interpreting the output of quantitative models or in the form of direct reading\cite{rayson2008keysem}.
It seems likely that such properties pose the biggest challenge to the classic question ``how large should a corpus be?'', which requires more rigorous specification of these.






\subsection{Sources of Error in Corpus Construction}
The validity of a sample is based not only on its representativeness for a given question, but also how well that question may be related to reality, and what limitations are imposed by the process of retrieving data.
Corpus builders are not blind to these challenges---indeed, most literature on corpus construction devotes a large portion of its content to practical issues\cite{wynne2005developing,atkins1992corpus,EagTcwgCtypeaglespreliminary}.

% \til{from bartlett book}


The requirements discussed above are satisfiable in a number of ways, each of which will exclude and promote certain uses of the result.  Generally, threats to the validity of inquiry based upon these samples may be broken down into three areas:

\begin{itemizeTitle}
    \item[External Validity] How relevant the sample is to the population about which we wish to infer something.  These are likely to limit the generality of a study, or lead to under/overestimation of its effects.
    % (Representativeness vs. transferrability, coverage of population, size [sampling unit]) sampling design issues
    \item[Internal Validity] How much a study can rely on document annotations and data in order to draw conclusions.  Issues here are likely to cause false results.
    % (internal proportions/categorisation, stratification, covariates, comparability)
    \item[Practical Issues] Limitations on the mechanics of sampling.  These issues may cause either or both of the above.
    % that prevent gathering an ideal sample
\end{itemizeTitle}

This section identifies potential issues with corpus sampling methods.  The majority of these are practical issues for which fixes must be designed carefully and on a case-by-case basis.


\subsubsection{Distributional Issues}
This category largely describes the manner in which certain important covariates are selected for sampling, or sampled.  Language contains an unknown (but undoubtedly very large) number of possible dimensions to study and compare, and selection of features to describe variation across texts is far from simple.  Such covariates may be listed as external metadata (author age, genre) or for linguistic features (prepositions, wh-relative passives).

Atkins et al.\cite{atkins1992corpus} provide a series of recommendations for covariate selection, which has been used to guide this discussion.

The population for whom a corpus describes language use is relatively difficult to describe, as it is largely a self-referential problem.  Seemingly, one reason why corpora are not demographically representative is because their selection processes rely on lists that are compiled using data for which it is difficult to determine comparable bounds.  The BNC's policy for selecting written materials, for example, is segmented into the following sources:

% \til{Insert more detailed overview, more critical analysis thereon}
\begin{itemize}
 \item Books, selected from bestsellers, literary prices, library loans, additional texts;
 \item Periodicals and magazines (including newspapers);
 \item Other media (essays, speeches, letters, internal memoranda).
\end{itemize}

Of the lists used in the books category, they specify the following criteria~\cite[p. 9]{burnard1995users}:

\begin{quote}
Each text randomly chosen was accepted only if it fulfilled certain criteria: it had to be published by a British publisher, contain sufficient pages of text to make its incorporation worthwhile, consist mainly of written text, fall within the designated time limits, and cost less than a set price.
\end{quote}

It is often difficult, using other sources of data, to establish the details of an author's nationality, the age of a text, or the suitability of the time limits\cite{dollinger2006oh}.
% Indeed, it seems likely that these bounds are likely to vary according to which type of text is sampled, but this is a relatively minor issue which would complicate use of the corpus).
These issues are typically better addressed by more specialist corpora, which are subject to easier-to-determine bounds such as social role, context or text type\cite{kucera2002czech,przepiorkowski2008towards,kyto1993manual}.

Ambiguity in population definition has a number of direct influences on other issues of corpus validity. Selection of stratum sizes, for example, must be based on the relative proportions of language used by the given population.  If a population is defined using purely internal (linguistic) properties, this becomes a reflexive and circular task---we end up selecting proportions of language to match proportions seen in language.  Use of auxiliary data to augment sampling policies (such as social demographics taken from other, large-scale surveys) must also be matched to the population in question.

Where selection of genres is defined by linguistic content, it is necessary to ensure that the frequencies used do not have systematic correlations with features being studied.  This is impossible to automate, and must be assumed based on experience and theoretical reasoning.

%-- 

Speech corpora are often specified in a significantly less ambiguous manner (though not always with more proportional sampling). This seems to be due to the direct nature of speech sampling, which involves the person actually performing an utterance (rather than the language use itself being a persistent and concrete artefact).

This difference is identified by Leech\cite{leech2006new}, and will be inspected in greater detail below, as it is a potential major source of disagreement between construction and use. % [section about production/consumption]\td{comment more}



One source of ambiguity when defining a population is inherent `measurement error' in the classification scheme used to delineate the texts.
Efforts such as EAGLES\cite{EagTcwgCtypeaglespreliminary} identified this issue as a significant one, and much effort has since been spent on minimising the problem of classifying texts.  Nonetheless, the difficulty in identifying a widely-agreed-upon classification scheme means that this remains an important design decision.

This manifests in two ways. The former being the problem of selecting texts according to external variables about which we may wish to infer findings, such as level of education of the author, nationality, target audience of text, etc.  Many of these factors are imposed by existing structures and systems of classification which were developed for some other use (e.g.\ search engines, library lists).  The latter of these issues is selection and stratification according to externally-imposed, but textual features such as genre, text type, medium, etc.  These two issues are likely to overlap somewhat: those distinctions useful for one research question may prove ambiguous for another.

A secondary, yet related, issue is that of metadata completeness: often, texts are sourced from very different places, and come with differing levels of metadata.  Normalising and homogenising these metadata is particularly challenging, and may include a loss of resolution.

The temporal aspects of texts are a special case of this.  Diachronic corpora seek to represent a `slice' through time of language use, and this requires a representative distribution of text age across the stated population.  Much of this distribution is defined by the production/consumption balance chosen for the corpus: for a given population, how old are texts used on a day-to-day basis?  When sampling online, or sampling from historical texts, merely identifying the age of a text is also challenging.

These questions are also important when analysing historical or monitor corpora, where they must be answered in order to allow accurate sub-sampling.  Corpora such as EEBO\cite{blum2002early} include such wide temporal coverage that their use often requires identification of a historical context.







\subsubsection{Practical Issues}
Copyright is one major problem with sampling large volumes of text from any source, especially those already available for-profit from large publishers, who are acutely aware that their entire business model is based on controlling access to their intellectual property.

This is stated in the original documentation for the Brown corpus as one reason for their choice of sampling unit, and often causes `black spots' in the sampling frame of a corpus, where certain publishers are known for their unwillingness to offer material\cite{tonymceneryandrewhardie2011}.

% \til{Tony's talk at CL'11 had this mentioned but his abstract in the proceedings doesn't.}
% http://www.birmingham.ac.uk/documents/college-artslaw/corpus/conference-archives/2011/abs-5.pdf
%

As a large number of documents are designed as ephemera, these are often not to be found in archives.  As a result, out-of-date ephemera are difficult to obtain retrospectively.  Many digital ephemera are targeted at individuals, introducing ethical issues analogous to covert research.

For speech data, and written data that is not intended for public dissemination, privacy must be considered as a legal and ethical issue.  Any auxiliary data is unlikely to suggest proportions for documents used within a private context (e.g.\ notes left on a fridge), or those controlled by formal social structures (such as documents used in many offices).

Some topics and contexts that ought to be represented are difficult to sample due to the cultural taboo surrounding them. This is especially relevant for speech corpora, since speech is often used for informal transactions, and because it is difficult to separate the sampling procedure from the speaker himself.

These issues are particularly pertinent to verbatim recordings in natural settings, such as those used in many spoken corpora.  Such recordings, even if taken in public places, may be subject to laws such as the Human Rights Act 1998 which, through guarantees of privacy, may restrict the release of such data to those not present at the time.

Such ethical pressures pose a challenge to unbiased sampling, and one that cannot easily be worked around.  The issues surrounding covert research are discussed in more detail in Chapter~\ref{sec:personal}, Section~\ref{sec:personal:discussion:ethics}.

Finally, though this is increasingly simple as the world moves towards digital storage, transcription and physical acquisition of data is often slow, difficult, and (thus) expensive.

Paper documents must be scanned, digitised, manually corrected and converted into a normal form. Speech must be gathered in an ethically defensible manner, and metadata about the speakers must be gathered.  Electronic documents require significant format conversion.

These problems are easing: digitisation technology, especially that used for paper documents, has recently progressed significantly due to efforts to preserve historical documents, and libraries' attempts to digitise older publications. In some cases, scanners are capable of scanning entire books without intervention.  Such advances are not without their own challenges, and significant further work is needed to reduce error rates in line with human transcription\cite{ameliajoulainjay2015,alex2012digitised,6065393}.%,Evershed2014CNO25951882595200}.








\subsubsection{Stratification}
Though a problem for categorisation and taxonomy selection, the `fuzzy edges' of genre, medium, popularity and other covariates often leads to ambiguity surrounding which category a text should belong to.

Some classification schemes apply multiple labels to a text in order to avoid this\cite{sharoffs2015}.  This approach may provide a way of producing a more accurate overall classification, but complicates many analyses, particularly quantitative ones relying on regression.

Biber and others\cite{leech2006new,biber1993representativeness} recommend that sampling should occur in an iterative process, with the contents of a corpus being used as evidence to weight selection of strata from the next version.

As Varadi\cite{varadi2000corpus,varadi2001linguistic} notes, this simply doesn't happen.  Reasons for this may include the shift in classification priorities, and the time required to re-code and align a previous corpus' annotation structure to that which is to be built. 

% Having acquired a first iteration of a corpus, there are many methods for assessing the 'completeness' of a sample.  some of these are explored by Biber, and others (including Evert, Greis) go on to describe further internal measures\cite{evert2004simple}.

% Note that this is quite aside from the problem of assessing corpus proportions using external variables, which is easier to inform using other studies in a multi-phase fashion (using demographic proportions, for example).

Auxiliary data from social surveys may be used as a rough indicator for this, though in reality no data source exists that can describe, in social terms, the `types' of language used (w.r.t.\ the primary dimensions of sampling for corpora).  Questionnaires, ethnography, and/or direct sampling may be the only ways to establish a ground truth for this, but for now it remains an open research question.% [until the personal corpus section ;-)]





% This problem can be seen two ways, depending on the sampling policy.  In an ideal world, strata are entirely proportional, and the problem of identifying the minimum size is one of ensuring that all variability within the smallest stratum is adequately represented.  In practice, one of the reasons for stratified sampling is to, with philosophical rather than mathematical justification, boost the prevalence of a stratum by artificially oversampling it.

% Both approaches are correct from their respective viewpoint, however, the deliberate oversampling seen in corpus construction is often informal.  Algorithms for internal saturation/sufficient sample size may be used for both the formar and latter, though it's worth noting that the sample is less random if used in the latter way.  The highly Zipfian distribution of language will, for most practical purpose, necessitate the use of inflated stratum sizes for small strata.



\subsubsection{Randomness}
The pragmatic issues surrounding corpora do not apply equally to all genres, media, social demographics, or settings.  This results in the need to apply large qualitative corrections to the sample design, which restricts the ability to use random sample designs.

A prime example of this is the proportions of written and spoken texts in many corpora, and even the proportion of elicited spoken vs. `natural' spoken texts, due to the difficulty in obtaining consent.  Another example may be seen in the BNC's proportion of academic or newspaper texts, both of which comprise a very large proportion of the corpus, yet are read by a relatively small proportion of the population.

% (i.e. web crawls)
In many cases, such as web crawling, nonrandom sampling strategies are used due to the lack of an authoritative (or reliable) central index.  In others, such strategies may be the result of other practical issues, such as the location of researchers or legal concerns surrounding certain contexts.

It's possible to augment and correct for a lot of the problems that are introduced through nonrandom sampling, for example, Schafer and Bildhauer do this in their web-scraping corpus building tools, which attempt to stratify their samples by top-level-domain \textsl{after} selection\cite{schafer2014focused}.

Nonrandom sampling is an unavoidable truth of corpus building in almost all contexts, and with care should not unduly influence the result of a corpus building effort: after all, most fields face similar practical issues.
%With proper adjustment, this problem is to be of equal or lesser magnitude to that of omitting some variables from stratification efforts.

Sample size calculations are a complex topic, particularly where the variance for a given population is difficult to determine.  Due to limitations in power analysis methods for mixed-methods designs, many of the sample size judgements required during corpus construction will necessarily include some expert opinion.

LNRE models offer possibly the most advanced generative method for this\cite{baayen1998sample,evert2007zipfr}, and can be seen as a progression upon Biber's variation analyses of the early nineties.  The issues with using such models to determine corpus variability sit neatly in two categories:

\begin{enumerate}
 \item Selection of interesting variables to measure sufficiency (this, in reality, would be part of the corpus documentation: we can say it is sufficiently representative for frequency counts, grammatical analyses up to $n$ tokens, etc.\ with little risk)
 \item Decisions on how much data is enough data.  This is fairly easy to approximate with a sufficiently accurate language model, and even simple binomial approximations prove useful in judging the likely frequency of features.
\end{enumerate}

In spite of the practicality of the techniques surrounding measures of internal feature saturation, few currently do such analyses.  This is perhaps because the linguistic researcher himself must usually perform the analyses in terms of his own study criteria, rather than the corpus builder.

The problem of quantifying variation was tackled by Gries\cite{gries2006exploring}, where he presents a method for estimating confidence intervals for small-scale features, and uses these to explore the homogeneity of corpora and their internal distinctions.  Therein he focuses not on simple frequency variation, but on grammatical properties that, he claims, are more representative of the sort of inquiry made using corpora.  He starts by using the $Z-score$ centrality measure, noting the limitation of existing partitioning schemes~\cite[p.123]{gries2006exploring}:

\begin{quote}
In order to measure the homogeneity of a corpus with respect to some parameter, it is of course necessary to have divided the corpus into parts whose similarity to each other is assessed. 
\end{quote}

He then goes on to augment this method by way of bootstrapping, in order to identify regions of maximum homogeneity within a corpus in a ground-up fashion.  Such permutation testing offers a ground-up mechanism for identifying genres, at the potential cost of interpretability.




\subsubsection{Sampling / Analysis Unit Mismatch}
% i.e. For a given corpus size in words, the size in units may be too small.
Any sample is, strictly speaking, best analysed in terms of its sampling unit.  Any disagreement leads to a reduction in how free each data point within the sample is to vary\cite{kilgarriff2005language,lijffijt2014significance,paquot2009distinctive}.

Biber's initial assessments of language variation in 1988\cite{biber1988variation} examined the suitability of the $2,000$-word sampling unit by splitting each sample and comparing the relative frequency of features in each half.  He determined that, if they were the same, then the sampling unit size was sufficient to represent a given feature.  This lead to the conclusion that corpora were adequate for inspection of `small-scale grammatical features', something that seems to be borne out by the success of computational models in this area (POS tagging, etc.).

This issue is often phrased in terms of dispersion\cite{kilgarriff2005language,sparkjones1972tfidf}, i.e.\ the tendency for a feature to be represented evenly in all documents throughout the corpus.  Dispersion is something that manual inspection of concordances and corpus data renders particularly transparent, as it is often possible to see that all instances of a particular idiom are traceable to a given author, or all coverage of a certain topic is from one publication.  This is a prime example of the unit of analysis being far smaller than the sampling unit, a problem that is receiving increasing recognition.

Evert, with his library metaphor\cite{evert2006random}, describes a sampling policy for corpus linguistics that avoids this disparity.  In it, he describes randomly sampling progressively smaller units in a virtual library, moving from books through pages to sentences.%  todo: check if he stops short of words, iirc he does.

Since many statistical problems arise from the disparity between sampling and analysis units, a particularly problematic instance of this issue is the use of bag-of-words (BOW) models.  These model language without taking into account order, and as such would be best applied to a corpus sampled at the word level: any section of text beyond this is going to exhibit `clumpiness' effects that are beyond the comprehension of the model.

The prevalence of BOW models is such that many people choose to phrase their objections in terms of the accuracy of binomial models of language frequency\cite{kilgarriff2005language,evert2004simple,evert2007zipfr}.  Undoubtedly they have a point---more complex LNRE models are capable of far more useful inferences, however, it's worth noting that only a model that can integrate the linguistic influence of word 1 upon word $2,000$ in a sample will truly justify a $2,000$-word sample\footnote{This is a quantitative form of Hoey's argument that whole texts are necessary to truly describe human expectations of word use\cite{hoey2005lexical}.}.

%--

One part of the problem is caused by a fixation on word frequencies. A given corpus, $100,000$ words in length, may comprise just $50$ texts. For the purposes of many analyses involving person-person variation, it can be said that we really only have $50$ data points. Though care is often taken to sample these texts broadly, the idea of targeting a corpus in terms of its word length, rather than the number of samples, leads to extremely poor suitability for analysis of many features.

The problem of quite what sample size to choose is a trade-off: if we were to sample single sentences for our $100,000$ word corpus, we would soon require thousands of samples, each from a randomly selected and carefully-stratified source. If we wished to perform a complex analysis of narrative structure within those sentences, we'd find there is insufficient data.

Except in certain cases, there is a plateau of difficulty for sample size: sampling $2,000$ words from a book incurs very similar levels of practical obstacle than does sampling $100$. Sampling units should be chosen in accordance with the complexity required by researchers of the time---for example, those working in information retrieval and NLP fields will demand relatively large sample units by comparison to many researchers in linguistics.

Corpora should, ideally, be defined in terms of the number of \textsl{datapoints}, rather than words.  This is one area where compatibility with existing corpora and techniques is arguably damaging to the final result, and where documentation should be clearer in guiding valid use.















% -----------------------------------------------------------------
\subsection{Validity Concerns in Corpus Analysis}
One of the primary reasons for using quantitative methods in research is their objectivity and empirical basis.  This is especially the case in linguistics, which seeks to generalise about a social property that is difficult to quantify or relate to other users.

In addition to the above procedural concerns, manual inspection and summarisation of corpora (or collections of features extracted by corpora) often leads to situations where errors of human judgement may imply certain findings.  These cognitive biases are widely recognised in many cases, and have been identified in other fields as common causes of error\cite{jain2012does,lieberman2009type}.

Many of these biases are fairly minor, and scientific methods are designed to counter-act their effect.  Nonetheless, qualitative analysis, poor corpus design, and presentation of certain features once extracted for inspection, can introduce their effects.


% \til{Until I decide to keep this section, here's an informal list:}

\begin{itemizeTitle}
    \item[Insensitivity to Sample Size] People are liable to underestimate variation in small samples.  Manual inspection of particularly intricate features extracted due to their grammatical form, especially from subsamples of the corpus (such as the spoken part) are likely to qualitatively imply false results\cite{rabin2000inference}.

    \item[Clustering Illusion] A tendency to find patterns where they are none (aphophenia).  This is more of an effect when seeing larger data sets, such as when inspecting frequency lists or concordances.

    \item[Prosecutor's Fallacy] Assuming conditional probabilities indicate unconditional ones, and vice-versa.  This might be less prevalent, but when conditioning on a certain feature, subsample of the corpus, etc.\ it is common to assume that the trends identified are indicative (or anti-indicative) of similar trends in the rest of the corpus.

    \item[Texas Sharpshooter Fallacy (post-hoc theorising)] Testing hypotheses on the data they were derived from, i.e.\ inspection then derivation then testing.  Also called `data dredging', this is a risk of large-scale community reuse of corpora.  A solution to this is to reuse sample \textsl{designs}, but not the actual data itself.

    \item[Availability Heuristic] The tendency for people to mentally overestimate the probability of features which they can immediately recall examples for\cite{tversky1973availability,schafer2014focused}.  This effect is likely to occur when examining corpora qualitatively, particularly if features are inspected in response to searches on particular features.
\end{itemizeTitle}

Many of these concerns are difficult to address without fully quantifying or automating analysis stages that are currently performed manually, something that is often a technical challenge.  Though reduction of their incidence is the goal of a corpus builder, there is often little that can be done directly prior to analysis.

The quantitative stages of corpus analysis are also not free from statistical challenge.  Many of these issues surround the use of models making flawed assumptions about variability\cite{kilgarriff2005language}, something that is a direct result of disagreements between sampling unit and analysis unit.

One approach to this is advocated by Wallis\cite{wallis2013vexed}, who proposes using a baseline linguistic feature as a control.  This serves to normalise the frequency against which one is comparing during statistical tests, but remains subject to issues of low power due to use of samples that contain too few independent sources.

Use of better-informed linguistic models in order to take account of non-orthogonal variation in linguistic features is also possible.  One method of doing this is to adjust for (or at least recognise qualitatively) the dispersion of features across samples\cite{gries2008dispersions,gries2010dispersions}.  Research is needed in order to best use these measures for qualitative interpretation (dispersion is already displayed in some popular tools\cite{anthony2011antconc,rayson2008wmatrix}) and to adjust existing statistical procedures.




% \til{Add refs to those points made in CL'15's stats thingy}








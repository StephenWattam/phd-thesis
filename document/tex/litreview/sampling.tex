% This will be a review of more formal sampling theory, comparing it to methods for acquiring language.


Corpus building methods are largely based on sampling methods from the social sciences.  These methods are well developed, and their formal frameworks specify a number of design choices that must be made whilst designing a sample.  These decisions largely affect the suitability of a corpus for different forms of analysis, and the frameworks they are based on may be used to motivate design of the sampling process itself.

The goal of any sample is to present a scaled-down population, containing individuals that represent all variation within the population.  Before discussing the implication of various approaches, it's therefore important to draw a distinction between variables which are controlled by an experimentor (indendent) and those that remain free to vary (dependent).

This distinction has a large impact on sample design, as it is impossible to draw conclusions about the population by inspecting independent variables.  Further, the selection of documents according to these controls often leads to systematic bias in dependent variables.  In order to come to conclusions about representativeness and sample quality, it is thus necessary to identify these variables ahead of time.
% It is this that leads me to stress the importance of corpus documentation

In the case of sampling documents, I will be presuming that users of corpora are primarily interested in inspecting `internal' text features, which are described in terms of `external' document metadata.  This guiding principle mirrors the metadata/data dichotomy seen in corpus tools, and should be uncontroversial\footnote{Note that many definitions of genre, text type, etc. make this circular, as they are defined by document content.}.  I this thesis, I will often label variables as internal or external based on these criteria, with the implication that external variables are independent.

The ideal sampling scheme for a given population and selection of variables, then, contains variables as controlled specifically for the research question about which one wishes to infer, and maximises coverage of internal document content (and uncontrolled-for metadata values).

Taking this principle to extremes yields the maximally-representative census sample: 100\% of the population of interest.  At this point, any inference is mere observation, and the only potential pitfalls are ones related to whether or not the question itself is worth asking, rather than the validity of its answer.  A census still contains theory-laden assertions, however, in the form of its population definition.


At the high level, samples may be classified into two main groups: \textsl{probability}, and \textsl{nonprobability} samples.  The former of these has a sampling frame defined by random selection, and the latter is primarily guided by a systematic or subjective choice.

% For statistical analysis, probability sampling is necessary, with the simplest case being simple random sampling (SRS).  In practice such a thing is seldom possible, and methods such as weighting and stratification may be used.
\subsection{Nonprobability Sampling}
The selection of data points in a nonprobability sample is performed either by an objective system, subjective reasoning, or some combination of the two.  Factors influencing selection are often situational or theoretical, meaning that samples require a greater understanding of the subject to avoid accidental biases.  

% Because a number of  there are often nonprobability elements that creep into larger samples.

Three main forms of nonprobability sampling are identified by Barnett\cite{barnett1991sample} and Teddlie\cite{Teddlie01012007}:

\subsubsection{Availability Sampling}
Also known as convenience sampling, this approach simply takes the most readily available data points.

This method has been widely used in the social sciences, with experimenters often using students from their affiliated instutitions (an approach used for some very famous studies\cite{zimbardo1971stanford}.

Convenience sampling is rightly regarded as extremely unrepresentative of wider populations, particularly in fields (such as psychology) where there are a great number of covariates.  If corpus linguists were to work with data sampled as conveniently as those in the Stanford prison experiment, they would merely be reading books from their own bookshelves.

There are a number of methods that are designed to adjust for these biases.  

Birnbaum \& Munroe\cite{birnbaum1950munroe} present a model of the bias resulting from unavailable population members after random selection.  This approach is difficult to operationalise in a linguistic context as they require enumeration of data points prior to determining whether or not they are available, rather than the more haphazard approach of true convienenince sampling.

Farrokhi\cite{farrokhi2012rethinking} approaches the problem of constructing two groups: a control and treatment group, using a series of predetermined criteria to assign membership.  This approach mirrors the comparative nature of many corpus linguistic methods, but does not directly offer a way to produce a more representative corpus for `general use' aside from the principles of using heuristics to guide design.

Despite the drawbacks of conveinence sampling, such an approach may be appropriate for populations where it is agreed that there is little variation between individuals for the variables of interest: for example, a study which seeks to establish the modal number of eyes humans have might be well served with a very small sample.  Smaller linguistic features such as unigram frequencies are likely to be similarly easy to represent.



\subsubsection{Purposive Sampling}
Purposive sampling describes the case where those constructing the sample make a deliberate and systematic choice of inclusion, based on expert opinion.

This approach is primarily effective against well-known sources of bias, and the validity of any sample built using it is entirely dependent on identification of these.  Where the expert design encompasses all variables of interest to a study, and where selection has been performed in such a way as to encompass individual data points from across the population, such an approach may result in a high-quality and defensible data set.

The main difficulty is in avoiding previously-unknown correlations between the theoretical selection criteria and the study variables.  As selection criteria are based on domain knowledge, and heavily theory-laden, it is often difficult to anticipate their interactions with other variables of interest.  As theories improve over time, this may also to lead to the effect of samples being considered more biased as knowledge of an area improves, gradually reducing the perceived validity of any results.

Criteria for selection generally accomplish differing goals, and will suit varying goals of study\cite{advice2000study} (for example, some of these may be very useful for exploring rare features):

\begin{itemizeTitle}
    \item[Heterogeneous] An attempt to cover as much of the population as possible, by selecting data points that are unlike ones already in the sample.
    \item[Homogenous] Data points are selected to be as similar as possible according to given variables.  This is suited to in-depth qualitative analysis, being somewhat analogous to merely controlling for more variables (i.e. reducing the population).
    \item[Typical Case] Data points are selected according to a theoretical/rational idea of typicality.  This is distinct from statistical representativeness (which relies on the central limit theorem).
    \item[Extreme Case] Only those data points regarded as atypical are sampled.  May be used to explore reasons for atypicality.
    \item[Critical Case] Data points are selected based on previous theories, such that they explain certain hypotheses.
    \item[Total Population] An entire sub-population is sampled, due to its ease of access (for example, all members of an organisation, or all publications by a certain author).  If no other items are sampled, this merely becomes a census with a very tight population.
    \item[Expert] Expert opinion is used to determine inclusion in the sample.
\end{itemizeTitle}

From a corpus construction perspective, where the sampling body is often distinct from the analyst, the complexity of sample inclusion criteria brings with it the need for extensive documentation: without awareness of the expert's choices, it becomes very difficult to defend any use of the sample.  Due to this nuanced validity purposive samples are valid only for qualitative analyses, where interactions with the study design are able to be rationalised and explained in context.

In fields which lack a cohesive, quantitative model of their population, it is often necessary to start from an expert-defined base.  Ideally, information from this sample may then yield methodological improvements, and, eventually, an unbiased mechanism for retrieving a statistically representative sample.
% cite: http://dissertation.laerd.com/purposive-sampling.php


\subsubsection{Quota Sampling}
This is a two-stage design, with a number of sub-populations being identified and then selected on a purposive/availability basis.  It is essentially a nonprobabilistic form of stratified sampling: the population is split into mutually exclusive groups, into which data points are placed until each group is `full'.

This approach is often used to control convenience sampling variation for some important variables, for example, controlling for sex and age whilst performing market research.  The BNC's spoken portion was `balanced' using this method, with a number of bins being allocated by context and speaker information.




\subsubsection{Summary of Nonprobability Methods}
Corpus sampling methods described above already closely resemble quota sampling, using a lot of expert opinion to define the quotas.  Nonprobability sampling, however, is particularly ill suited to statistical analysis and inference, which relies on random selection over uncontrolled variables.  Simply, nonprobability sampling techniques are very easy to bias\cite[p.19]{barnett1991sample}:

\begin{quote}
...there is no yardstick against which to measure `representativeness' or to assess the propriety or accuracy of estimators based on such a sampling principle.
\end{quote}

This opacity leads to limitations in corpus analysis, where the goal is to use large volumes of data as objectively as possible.  For any quantitative analysis to be scientifically defensible on empirical (rather than rational) grounds, probability sampling is \textsl{required}.




\subsection{Probability Sampling}
There are many probability-based designs available, and the choice of them is largely dependent on the methods of inquiry that are to be applied to the resulting data.  In all cases, the goal is to allow the dependent variables to vary randomly, ultimately allowing for statistical inference if sample sizes are sufficient.

Random selection of data points provides both a mechanism for unbiased estimation of parameters such as means, as well as knowledge of variation.  The latter of these is the key difference between a probability-based sample and a well-chosen nonprobability one, and is vital to basic hypothesis testing procedures.  Further, notions of sample size are entirely based on these measures: power analysis demands some knowledge of variance and effect size, both of which are only meaningful under conditions of random selection.

Random sampling methods identified by Lohr, Barnett, and Teddlie include\cite{lohr2009sampling,barnett1991sample,Teddlie01012007}:

\subsubsection{Simple Random Sampling}
This approach selects members of a population entirely at random.  Each member of the population has a probability of selection that is simply $sample~size/population~size$.

SRS is free of any errors that stem from classification, reducing the importance of potentially circular genre definitions and taxonomies.

The main disadvantage is that it requires a complete sampling frame: that is, all individuals in the population must be known in order to be randomly selected from.  Though desirable, simple random sampling is essentially infeasable in linguistics due to this limitation.



\subsubsection{Stratified Sampling}
Stratification is the process of breaking a random sample into a set of bins, with sizes weighted according to some policy.  In the case that the strata are selected in an unbiased manner, this should yield the same sample as SRS.

This method ensures that at least one invdividual is selected from each stratum: something that may be used to improve representation for populations with highly heterogenous distributions.  This adjustment can be tweaked to allow comparison of low-incidence and high-incidence effects by deliberately increasing the proportion of the population sampled for infrequent strata (for example, selecting more people from areas with low population density).

Stratum selection should be along real-world subpopulation boundaries, and is usually selected in order to maximise representativeness (by ensuring that strata are sized according to the population) or statistical power (by ensuring that strata are sized according to the amount of variance in the population).

All stratification requires the ability to exhaustively separate the population into mutually exclusive categories, and sample from these categories in a random manner.  Both of these are a challenge in corpus linguistics, as removing the first level of `no indexing' often leads to another.  Nonetheless, where information silos exist (such as in academic publishing) then this approach is particularly suited.

Where auxiliary data exists, statistical benchmarking may be performed by adjusting sample weights according to the proportions seen in this data.  This may be applied after sampling itself (otherwise said auxiliary data is simply used to determine the initial strata sizes), and so is often referred to as `post-stratification'.  This approach is applicable where ordinary stratification is impossible, for example, where the variables on which to stratify are unknowable at the time of sampling.



\subsubsection{Multi-stage Sampling}
Multi-stage sampling involves multiple rounds of random sampling with progressively diminishing sampling units.  It is analogous to Evert's library metaphor\cite{evert2006random}.

It is particularly applicable in cases where \textsl{all} of the data points in the population are accessible through a hierarchical structure.  This is clearly the case for smaller linguistic features, but much less so for whole documents (or large extracts).

This approach often significantly speeds up the process of enumerating and selecting from a population, and reduces the need to classify things compared to stratified approaches.  The increased structure may also be of use to some models, allowing for multi-level modelling approaches during analysis.  Because the method relies on excluding large areas at once, it is less representative than SRS for the same sample size, and this may complicate analysis.

Cluster sampling, a form of multi-stage sampling that relies on existing organisation of data, has particular applicability to web sampling, where documents are often stored in academic repositories or simply under the same domain, or to sampling from different publishing houses.

\subsubsection{Probability Sampling Summary}
Simple random sampling is often seen as the ideal probability sampling design, in that it yields high quality results using statistical methods, without the need for weighting and adjustment of results.  It is also by far the hardest method to apply to text due to its incompatbility with the process of accessing the data.

Stratified sampling is arguably simpler, in that it allows for testing and definition of strata prior to retrieval of texts, and stratum sizes may be computed from existing corpora in a multi-stage design.  Another benefit of stratified approaches is the ability to examine and adjust the distribution of strata according to expert opinion, offering a hybrid design that is able to compensate for known practical issues.  This is often used to artificially boost the stratum sizes of minor groups within the population in order to ensure that they are over-represented in the final sample --- something that may be desirable if their influence is particularly important to a research question.



\subsection{Sample Size Estimation}
Questions of sample size are primarily based on presumptions of randomness, and power analysis requires the specification of a study design in order to formally establish limits.

Any hypothesis test is defined by four parameters\cite{ellis2010essential}:

\begin{itemizeTitle}
    \item[Probability of Type I Error ($\alpha$)] The probability of falsely rejecting the null hypothesis.
    \item[Power ($1 - \beta$)] The probability of correctly rejecting the null hypothesis.
    \item[Effect Size ($d$)] The observed change in a parameter necessary to identify an effect with probability $a$.
    \item[Sample Size ($n$)] The number of individual units in the sample.
\end{itemizeTitle}

These properties are related thus:

$$
n = \frac{Z^2p(1-p)}{d^2}
$$

Where $Z$ is selected according to the area under the population distribution according to $\alpha$, and $p$ is the parameter value within the population (often estimated as $\hat{p}$).  This implies that a reduction in any effect size we wish to detect increases the required sample size (and vice versa), as do increases in the confidence we wish to reject $H_0$ at.  This general trend applies to all sample types above.

Current methods for computing sample size are reliant on assumptions of independence, often phrase in that members of the sample must be `independent and identically distributed' (IID).  This assumption is violated massively by conventional corpus construction: corpora are sampled at an extract or document level, 

In the absence of IID samples, sample size calculations are reduced to what Cohen\cite[p. 145]{cohen1977statistical} termed ``non-rational bases'' such as past experience or rules-of-thumb.  The process of building a corpus that is IID for a given research question is so theory-laden as to be inapplicable to those building general-purpose corpora using the current model (where many users all share a large corpus for diverse tasks).

The reliance of corpus linguistics (and to some degree NLP) on qualitative methods and rational definitions of parameters such as population and sampling frame make application of existing power analysis to corpus construction challenging.

An alternative method, suited to qualitative and mixed-methods approaches (and thus less reliant on the randomness assumptions violated by document-level sampling) is presented by Glaser\cite{glaser1965qualitative}, who suggests that those performing experiments should be guided by `saturation': the point at which no (or negligable) new information is presented by adding new data points to the sample.  This mirrors the methods of Good-Turing frequency estimation\cite{GOOD01121953}, as used by Biber in his 1993 assessment of representativeness\cite{biber1993representativeness}.

Saturation-based measures offer a less-formal mechanism for assessing the adequacy of sample sizes, however, they still require definition of a study design prior to assessment of sample size sufficiency.  Such methods are also particularly lacking when sampling is nonrandom, as they rely on the discovery rate of new information being unbiased.  Both of these are strong assumptions in the corpus building sphere.



\sepline

\subsection{Sample Design}
The design of a sample can be broken into a number of stages, each of which informs the next.  As we have seen, many of these require theoretical justification or analysis based on the final study design.

\subsubsection{Research Question Definition}
The definition of a research question is an important first stage to selecting a sample.  This is in order to define any theoretical assumptions and the analysis design, both of which have significant impact on the size and form of any sample.

\subsubsection{Sample Unit Specification}
The unit of analysis should be in agreement with the aims of the research question above, and ideally should match the sampling unit.  Where disagreement occurs, this is likely to abrogate the quality of any analysis based on IID assumptions, including power analysis for sample size estimation below.

\subsubsection{Variable Selection}
Dependent and independent variables should be specified unambiguously, and any relationships between the two should be examined from past experience and existing literature, in order to reduce unanticipated correlations causing bias.  Where the research question and intended analysis are already rigorously designed, this task should be fairly transparent to any circularity, however, this becomes more challenging as qualitative elements are included.

\subsubsection{Population Definition}
The bounds of the sampling frame must be defined in terms of the variables to be sampled, such that any retrieval efforts can unambiguously use these.  A population definition is, in effect, definition of an independent variable which is controlled prior to sampling, and as such its relationship to dependent variables and to the original research question is similar to that of any other variable.

\subsubsection{Sample Design}
The manner of sample must be decided based on practical issues, and on the various properties specified above.

\subsubsection{Sample Size}
Depending on the design and analysis method proposed, this may take the form of a quantitative power analysis, or a plan to implement qualitative controls during construction of the sample.  Some a-priori objective criteria should be defined, particularly when a sample is based on qualitative assessment, in order to avoid data dredging (or `dredging' of variables correlated with those to be studied).

\paragraph{}
The existence of previous studies, literature, and datasets may guide all of the above stages, and the progression from expert-opinion-based sampling to a more quantitative understanding of population variance is made possible by iterating the above: something that happens naturally as a field progresses.

It is notable that traditional corpus construction efforts are largely unable to specify research questions, and that most corpus analyses are at least partially qualitative.  It seems likely that these properties pose the biggest challenge to the classic question ``how large should a corpus be'', which requires more rigorous specification of these.






\subsection{Sources of Error in Corpus Construction}
The validity of a sample is based not only on its representativeness for a given question, but also how well that question may be related to reality, and what limitations are imposed by the process of retrieving data.
Corpus builders are not blind to these challenges---indeed, most literature on corpus construction largely focus on practical issues.

% \til{from bartlett book}


The requirements discussed above are satisfiable in a number of ways, each of which will exclude and promote certain uses of the result.  Generally, threats to the validity of inquiry based upon these samples may be broken down into three areas:

\begin{itemizeTitle}
    \item[External Validity] How relevant the sample is to the population about which we wish to infer something.  These are likely to limit the generality of a study, or lead to under/overestimation of its effects.
    % (Representativeness vs. transferrability, coverage of population, size [sampling unit]) sampling design issues
    \item[Internal Validity] How much a study can rely on document annotations and data in order to draw conclusions.  Issues here are likely to cause false results.
    % (internal proportions/categorisation, stratification, covariates, comparability)
    \item[Practical Issues] Limitations on the mechanics of sampling.  These issues may cause either or both of the above.
    % that prevent gathering an ideal sample
\end{itemizeTitle}

This section identifies potential issues with corpus sampling methods.  The majority of these are practical issues for which fixes must be designed carefully and on a case-by-case basis.


% Not listed here are cognitive biases. It's worth noting that one of the main advantages of statistical and quantitative methods are their objective properties: humans are exceptionally poorly suited to reasoning about large numbers, and our ability to digest them mathematically into smaller quantities of information is crucial.% This is embodied in my original working defintion of a corpus, that it is not meant to be read by a human.  



\subsubsection{Population and Covariate Specification}
This category largely describes the manner in which certain important covariates are selected for sampling, or sampled. Though language contains an unknown and very large number of possible dimensions of variation for study, many of them are judged to be of particular importance in describing variation across texts.

Atkins et al.~\cite{atkins1992corpus} provide a series of recommendations for covariate selection, which I have used to guide this list.




\paragraph{Poor Definition of Population}
% \til{see [this](http://www.natcorp.ox.ac.uk/docs/urg/)}
The population for whom a corpus describes language use is relatively difficult to describe, as it is largely a self-referential problem. Though efforts are often made in speech corpora to balance content according to populations of language speakers, the same cannot be said for written samples, which are often selected from various indexes.

The utility of linguistics, and linguistic studies using corpus methods, is in their capacity to infer truths about a given population. Any well constructed sample should adequately represent each member of this population, and hence any study based upon the data is necessarily limited by the scope of this sample. Though there is doubtless great value in imperfect samples, much of that value is extracted by reasoning qualitatively about a `typical' user who is properly represented. Poor definition of the population in terms used within analyses renders this reasoning unreliable.

Seemingly, one reason why corpora are not specifically targeted is because their selection processes rely on lists that are compiled using data for which it is difficult to determine comparable bounds.  The BNC's policy for selecting written materials, for example, is segmented into the following sources:

% \til{Insert more detailed overview, more critical analysis thereon}
\begin{itemize}
 \item books, selected from bestsellers, literary prices, library loans, additional texts;
 \item periodicals and magazines (including newspapers);
 \item other media (essays, speeches, letters, internal memoranda).
\end{itemize}

Of the lists used in the books category, they specify the following criteria:

\begin{quote}
``Each text randomly chosen was accepted only if it fulfilled certain criteria: it had to be published by a british publisher, contain sufficient pages of text to make its incorporation worthwhile, consist mainly of written text, fall within the designated time limits, and cost less than a set price''.
\end{quote}

It is often difficult, using other sources of data, to establish the details of an author's nationality, the age of a text, or the suitability of the time limits\cite{dollinger2006oh}.
% Indeed, it seems likely that these bounds are likely to vary according to which type of text is sampled, but this is a relatively minor issue which would complicate use of the corpus).
These issues are typically better addressed by more specialist corpora, which are subject to easier-to-determine bounds such as social role, context or text type\cite{kucera2002czech,przepiorkowski2008towards,kyto1993manual}.

Note that the problem of specifying a population is quite apart from that of determining what proportion of each stratum should be included in a corpus.  The relative proportions of each of the above categories (as well as the problem of selecting proportionally from each list) are a problem of the validity of the inference made from a corpus, i.e. internal consistency.  Use of a corpus to evidence truths about a given population, where this is either unknown or different, leads to internally consistent, but externally irrelevant conclusions.

Ambiguity in population definition has a number of direct influences on other issues of corpus validity. Selection of stratum sizes, for example, must be based on the relative proportions of language used by the given population.  If a population is defined using purely internal (linguistic) properties, this becomes a reflexive and circular task---we end up selecting proportions of language to match proportions seen in language.  Use of auxiliary data to augment sampling policies (such as social demographics taken from other, large-scale surveys) must also be matched to the population in question.

Where selection of genres is defined by linguistic content, it is necessary to ensure that the frequencies used do not have systematic correlations with features being studied.  This is impossible to automate, and must be assumed based on experience and theoretical reasoning.

%-- 

Speech corpora are often specified in a significantly less ambiguous manner (though not always with more proportional sampling). This seems to be due to the direct nature of speech sampling, which involves the person actually performing an utterance (rather than the language use itself being a persistent and concrete artefact).  Essentially, this is nothing more than a paradigm shift: there is functionally no difference in sampling a work that is synchronously `performed' and received to one asynchronously.

This difference is identified by Leech~\cite{leech2006new}, and will be inspected in greater detail below, as it is a potential major source of disagreement between construction and use. % [section about production/consumption]\td{comment more}









\paragraph{Time Distribution}
The temporal properties of a corpus will vary greatly depending on other design decisions within this list: corpora that aim to sample use will necessarily contain many older texts, whereas those based on production may be stricter.
% The Brown and LOB corpora, for example, sampled texts produced in 1961 only.
Sampling online, or sampling from historical texts, merely identifying the age of a text is also challenging.

Though the concept of sampling texts in a single time frame makes perfect sense when sampling language \textit{production} only, many corpora (including, explicitly, Brown and the BNC, aim to trade-off both production and reception of texts in order to balance the zipfian popularity of many kinds of text.  For example, the vast majority of published works are seldom read, yet certain books might be read millions of times.

The differences between distribution of production and consumption of texts leads to an interesting question: for a given population, how old are texts used on a day-to-day basis?

This question, with its direct relation to the original ideals of corpus sampling, seems largely unaddressed in the literature. Some efforts have been made to incorporate linguistic change over time into corpus design, however, these do so in an abstract manner and under the assumption that synchronic sampling is fundamentally valid in order to represent language use (use here refers to the proportional mix of production and consumption).

% \til{Mention various efforts to specify this, if i can find any. Talk more about monitor copora and specifically historical/diachronic corpora.}






\paragraph{Metdata Comparability}
Efforts such as EAGLES identified this issue as a significant one, and much effort has since been spent on minimising the problem of classifying texts.  Nonetheless, the difficulty in identifying a widely-agreed-upon classification scheme means that this remains an important design decision.

This occurs in two ways. The former of these is the problem of selecting texts according to external, non-textual, variables about which we may wish to infer findings, such as level of education of the author, nationality, target audience of text, etc. This is perhaps the least agreed-upon, partially because of the focus on sampling using existing lists as proxies for proper demographic specification.

The latter of these is selection and stratification according to external, but textual features such as genre, text type, medium, etc. these are fairly well agreed upon and specified in a fair amount of detail in efforts such as EAGLES and Atkins, et al.  

A secondary, yet related, issue is that of metadata completeness: often, texts are sourced from very different places, and come with differing levels of metadata.  Normalising and homgenising these metadata is particularly challenging, and may include a loss of resolution for many texts.






\subsubsection{Practical Issues}

\paragraph{Copyright}
Copyright is one major problem with sampling large volumes of text from any source, especially those already available for-profit from large publishers, who are acutely aware that their entire business model is based on controlling access to their intellectual property.

This is stated in the original documentation for the Brown corpus as one reason for their choice of sampling unit, and often causes `black spots' in the sampling frame of a corpus, where certain publishers are known for their absolute declination in offering material.

\til{there are various quotes about this, but are any on-the-record?}

\paragraph{Lack of a central index}
The lack of a central index for a given set of documents makes random selection within strata difficult. Any indexes that do exist are highly heterogenous in purpose, form and extent. Digital indices such as those maintained by search engines are skewed by corporate forces and also only partially cover their indexed population.

Many indexes conflate the concepts of production and consumption.  Search engines are an ideal example of this, as their goal is not to return all results for a term, but to return the `most useful'.  This may actually be a desirable property if seeking to include real-world use in a sampling scheme.

% \til{Cover which lists *do* exist and relate back to the brown, BNC design (publishers' lists, libraries, etc).}



\paragraph{Time Spent Gathering Data}
Gathering data from physical resources is expensive and difficult.  

Paper documents must be scanned, digitised, manually corrected and converted into a normal form. Speech must be gathered in an ethically defensible manner, and metadata about the speakers must be gathered.  Electronic documents require significant format conversion.

Beyond these first efforts, any metadata and annotation must be added for the final form to be useful.

Digitisation technology, especially that used for paper documents, has recently progressed significantly due to efforts to preserve historical documents, and libraries' attempts to digitise older publications. In some cases, scanners are capable of scanning entire books without intervention.




\paragraph{Ethics}
For speech data, and written data that is not intended for public dissemination, issues of privacy must be considered.  Any auxiliary data is unlikely to suggest proportions for documents used within a private context (e.g. notes left on a fridge), or those controlled by formal social structures (such as documents used in many offices).

Some topics and contexts that ought to be represented are difficult to sample due to the cultural taboo surrounding them. This is especially relevant for speech corpora, since speech is often used for informal transactions, and because it is difficult to separate the sampling procedure from the speaker himself.



\paragraph{Ephemera}
Many documents encountered in everyday life are designed to be discarded. This means that many are difficult to obtain after-the-fact.  Also, the presumption that out-of-date ephemera are difficult to obtain forms a significant part of the privacy people take for granted, raising further ethical concerns (indeed, this concept is enshrined in law in germany, who have `the right to be forgotten').


\sepline


\subsubsection{Stratum Selection [external validity?]}

% \paragraph{ most covariates not considered, or considered in a haphazard manner}
% Though it is clearly impossible to stratify a sample according to all of its main covariates (especially in the case of language, where this set is both large and unknown), the selection of covariates upon which to stratify a selection is somewhat haphazard and seldom justified in terms of the research value of the corpus.

% In some cases, one part of a corpus is stratified one way (books, for example from index lists), with another part (typically speech, sampled demographically) being stratified another. This must clearly lead us to cast suspicion on the validity of any comparisons between these parts (and the relevance of subsamples from each to the original corpus documentation's statement of representativeness and purpose).

\paragraph{Lack of Mutual Exclusivity}
Though a problem for categorisation and taxonomy selection, the `fuzzy edges' of genre, medium, popularity and other covariates often leads to ambiguity surrounding which category a text should belong to.

Though this will always persist in some form, a reasonable solution is to precisely specify objective conditions for selection. These may then be inspected by a researcher in order to assess the value of the sample to their particular questions.

Some classification schemes apply multiple labels to a text in order to avoid this\td{which}.  This approach may provide a way of producing a more accurate overall classification, but complicates many analyses, particularly quantitative ones relying on regression.


\paragraph{ lack of variability/saturation analysis/multi-phase}
Biber and others\cite{leech2006new,biber1993representativeness}\td{check Leech ref} recommend that sampling should occur in an iterative process, with the contents of a corpus being used as evidence to weight selection of strata from the next version.

As Varadi~\cite{varadi2000corpus}\cite{varadi2001linguistic} notes, this simply doesn't happen.  indeed, the simpler option of using a previous corpus to guide one's stratum selection is also ignored.  Reasons for this may include the shift in classification priorities, and the time required to re-code and align a previous corpus' annotation structure to that which is to be built. 

Having acquired a first iteration of a corpus, there are many methods for assessing the 'completeness' of a sample.  some of these are explored by Biber, and others (including Evert, Greis) go on to describe further internal measures\cite{evert2004simple}.

Note that this is quite aside from the problem of assessing corpus proportions using external variables, which is easier to inform using other studies in a multi-phase fashion (using demographic proportions, for example).



\paragraph{Coverage of Population}
An issue of external validity, and distinct from the problem of specifying a population.  Having specified a population, it is necessary to establish the connection between this definition and the population actually retrieved.  Ideally this would be based on auxiliary data and large-scale studies of language use, however, in practice there is much expert opinion involved.

This is mainly a problem of ensuring that the strata selected, in whatever proportions, cover all language use in the given population.  This is distinct from the quality of each stratum, or the relevance of its weighting, and is a problem of omission: the opinions of a group of researchers (though skilled) are unlikely to fully cover language use of highly disparate social groups within a society.

Auxiliary data from social surveys may be used as a rough indicator for this, though in reality no data source exists that can describe, in social terms, the 'types' of language used (w.r.t. the primary dimensions of sampling for corpora).  Questionnaires, ethnography, and/or direct sampling may be the only ways to establish a ground truth for this, but for now it remains an open research question.% [until the personal corpus section ;-)]





% \paragraph{External demographic information}
% this is a problem of weighting strata to fit the population.  

% in the case of speech data, there is often some attention given to the deompographics of the ext producers.  in the case of written data, the popularity of items on an index is often taken as a proxy for this.  

% check: brown and lob, iirc, 'balance' their demographics by choosing equal proportions of some major variables, which is not really balancing.  the bnc may also do this.  there is simply unknown balance for the books, and it's worth noting that different people buy books vs. go to libraries, making the populations for each quite hard to compare scientifically.



\paragraph{Stratum Sizing}
This problem can be seen two ways, depending on the sampling policy.  In an ideal world, strata are entirely proportional, and the problem of identifying the minimum size is one of ensuring that all variability within the smallest stratum is adequately represented.  In practice, one of the reasons for stratified sampling is to, with philosophical rather than mathematical justification, boost the prevalence of a stratum by artificially oversampling it.

Both approaches are correct from their respective viewpoint, however, the deliberate oversampling seen in corpus construction is often informal.  Algorithms for internal saturation/sufficient sample size may be used for both the formar and latter, though it's worth noting that the sample is less random if used in the latter way.  The highly Zipfian distribution of language will, for most practical purpose, necessitate the use of inflated stratum sizes for small strata.



\subsubsection{Randomness}

% \paragraph{ most covariates not considered (see stratum selection)}
% see 'insufficient consideration of external demographic information' for a specific case of this.  

% one of the key reasons for using stratification is that it allows one to correct for the biases implicit in sampling technique which may be caused by difficult-to-predict practical concerns such as patterns of behaviour through time (a classic example being the disproportionate sampling of women through going door-to-door during working hours).  ignoring a multitude of covariates relinquishes them from this control, meaning that we should generally stratify according to any we believe to be particularly crucial to corpus integrity and purpose.

% whereas some covariates, such as genre, date of authorship, etc, are well considered in the design of corpora, many are selected in accordance with availability or legality.  this is one cause of the large disparity between spoken and written corpora in most general-purpose offerings (for example, the bnc is just [n%] spoken).  if we were to control for this proportion, analyses relating this to other important variables would be possible.

% practically speaking, the importance of variables to the 'balance' of a corpus may be seen as zipfian: a few key features, such as genre or formality, are going to heavily influence content and vary widely across the population in ways we wish to investigate, whereas most variables are going to exert only a small influence on the resultant selection of linguistic data.  much of this decision must be made in a qualitative manner, and there will always be a small population of variables we *should* have sampled for a given study.  this problem is one of degree.



\paragraph{Availability Sampling}
The pragmatic issues surrounding corpora do not apply equally to all genres, media, social demographics, or settings.  Interactions between the key variables of interest and the sampling method cause gross over-sampling of things that are easy to access.  Technically speaking, this is a composite of many other biases, though the final effect is easy to see.

A prime example of this is the proportions of written and spoken texts in many corpora, and even the proportion of elicited spoken vs. 'natural' spoken texts, due to the difficulty in obtaining consent.  Another example may be seen in the BNC's proportion of academic or newspaper texts, both of which comprise a very large proportion of the corpus, yet are read by a relatively small propotion of the population.

% [interestingly, corpus builders seem to be willing to spend umpteen hours manually coding a book, yet refuse to properly sample something if it doesn't already have a centralised index.  perhaps this is because they can hire grad students for the former ;-) ]


% \paragraph{ lack of variance/saturation analysis (see stratum selection)}
% see 'size.no methods for saturation/internal measures'


\paragraph{Nonrandom Sampling}
% (i.e. web crawls)
Though much effort is made to randomly sample where possible, some forms of corpus are particular susceptible to the siren-like call of availability sampling.

In many cases, such as web crawling, nonrandom sampling strategies are used due to the lack of an authoritative (or reliable) central index.  In others, such strategies may be the result of other practical issues, such as the location of researchers or legal concerns surrounding certain contexts.

It's possible to augment and correct for a lot of the problems that are introduced through nonrandom sampling, for example, Schafer and Bildhauer do this in their web-scraping corpus building tools, which attempt to stratify their samples by top-level-domain \textsl{after} selection\cite{schafer2014focused}.

Nonrandom sampling is an unavoidable truth of corpus building in almost all contexts, and with care should not unduly influence the result of a corpus building effort.
%With proper adjustment, this problem is to be of equal or lesser magnitude to that of omitting some variables from stratification efforts.



% \paragraph{ incomplete coverage within each stratum [use of proxy vars]}
% note that this is distinct from specifying the population, or ensuring that the strata cover the population.

% this is todo.  Stuff was written here, but it was poorly differentiated from some other sections.

% TODO this!

%ignore the blockquote below, it's a bit conflated.:
%> a prime example of this is the sampling of books from the bnc (as stated above, perhaps re-paste?).  this strategy selects from a number of different lists under requirements of the books being popular and written in a given year.  though the bnc aims to represent language use in britain synchronically, this portion of it could more accurately be described as a sample of authors' language use in that year.  it seems irrational to suppose that authors are representative of the general british-english speaking population, or even that those *reading* the works (who may or may not 'agree' with the language used within) are representative of that wider population.  indeed, if we believe the taboids, the percentage of people who read books is fairly small.  further to this, among book readers, it is important to consider that many of them will be reading classics or mixing their reading through time, something deliberately excluded by the bnc's sampling strategy.






\sepline
\subsubsection{Size}
Sample size calculations are a complex topic, particularly where the variance for a given population is difficult to determine.  Due to limitations in power analysis methods for mixed-methods designs, many of the sample size judgements required during corpus construction will necessarily include some expert opinion.


% \paragraph{ no overall size established }
% (w.r.t strata, and randomness above)

\paragraph{Internal Saturation Measures}
% (see randomness)
Within strata, efforts should be made to ensure that a corpus contains sufficient data to represent linguistic variation.  The obstacles to constructing such an internal measure are great, since they relate back to problems of establishing the limits of variation within language.

Evert's (zipfr) LNRE models offer possibly the most advanced generative method for this, and can be seen as a progression upon Biber's variation analyses of the early nineties.  The issues with using such models to determine corpus variability sit neatly in two categories:

\begin{enumerate}
 \item Selection of interesting variables to measure sufficiency (this, in reality, would be part of the corpus documentation: we can say it is sufficiently representative for frequency counts, grammatical analyses up to n tokens, etc. with little risk)
 \item Decisions on how much data is enough data.  This is fairly easy to approximate with a sufficiently accurate language model, and even simple binomial approximations prove useful in judging the likely frequency of features.
\end{enumerate}

In spite of the practicality of the techniques surrounding measures of internal feature saturation, few currently do such analyses.  This is perhaps because the linguistic researcher himself must usually perform the analyses in terms of his own study criteria, rather than the corpus builder.

\til{notably, [someone, ahrg, mentioned in greis 'exploring variability' iirc], has looked into applying these methods to corpus stuff... find+review paper.}



% \paragraph{ no multi-phase/iwp design}
% *todo: this really belongs elsewhere, since this point repeats some of the others and the discussion would suit there instead*
% perhaps due to biber's dismissal of the design, little work has been done on constructing truly proportional corpora.  biber's primary objections were phrased in statistical terms:

% \begin{quotation}
% Biber's objections
% \end{quotation}

% Yet are motivated by entirely practical considerations. Since his description of representativeness served to flatter many corpora of the time, it seems to have been widely accepted by those without particular interest in statistics. Nevertheless, some have challenged particularly this assumption.  varadi (2001) flat-out rejects his definition, stating that it is particularly unwise to re-approriate such a widely used and recognised term.  [check greis' discussion, get quote from leech's paper].

% ultimately, i am willing to entertain biber's point in relation to stratified sampling in the general case: it is often desirable and necessary to inflate the smaller strata in order to properly represent the variation within. This, however, is purely a *practical* issue, and should not be confused with the ideal case of pure random sampling.

% biber goes on to advocate a watered down (non-proportional) multi-phase design, including the figure:

% [figure from p256 biber93]

% which has been seen by many (and recommended by eagles) as a wise strategy for corpus building. Nevertheless, as is noted above, it is seldom followed even roughly.


\subsubsection{Sampling Unit}
% \til{Might be worth clarifying the two approaches to this (models vs sampling) and perhaps link in Sean Wallis' general bent of establishing a baseline.}

\subsubsection{Sampling / Analysis Unit Mismatch}
% i.e. For a given corpus size in words, the size in units may be too small.
Any sample is, strictly speaking, best analysed in terms of its sampling unit. This issue has been inspected many times, as it is particularly visible during corpus analyses.

Biber's initial assessments of language variation in 1988~\cite{biber1988variation} assessed the suitability of the $2000$-word sampling unit by splitting each sample and comparing the relative frequency of features in each half.  He determined that, if they were the same, then the sampling unit size was sufficient to represent a given feature.  This lead to the conclusion that corpora were adequate for inspection of 'small-scale grammatical features', something that seems to be bourne out by the success of corpus methods in this area (POS tagging, etc.).

This issue is often phrased in terms of dispersion, i.e. the tendency for a feature to be represented evenly in all documents throughout the corpus.  Dispersion is something that manual inspection of concordances and corpus data renders particularly transparent, as it is often possible to see that all instances of a particular idiom are tracable to a given author, or all coverage of a certain topic is from one publication.  This is a prime example of the unit of analysis being far smaller than the sampling unit, a problem that is receiving increasing recognition.

Evert, with his library metaphor~\cite{evert2006random}, describes a sampling policy for corpus linguistics that avoids this disparity.  In it, he describes randomly sampling progressively smaller units in a virtual library, moving from books through pages to sentences.%  todo: check if he stops short of words, iirc he does.

Since many statistical problems arise from the disparity between sampling and analysis units, a particularly problematic instance of this issue is the use of bag-of-words (frequency) models.  these model language without taking into account order, and as such would be best applied to a corpus sampled at the word level: any section of text beyond this is going to exhibit 'clumpiness' effects that are beyond the comprehension of the model.

The prevalence of BOW models is such that many people choose to phrase their objections in terms of the accuracy of binomial models of language frequency\cite{kilgarriff2005language,evert2004simple,evert2007zipfr}.  Undoubtedly they have a significant point---more complex LNRE models are capable of far more useful inferences, however, it's worth noting that only a model that can integrate the linguistic influence of word 1 upon word $2000$ in a sample will truly justify a $2000$-word sample\footnote{This is a quantitative form of Hoey's argument that whole texts are necessary to truly describe human expectations of word use.}.

%--

One part of the problem is caused by a fixation on word lengths. A given corpus, $100,000$ words in length, may comprise just $50$ texts. For the purposes of many analyses involving person-person variation, it can be said that we really only have $50$ data points. Though care if often taken to sample these texts broadly, the idea of targeting a corpus in terms of its word length, rather than the number of samples, leads to extremely poor suitability for analysis of many small-scale features.

The problem of quite what sample size to choose is a trade-off: if we were to sample single sentences for our $100,000$ word corpus, we would soon require thousands of samples, each from a randomly selected and carefully-stratified source. If we wish to perform a complex analysis of narrative structure within those sentences, we'd find there isn't sufficient data.

Except in certain cases, there is a plateau of difficulty for sample size: sampling $2000$ words from a book incurs very similar levels of practical obstacles than does sampling $100$. Sampling units should be chosen in accordance with the complexity required by researchers of the time---for example, those working in information retrieval and NLP fields will demand relatively large sample units by comparison to many researchers in linguistics.

Corpora should, ideally, be defined in terms of the number of datapoints, rather than words.  This is one area where compatibility with existing corpora and techniques is arguably damaging to the final result, and where documentation should be clearer in guiding valid use.



% \paragraph{conventionalised to a weird/large number}
% % (2k) (biber)
% As mentioned above, the sampling unit for general-purpose corpora is conventionalised to a figure of 2000. This figure has undergone almost no refinement since it was originally determined by the brown designers, and has (again, as mentioned above) been reinforced by biber's determination that it is sufficient to represent small-scale grammatical features.

% Though 2000 words is far from an unsuitable selection, people's targeting of word lengths, and use of word-frequency analyses, leads me to believe that it should be significantly shorter in order to maximise the utility of a corpus.



% \paragraph{heterogenous between corpora }
% (see comparison.  as mentioned there, this isn't much of an issue really)



% \paragraph{"clumpiness" of language is not considered}
% see discussion on "too large" above.  this is merely another way of phrasing it.


% \paragraph{not formally specified or justified using semantic models}
% as discussed in "too large", the sampling unit should be selected according to, ideally, the internal properties of language.  philosophically speaking, language is a set of conventionalised symbols with which we may communicate with other people---the limit beyond which one word no longer meaningfully associates with others should be the point at which we draw the sampling unit.

% given context of a document, this may be un-nervingly large.  strictly speaking, it probably encompasses the whole history of the universe (in a manner similar to bayesian priors), however, that's only if we reason ad nauseum.

% a more practical limit, at least until neurolinguistics establishes the latter in a meaningful manner, is to sample at the maximum size for which we have models of language that may practically be applied.  for many black-box nlp techniques, this means we wish to have large sampling units in order to tease apart subtle interactions.  for many manual inspections of corpora for [especially qualitative] linguistic research, we will want to select a shorter sampling unit.



% \subsubsection{comparability}

% \paragraph{sampling unit}
% see also 'sampling unit'
% the differing size of the sampling unit between corpora renders many of them incomparable.  this is a fairly minor point, since many use the brown-standard 2k-word unit, chosen randomly from the text.

% \paragraph{poor documentation of intent, sampling policy, population bounds}
% see 'population'

% in order to perform scientifically meaningful comparisons, one must be sure that the populations between corpora differ in known ways.  though it is often possible to identify corpora that claim to be comparable in this manner, and use strikingly similar selection policies due to this, the smaller undocumented assumptions made by builders often cause minor differences, which may be crucial to certain forms of inquiry.

% prime examples of this are sampling from index lists compiled by different companies, or using different cultural definitions of formality.  more with some proper examples...

% the issue with this is not that corpora are very heterogenous and likely to be incomparable at a conceptual level, but that the lack of documentation on things such as the objective bounds of strata (see stratum selection) makes reasoned comparison by a user of the corpus impossible.















% -----------------------------------------------------------------
\subsection{Validity Concerns in Corpus Analysis}
One of the primary reasons for using quantitative methods in research is their objectivity and empirical basis.  This is especially true of linguistics, which seeks to generalise about a social property that is difficult to quantify or relate to other users.

In addition to the above procedural concerns, manual inspection and summarisation of corpora (or collections of features extracted by corpora) often leads to situations where errors of human judgement may imply certain findings.  These cognitive biases are widely recognised in many cases, and have been identified in other fields as common causes of error\cite{jain2012does,lieberman2009type}.

Many of these biases are fairly minor, and scientific methods are designed to counter-act their effect.  Nonetheless, qualitative analysis, poor corpus design, and presentation of certain features once extracted for inspection, are possible cause.

% \til{Until I decide to keep this section, here's an informal list:}

\begin{itemizeTitle}
    \item[Insensitivity to Sample Size] People are liable to underestimate variation in small samples.  Manual inspection of particularly intricate features extracted due to their grammatical form, especially from subsamples of the corpus (such as the spoken part) are likely to qualitatively imply false results\cite{rabin2000inference}.

    \item[Clustering Illusion] A tendency to find patterns where theye are none (aphophenia).  This is more of an effect when seeing larger data sets, such as when inspecting frequency lists or concordances.

    \item[Prosecutor's Fallacy] Assuming conditional probabilities indicate unconditional ones, and vice-versa.  This might be less prevalent, but when conditioning on a certain feature, subsample of the corpus, etc. it is common to assume that the trends identified are indicative (or anti-indicative) of similar trends in the rest of the corpus.

    \item[Texas Sharpshooter Fallacy (post-hoc theorising)] Testing hypotheses on the data they were derived from, i.e. inspection then derivation then testing.  Also called 'data dredging', this is a risk of large-scale community reuse of corpora.  A solution to this is to reuse sample \textsl{designs}, but not the actual data itself, fetching new data each time.

    \item[Availability Heuristic] The tendency for people to mentally overestimate the probability of features which they can immediately recall examples for\cite{tversky1973availability,schafer2014focused}.  This effect is likely to occur when examining corpora qualitatively, particularly if features are returned in response to searches on particular features.
\end{itemizeTitle}

Many of these concerns are difficult to address without fully quantifying or automating analysis stages that are currently performed manually, something that is often a technical challenge.  Though reduction of their incidence is the goal of a corpus builder, there is often little that can be done directly prior to analysis.


% see notes... 
% 
% \subsubsection{Categorising Reference Corpora}
% \til{Discuss the problem of meaningfully separating parts of a corpus for subsampling and description.}
% \begin{itemize}
%     \item Comparison to other corpora
%     \item Balance, breadth and generalisability (inc. efforts to establish standards for these)
%     \item Supcorpora, special-purpose-from-single-purpose
%     \item Parallel and comparable corpora
% \end{itemize}
% \subsubsection{Dissemination}
% \til{Descriptions of how to best spread corpora and the importance of sharing.  Cover the single-sample problem, open source corpora, twitter+news redactions, etc}
% 
% 
% \subsubsection{Balance}
% \til{Critiques of attempts to balance corpora against language (genres, special purpose corpus selection) and against one another (parallel corpora, brown, ae06/be06)}
% \subsubsection{Replicability and Reliability}
% \til{Linguistic reviews on `scientific issues' that do not centre on issues of text selection.  Include Open source corpora here, if they are not to be in the web section.}
% 
% 







% \til{The review of literature begins with a discussion of the current state of affairs w.r.t. sampling in corpus linguistics: how do people go about acquiring corpora, what their motivation and rationale is, and what people are currently developing in terms of sampling methodology.  I'll attempt to include some historical rationale too here.}
% 
% \subsection{Criticism of Current Data}
% \til{Here I will go on to discuss the failings of current widely-used lexical resources from the standpoint of their common use --- the section will focus on the representativeness of studies based on corpora as a method of commenting on their quality, and it's therefore necessary to draw the distinction between}
% \begin{itemize}
% 	\item General purpose corpora, and;
% 	\item Specialist corpora (and therefore some methods people use to build them).
% \end{itemize}
% 
% 
% 
% \subsection{Current Discussions of Representativeness}
% \til{Here I'll focus on work that is in the same vein as the thesis, i.e. how can we go about improving things?}











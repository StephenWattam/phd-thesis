% This will be a review of more formal sampling theory, comparing it to methods for acquiring language.


Corpus building methods are largely based on sampling methods from the social sciences.  These methods are well developed, and their formal frameworks specify a number of design choices that must be made whilst designing a sample.  These decisions largely affect the suitability of a corpus for different forms of analysis, and the frameworks they are based on may be used to motivate design of the sampling process itself.

The goal of any sample is to represent variation in a population.  Before discussing the implication of various approaches, it's therefore important to draw a distinction between variables which are controlled by an experimentor (indendent) and those that remain free to vary (dependent).  In the case of sampling documents, I will refer to these as external and internal respsectively, simply because it is the common case to select documents based on (external) metadata and analyse (internal) linguistic content.

The ideal sampling scheme for a given population and selection of variables, then, contains variables as controlled specifically for the research question about which one wishes to infer, and maximises coverage of internal document content (and uncontrolled-for metadata values).

Taking this principle to extremes yields the maximally-representative census sample: 100\% of the population of interest.  At this point, any inference is mere observation, and the only potential pitfalls are ones related to whether or not the question itself is worth asking, rather than the validity of its answer.


At the high level, samples may be classified into two main groups: \textsl{probability}, and \textsl{nonprobability} samples.  The former of these has a sampling frame defined by random selection, and the latter is primarily guided by a systematic or subjective choice.

% For statistical analysis, probability sampling is necessary, with the simplest case being simple random sampling (SRS).  In practice such a thing is seldom possible, and methods such as weighting and stratification may be used.

Three main form of nonprobability sampling are identified in\cite{}\td{cite barnett, sample survey principles and methods}:

\begin{itemizeTitle}

    \item[Availability] This approach simply takes the most easily available data points.  This may be appropriate if very large effect sizes and homogenous populations are expected, or for exploratory purposes.

    \item[Purposive] Purposive sampling describes the case where those constructing the sample make a deliberate and systematic choice of inclusion, based on expert opinion.
        
    \item[Quota] This is a two-stage design, with a number of sub-populations being identified and then selected on a purposive/availability basis.

\end{itemizeTitle}

Corpus sampling methods described above already closely resemble quota sampling, using a lot of expert opinion to define the quotas.  Nonprobability sampling, however, is particularly ill suited to statistical analysis and inference, which relies on random selection over uncontrolled variables.  Simply, nonprobability sampling techniques are very easy to bias:

\td{cite barnett, p19}
\begin{quote}
...there is no yardstick against which to measure `representativeness' or to assess the propriety or accuracy of estimators based on such a sampling principle.
\end{quote}

This opacity leads to limitations in corpus analysis, where the goal is to use large volumes of data as objectively as possible.  For any quantitative analysis to be scientifically defensible, probability sampling is required.

There are many probability-based designs available, and the choice of them is largely dependent on the methods of inquiry that are to be applied to the resulting data.  In all cases, the goal is to allow the dependent variables to vary randomly:

\begin{itemizeTitle}

    \item[Simple Random] This approach selects members of a population entirely at random.  Each member of the population has a probability of selection that is simply $sample~size/population~size$.

    \item[Stratified] Stratification is the process of breaking a random sample into a set of bins, with sizes weighted according to some policy.  In the case that the strata are selected in an unbiased manner, this should yield the same sample as simple random sampling (SRS).

    \item[Multi-stage] Data from a first round of sampling is to guide retrieval of a final set of data points.

\end{itemizeTitle}

Simple random sampling is often seen as the ideal probability sampling design, in that it yields high quality results using statistical methods, without the need for weighting and adjustment of results.  It is also by far the hardest method to apply to text due to its incompatbility with the process of accessing the data.

Stratified sampling is arguably simpler, in that it allows for testing and definition of strata prior to retrieval of texts, and stratum sizes may be computed from existing corpora in a multi-stage design.  Another benefit of stratified approaches is the ability to examine and adjust the distribution of strata according to expert opinion, offering a hybrid design that is able to compensate for known practical issues.  This is often used to artificially boost the stratum sizes of minor groups within the population in order to ensure that they are over-represented in the final sample --- something that may be desirable if their influence is particularly important to a research question.

Cluster sampling is also worth a mention as a form of multi-stage sampling that takes advantage of existing structures such as organisations or collections of data.  This may be seen as particularly applicable to web sampling, where documents are often stored in academic repositories or simply under the same domain, or to sampling from different publishing houses.



\subsection{Sample Size Estimation}
\til{
    effect size
    zipfianness
    
    CL is too mixed method, generally
    If the sample is shit, these measures mean nothing
}

\subsection{Sample Design}
\til{

    - rq definition

    - unit
    - size 
    - variables
    - population definition
    - use of auxiliary data

}

\subsection{Sources of Error}
\til{from bartlett book}

\til{Include section from 2.1.3}











\sepline
% Below is old as of 12-07-15 
Many of the problems listed in section~\ref{sec:litreview-corpora} may be mitigated with careful sampling.  There are a great number of available sampling strategies, and the purpose of this section is to review those which are suitable for use with text data.


%- *NB: I don't wish for this to get too prescriptive.  It's intended as an overview of an idealised procedure, rather than a "how to", which will basically come later in a far more practical form*

\subsection{Population Definition}
The population covered by a corpus is likely to be defined in terms of social demographics, linguistic features, or media type.  The manner in which each of these may be estimated, enumerated, and sampled is highly variable mainly due to the availability of auxiliary data.

An accurate population definition is necessary for two purposes.  At one level, specification of this will define the bounds of external validity, guiding use of the corpus and defining the set of problems for which it is useful.  A second reason for accurate population specification lies in estimation of its size, which is necessary for power analysis and sample size calculation.

% Discuss justification of external validity, how this must be strict for use with general-purpose corpora
Ultimately, any statement of population coverage will be unique to the corpus being built.  In the case of general-purpose corpora, documentation of this population is particularly important, as the corpus is likely to be reused and possibly subsampled.  Further, the relationship each sample point within the corpus has to the overall population should be annotated, so that any resampling is defensible.

% Discuss causal theories of language, and how these must inform a choice
If sampling from linguistic features, the population items are taken from may be purely theoretical.  This initially would seem to violate the principles of corpus linguistics sampling language `as it is used', however, each example is necessarily also covered by a demographic population from which those data were taken.  It is particularly important that both are documented, especially given disagreements between theories regarding language use (for example, some researchers may consider two features related where others do not)\td{examples}.


\til{p'raps examples of how corpora do this right now, and other others do this?}

\til{mention external nature of definition, homogeniety of population (though the latter might be less important given the purpose comments in the corpus section)}



% 
% \begin{itemize}
%  \item Applications to language (not sure of any in the abstract, todo: find some.)
%  \item Applications to the web (google/yahoo estimation papers from DA bibtex)
% \end{itemize}








\subsection{Sampling Frame}
Definition of a sampling frame is a very similar problem to the population definition above, except that, where a population definition may relate to the aspects being studied, definition of a sampling frame is necessarily performed in terms of texts themselves.

One significant challenge here is that a well defined population may yield a very poorly defined sampling frame (or one which only matches under certain strong assumptions).  The difficulty of acquiring texts from many different sources means this is a particularly pertinant issue to corpus validity.

\til{say what a sampling frame is :-)}

\til{Enumeration of sampling frame, is it necessary for each sampling method?}



\subsubsection{Sampling Unit and Linguistic Dispersion}
\label{sec:sampling-unit-and-dispersion}
One of the key problems in determining an agreed-upon structure for general-purpose corpora is the selection of a sampling unit.  The sampling unit for many studies is self-evident: a cross-sectional sample of heights would, for example, be done on a person-by-person basis.  In such a case, there are obvious theoretical reasons to select the individual for study.

Linguistics is faced with a particular challenge in this regard, on a number of theoretical points that would seem to defy answers for the forseeable future:

\begin{itemize}
    \item Many studies of language require wildly different spans of text: those using lexical collocations have decided upon a 5-word limit\td{cite sinclair?}, whereas those studying sentence structure will demand more data, and some require whole documents in order to examine narrative and discourse.
    \item There is no agreement upon the human limits of perception for linguistic features, and so the above is unlikely to be answered by an authoritative model.
    \item Due to copyright concerns and the overheads associated with digitising and normalising the format of texts, sampling smaller portions of text is often easier, except that:
    \item There is an overhead associated with looking up each text, making sampling of particularly small units from many texts difficult.
\end{itemize}


These issues, and the dearth of any immediate solution thereto, has led to a situation where corpus sizes are commonly measured in words.  This, in turn, has raised some severe issues for corpus methods, since they must handle the lack of consistent inter-word variation when performing word-level variation.

The disparity between the unit of sampling (word) and the unit of analysis (often text, author, etc.) has led to much criticism aimed at statistical methods (which buy their validity from assumptions of randomness) when applied to corpus data\td{cite kilgarriff's h0 questioning paper, others}.  This criticism is well-placed, and has led to a large amount of reseach into methods that are able to model and compensate for the nonrandom (but stochastic) `dispersion' of a linguistic feature throughout a corpus\td{massive string of cite}.

This approach may be seen as a way of ensuring that the practical limitations mentioned above are respected whilst extracting maximum value from the data gathered therefrom.  This is undoubtedly a valuable endeavour for some uses of such corpora, however, it is no substitute for sampling using the correct unit to begin with.  This argument becomes particularly salient when one sees that each category within a popular corpus such as the BNC may hold only tens of texts, rendering a sample of authors, for example, low enough to damage the validity of even very unambitious study designs.

\paragraph{}
An alternative viewpoint is extolled by Evert's `library metaphor' paper, in which he posits that the source of randomness comes from sampling:

\begin{quote}
Something about getting a random word, then getting another random word etc.
\end{quote}

Though he goes on to publish a number of times on the subject of applying dispersion measures in linguistics (curiously contradicting a number of his arguments by implication), this publication serves to hint at the purist approach to sampling unit selection.

For the purposes of this thesis I shall be attempting to steer clear of the theoretical basis of sampling unit selection, except to note that the policy of applying dispersion measures after sampling is a poor substitute for appropriate initial selection of data.  As we shall see in later sections, the availability of WaC methods allows us to mitigate some of the practical barriers to ensuring measurement- and sampling-unit coordination.




\subsubsection{Auxiliary Information}
Auxiliary information may be used from a wide variety of sources to inform the specification both of a population and of a sampling frame.  The source of this information will vary depending on the focus of the corpus, however, there are some obvious sources of authoritative data:

\begin{itemize}
    \item Social surveys (for demographic, attitudinal information)
    \item Previous corpus building efforts
    \item Smaller-scale research on specific issues
\end{itemize}



\til{ More detail.  Methodological stuff.
    use of auxiliary information iteration to inform selection of texts}









% Much of this list belongs in the sampling methods bit below.
% \begin{itemize}
%     \item Subsampling with/without weights
%     \item Using auxiliary information to weight strata
%     \item artificial inflation of interesting sections of the population (modelling, stratification)
% \end{itemize}

\subsection{Sampling Method}

\til{A discussion of what each does, and where it seems to be used, split up into nonrandom:}

\til{Difficulties in random sampling due to the above problems with specifying a sampling frame}

\til{Opportunities in nonrandom sampling.  Cover nonrandom sampling used elsewhere where similar problems exist}


\subsubsection{Random}


\begin{itemize}
    \item SRS (more special purpose focus, mention use in re-sampling)
    \item Stratified (focus on this, mention IPW, use of external data to find proportions, primary dimensions of variation and link to taxonomic stuff)
    \item Multi-stage (link to above, perhaps belongs in that section)
    \item Cluster (may simplify some practical issues)
    \item Adaptive (possibly introduce later since it's a plausible solution to some issues, but not currently used.  Some parallels with the iterative method)
\end{itemize}

\subsubsection{NonRandom}

\begin{itemize}
    \item Snowball sampling (web crawlers)
    \item 'Purposive' in general (applies slightly to many things, perhaps worth mentioning but not really labouring)
\end{itemize}












\subsection{Size and Power}
\til{Perhaps go over hypothesis testing/types of error?  Is that too simple to put here?}
Methods for selecting sample sizes, (stratum sizes), sampling unit sizes [last of these is mainly linguistic, but might be worth mentioning].

\subsubsection{Population Size Estimation}
\til{Write up notes, cover various methods and assess suitability.  Cover existing efforts from SE literature (perhaps later in web stuff)}
% http://www.healthknowledge.org.uk/public-health-textbook/health-information/3a-populations/methods-population-estimation-projection
% http://www.jstor.org/discover/10.2307/3797301?uid=3738032&uid=2&uid=4&sid=21102138383343
% http://en.wikipedia.org/wiki/German_tank_problem
% http://www.cals.ncsu.edu/course/fw353/Estimate.htm


\subsection{Sample Size and Power}
The process of selecting an appropriately large sample for a given test, and ensuring that its results afford meaningful analysis, is called power analysis.  This is dependent upon four variables, each of which may be derived as a function of its bretherin:

\begin{itemizeTitle}
\item[Desired Effect Size] The minimum difference between the hypothesised value being tested and the null, that is, the smallest change we deem to be scientifically meaningful.  This may be difficult to determine when working with complex reductions of the dimensions within corpora, but should be related back to the original data in order to be interpreted anyway. \td{"The degree to which the null hypothesis is false" Cohen "Statistical power analysis for the behavioural sciences"}
\item[Probability of Type I Error] The confidence we wish to have in any result rejecting the null hypothesis, that is, the probability we will correctly reject the null hypothesis and state that there is a difference between the groups we have identified.
\item[Sample Size] The size of the sample.  Assuming it is taken without bias, this affects the amount of sampling error in the study---larger samples will exhibit less error (on top of the natural population variation).
\item[Statistical Power] The probability that a test will correctly reject $H_0$.  Like $\alpha$ above, this is conventionalised and should be chosen to fit the circumstances.
\end{itemizeTitle}

A test that is specified so as to be higher powered will be likely to detect smaller effects, to the point where it may detect effect sizes that no longer have ramifications for the real world.  Conventionally, this is warned against for practical and ethical reasons: the overheads required in gathering the data impose a practical disadvantage on the study.  This is less of an argument, however, for shared resources such as corpora, for which we may be tempted to say ``the more data the better'' if it were not for the insane complexity of larger portions of language, implying that more will never be enough for some anyway.  The question here is who those some are, and what portion of those wishing to use a corpus they comprise.  We shall revisit this conundrum later.

Where data is as dimensional as linguistic data (and formal statistics often explain very small proportions of variance), the detection of very small changes in a parameter of interest is also likely to represent spurious influence, evidencing not the alternative hypothesis presented, but instead some correlational quirk that may not be relied upon.  Philosophical purists may at this point be objecting to the implication that rejection of the null hypothesis implies acceptance of the alternate given.  They are right, of course, but in practice the distinction is respected less than it perhaps ought to be.



Underpowered tests suffer an altogether more familiar failure, in that they are unlikely to detect an effect where one actually exists.  Where this is the case it is largely impossible to determine if any observed effect is down to meaningful variation in the sample used (which should warrant further study) or simply due to the sample size being too small (which should [ideally] not).

This would seem to be of epidemic proportions in linguistics, especially where larger features are studied, with researchers often noting that an effect is interesting but statistically unconfirmable:
\begin{quote}
QUOTE STUFF FROM PEOPLE
\end{quote}
\til{Quote a load of things where people say "There's not enough data to prove it, but this effect looks interesting"}









Where a corpus is built in a special-purpose manner, the desirable sample size may be authoritatively calculated a priori, on conditions defined by the intended study design.\td{note how rarely this is done}  Where general-purpose corpora are defined, however, this is unlikely to be accurate due to the disparate study designs used after sampling is complete (see~\ref{sec:sampling-unit-and-dispersion} above).

It would seem that general-purpose corpora in their current form, as large shared pre-built repositories of text, are doomed to one of two fates: either they will be so large as to be difficult-to-process (containing enough text to satisfy power requirements of studies focusing on large units of text), or they will be incapable of offering valid insight into larger features due to insufficient sample sizes.






\subsubsection{Methods for Sample Size Calculation}

\til{Discuss methods for estimating sample size.}
\til{Debate suitability for various uses, esp. for general purpose corpora.  Focus on Zipfianness.}
\til{Cover saturation measures, 'new information' approaches}

% \begin{itemize}
%     \item Criteria for selection
%     \begin{itemize}
%         \item purpose-based models [should be big enough for x]
%         \item sufficiency/internal measures [should contain n xs]
%         \item breadth/variation [should contain n types of x]
%     \end{itemize}
%     \item Big data's approach
%     \item Size of auxiliary data for multi-stage designs
% \end{itemize}









% TODO: this section might not belong, or it might be best off later
\til{ The section below is probably best off being placed later, or at least where its focus is more well-defined}
\subsection{Post-hoc/Reweighting(/representation)}
\til{this seems to conflate the stratification stuff, restructure}
Methods for upping weights in line with auxiliary data, even of existing corpora.

\begin{itemize}
    \item Establishing weights using auxiliary data
    \begin{itemize}
        \item possible sources of valid data
        \item existing manual resampling by selection of categories (compare to above method)
    \end{itemize}
\end{itemize}












\subsection{Bias Estimation and Evaluation}
Methods for evaluating and comparing corpora

% http://en.wikipedia.org/wiki/Mean_signed_difference ?
% http://en.wikipedia.org/wiki/Mean_absolute_error

\begin{itemize}
    \item Bootstrapping
    \item Comparison to other corpora (validity, meaning, can we trust previous corpus to be a gold standard?)
    \item Comparison to humans (heuristics, summarising, "getting to know your corpus")
\end{itemize}






The field of linguistics is one concerned with description and formalisation of a particularly ethereal social concept.  The paucity of philosophical agreement upon the nature of language has led to many different approaches being taken through the years, many of which have accomplished great things in advancing our capacity to reason about, and derive conclusions from, language.


The most obvious method for inquiring about the nature of language seems to be to sample real-world use.  This process is followed in many other sciences concerned with social phenomena, and offers a tried-and-tested methodology for inferring results.  It is perhaps unsurprising, then, that this method has been used to a varying degree by many linguists throughout history.

In their 2001 book~\cite{macenery2001corpus}, McEnery and Wilson characterise the pre-Chomskian linguistic inquiry as primarily taking this form:

% page 2
\begin{quote}
The dominant methodological approach to linguistics immediately prior to Chomsky was based upon observed language use.
\end{quote}

They point to a number of studies using systematic analysis of samples to make their conclusions~\cite{kaeding1897haufigkeitsworterbuch,preyer1889mind,stern1924psychology,eaton1940semantic,west1953general}, and decribe Chomsky's influences on the field, which served to lead it to all but abandon corpus techniques in the 1950's, as a turn towards explanatory, rationalist theories of language.

These rationalist theories were often verified using experimentation or elicitation, in an effort to gather data that is detailed and reliable.  It's arguable just how valid and philosophically defensible these methods are, especially given the nature of language as a part-mental, part-tangible concept.%\td{rewrite this paragraph}

% These early corpora are of a quite different form to those of the `modern era', and will not be discussed in any great detail here except for historical context.


In a pre-digital world, collection and analysis of large-enough-to-be-useful samples of language is extremely difficult, making this rationalist approach a useful alternative.
Small samples are fundamentally unable to reveal some of the details examined by the structuralists, and construction and analysis of sufficiently large corpora in a non-digital era would prove a practical hard limit on the power of corpus studies.  To frame the focus on rationalist inquiry as an alternative to the empirical is a disservice to both: empiricism had supported the rationalist theories of the 50's, and would itself go on to be supported in turn as it once again rose to prominence.

The revival of corpus linguistics may be attributed almost solely to the availability of programmable computing machinery.  This offered a solution to the problems of scale encountered during earlier corpus analyses, making empirical data once more viable for detailed inspection of language.

This revival, often termed the `modern era' of corpus linguistics, has yielded what we would commonly call a corpus today: a large, machine-readable, annotated collection of texts sampled in order to represent some population of real-world language use.

Corpus studies are now widely relied upon across linguistics, and are often the method chosen to test theories derived from structuralist approaches.  This may be seen as a validation of their methods, as the two complementary philosophies of scientific inference are once more able to use one another without methodological suspicion.

For the purposes of this section I will be focusing on the design of `modern' corpora with respect to their use in validating linguistic theories, and for training automated Natural Language Processing (NLP) and information retrieval systems.  For this reason I will be covering mainly general purpose corpora: those that purport to represent such a large population as to cover a whole language.  This `style' of corpus is built so as to be useful to many research questions and researchers, in part to dilute practical issues surrounding sampling.  The latter type is the special-purpose corpus, which is designed to represent a restricted context.  These special-purpose corpora may be selected according to demographic or linguistic properties, and are typically much smaller.  Because of this, they are often built for a given study, or by re-sampling a general-purpose corpus.

% \til{More focus on sampling/representativeness}

% The collection of large volumes of real-world language is one method of increasing the empiricism of this process, and as such has been followed as a parallel stream of inquiry for a long time: [mention 1870's corpus studies, 1950's inquiries].  





% --

% 
% 
% 
% The use of corpora for linguistic analysis is a long one---one justification for this is that the alternative to using some kind of corpus is either to manually seek evidence for a linguistic feature (something that very easily leads to pseudoscientific, unfalsifiable theories), or to inspect the "idea" of language that a native speaker (or speakers) posesses.
% 
% Whilst, doubtless, much good work has been done using these alternatives, they lack the objective and empirical epistemological possibilities that define the scientific process.  Examination of one's idea of language is likely to be influenced by the knowledge of a linguist, and seeking evidence for a theory in a language with unknown or poorly-defined bounds is likely to yield it regardless of the reality. % when observed from other contexts
% 
% Corpus methods, then, free linguistics from the alchemy of human understanding, providing a convenient empirical truth against which we may assess hypotheses.
% 
% The value of any inferences evidenced by this empiricism is, however, limited to the extent to which a corpus is associated to the language upon which we wish to comment.  It is this requirement that has raised most objections, for the concept of language is little understood and poorly defined even where people claim to understand it.  Regardless of controversies over form, it is accepted that any corpus representative of useful portions of a language must be very large indeed (though the definition of 'large' is also debated!).
% 
% Until recently, the task of gathering, processing, and inspecting large volumes of text was arduous enough to prevent its widespread appeal: some efforts were made before the era of computerisation, %CITE
% , for example, did X etc \til{mention 1870s work, 1950's popularity}..% and there was a time when corpora were rather popular even without the benfits of computation
% 
% The introduction of programmable computing machinery, and (fast) electronic storage, opened the floodgates for easy, large-scale analysis of text.  This brought the second\td{does that mean we're on the third given its lull, or still second?} wave of corpus linguistics... \textsl{~~wavey wavey screen fades into next section like a flashback on 60's TV~~}
% 



\subsection{A Brief History of Modern Corpora}
The Brown Corpus of Standard American English~\cite{francis1961brown}% http://icame.uib.no/brown/bcm.html
is widely regarded as the first of the modern age of corpora.  Built in the 60's, Brown's corpus was the first electronic-format general purpose corpus and was roughly one megaword in size.% 1,014,312
It contains 500 samples, each roughly 2000 words in size, that were taken to represent a cross-section of works published in the United States in 1961.  The proportions and sizes of samples were selected in order to trade off pragmatic concerns with the possible kinds of analysis that could be performed at the time.

The `Standard' in its name referred to Kucera and Francis' intent that it become a regular feature across corpus linguistics---this was quickly realised, as Brown became a \textsl{de facto} standard for American English.  In order to maximise the value of comparisons within studies, other general purpose corpora chose to mirror Brown's sampling policies.

% --- 

\begin{table}[Ht]
    \centering


    \begin{tabular}{llcc}
    \hline
    Text categories & Number of texts in each category & American corpus & British corpus \\ \hline
    A & Press: reportage & 44 & 44 \\
    B & Press: editorial & 27 & 27 \\
    C & Press: reviews & 17 & 17 \\
    D & Religion & 17 & 17 \\
    E & Skills, trades, and hobbies & 36 & 38 \\
    F & Popular lore & 48 & 44 \\
    G & Belles lettres, biography, essays & 75 & 77 \\
    H & Miscellaneous & 30 & 30 \\
    J & Learned and scientific writings & 80 & 80 \\
    K & General fiction & 29 & 29 \\
    L & Mystery and detective fiction & 24 & 24 \\
    M & Science fiction & 6 & 6 \\
    N & Adventure and western fiction & 29 & 29 \\
    P & Romance and love story & 29 & 29 \\
    R & Humour & 9 & 9 \\ \hline
      & Total & 500 & 500 \\ \hline
    \end{tabular}

    \caption{The basic composition of the British and American corpora}
    \label{table:litreview:corpora:lobdist}
\end{table}


The Lancaster-Oslo-Bergen (LOB) corpus~\cite{johansson1986tagged} was built as a British counterpart to Brown.  
It uses the same stratification and sampling strategy (with one or two more texts in certain categories) and thus comprises roughly a megaword of British English, as published in 1961.  % See  http://clu.uni.no/icame/manuals/ Table 1 for a table of comparisons
Though the manual does note:

\begin{quote}
    The matching between the two corpora is in terms of the general categories only. There is obviously no one-to-one correspondence between samples, although the general arrangement of subcategories has been followed wherever possible.
\end{quote}

Table~\ref{table:litreview:corpora:lobdist} shows the proportions of texts in each genre, relative to Brown (the `American corpus'), as reproduced from the corpus manual.

% ---
% \til{ Not 100\% sure why I mention LLC here. }
% http://khnt.hit.uib.no/icame/manuals/londlund/index.htm
%  The corpus consists of 100 texts, each of 5000 words, totalling 500.000 running words of spoken British English. Information about the compilation of the corpus and explanation of the symbols (prosodic, phonetic, etc.
The London-Lund Corpus~\cite{greenbaum1990london} was released in 1990, and contains transcribed spoken text, annotated with a number of different markers to indicate intonation, timing and other extra-textual information.  
The corpus consists of $100$ texts, each of $5,000$ words, totalling $500,000$ running words of spoken British English.



% --- 
Collins' requirement for a corpus upon which to base their dictionaries spawned the COBUILD and its `representative' subset, the Bank of English~\cite{Jarvinen1994AMW991886.991985,sinclair1987looking}.  COBUILD uses a slightly different approach to corpus building: that of the monitor corpus.  Monitor corpora are continually added to, using a fixed sampling policy but an ongoing sampling process.  At the time of writing, the BoE is 650 million words in size (The whole COBUILD family used by Collins is 2.5 billion).

The approach taken by the BoE opens many possibilities for analysis of language over time (something also covered by diachronic/historical corpora using more conventional sampling).  Even so, such comparisons are complicated by the irregular additions (c.f. diachronic corpora), and this remains the only major corpus built in this fashion prior to automated web retrieval.


% ---
The de-facto standard of the day is currently the British National Corpus, which comprises $100$ million words of British English.  The BNC's design was influenced heavily by discussions on corpus building that centred around creating a standard, reliable approach to taxonomy, sampling and annotation that occurred around the early nineties.

The BNC aims at being a synchronic `snapshot' of British English in the early 1990s.  It consists of samples of text up to $45,000$ words each, and is deliberately general-purpose, containing a wide range of genres as well as a sizable spoken portion.  It was released in 1994, but has since been re-coded and augmented, particularly notably by Lee~\cite{lee2003bnc}, who constructed a significantly more detailed (and principled) taxonomy for its texts in 2003.

% 1.2 General definitions ( from http://www.natcorp.ox.ac.uk/docs/URG/BNCdes.html )
% 
% The British National Corpus is:
% a sample corpus: composed of text samples generally no longer than 45,000 words.
% a synchronic corpus: the corpus includes imaginative texts from 1960, informative texts from 1975.
% a general corpus: not specifically restricted to any particular subject field, register or genre.
% a monolingual British English corpus: it comprises text samples which are substantially the product of speakers of British English.
% a mixed corpus: it contains examples of both spoken and written language.


% ---
The rise of electronic communications has led to a reduction in the effort required to gather corpus data.  This has resulted in a great increase in the number of special-purpose corpora built for specific studies~\cite{westlabusenet2013,ferraresi2008introducing}.  These corpora are generally more focused than the larger ones built up until the 90's, but some still claim to be general purpose (or at least have a `wide remit').  

% Move? [These corpora are often less widely used, and serve to illustrate one extreme discussed below, that the purpose of a corpus is important to its construction]

This thesis is focused on sampling using automated, technical mechanisms to oversome some of the challenges facing conventional methods, and as such focuses on web-based methods (known as `Web as Corpus'.  WaC is concerned with sampling the web itself, as well as constructing samples that are representative of other data, yet are retrieved primarily from the web.

% In this thesis, I will be focusing on a specific (though popular) form of these corpora based on `web-as-corpus' (WaC) methods.  Please see Section~\ref{sec:litreview-newtech} REF for more.





















% ---------------------------------------------------
\subsection{What Makes a Corpus?}
The use of general purpose corpora as large monoliths, reused in many studies and systems, has led to much debate over the nature of a corpus.  This, as we shall see throughout the thesis, is a question unworthy of concrete answers---each purpose will exert certain demands upon corpus design criteria, and any widely-used corpus is likely to be a compromise around these.

This section exists primarily to define a corpus as used by the author: it is unlikely that the definition derived below is universal, however, it is designed to reflect most use-cases, across corpus linguistics and NLP.

\til{signpost?}
% This discussion is separated to cover a number of properties of corpora, and their parameters.  These 



In order to establish the important traits of corpora, it is wise to have an understanding of the motivation behind their existence.  Corpus methods are generally contrasted against two other methods of linguistic investigation: direct elicitation from a language speaker, and directed research into a linguistic feature under controlled conditions.  Both of these pose significant scientific challenges---both are reliant on at least one linguist's intuitive view of language (one that could hardly be said to be representative of most language users), and both require the acquisition of data without its usual context, something that is especially difficult given the varied and context-dependent complexity of language.

Corpora provide limited solutions to both of these issues.  In the former case, they provide an objective record of linguistic data that is free from all but the initial builders' influences (which, in the ideal case, may be documented and provided along with the data itself).  In the latter, they are as portable as any large volume of text, and may be annotated with context sufficient for a given linguistic task.


%------
\paragraph{}
At its most basic level a corpus is a sample of text.  This is reflected by some definitions:

\begin{quote}
A corpus is any body of text\td{cite}
\end{quote}

The definition used above is true if one presumes that any given research questions surround said body of text.  It is perhaps more useful to restrict this, and many authors choose to do so, stipulating that a corpus must be sampled with a known population:

\begin{quote}
Quote from Mcenery where they say someone's library is not a corpus\td{cite}
\end{quote}

\til{ TODO: Meyer, McEnery and Wilson definitions }

Further to this, the modern definition of a corpus has undergone a series of significant refinements thanks largely to the increase in both the ubiquity and power of computing machinery.  Corpora are, with very few exceptions, electronic (with an increasing number documenting texts of electronic origin), multi-modal (covering a wide variety of methods of communication and their linguistic features), and annotated with linguistic data.

Many authors go further by stating that a corpus should be machine-readable, annotated with information useful to linguistic inquiry, built for a specific purpose or methodology, available for use in other studies, finite in size or even stratified to provide multiple possible analysis methods with valid data.  Whilst I do not consider many of these to be requirements for a scientifically useful corpus, they may contribute greatly to corpus utility due to their alignment with common methodologies and uses.  There is undoubtedly a case for this---the utility of a corpus is often limited by its format, however, the extent to which this applies varies wildly by purpose makes it unreasonable to include many resources missing some of the above under the term ``corpus''.



This review, then, shall start with the most basic of definitions, that of `a body of text sharing some property that may be interrogated for some linguistic information'.  This takes into account the claims of generalisability that cannot be made for more haphazard collections of texts (often called \textsl{libraries} or \textsl{archives} with no relevant common features) which are not demarked by the boundaries of some notional property.









% The definition of a corpus is one which has been oft-discussed with little agreement, indeed, as the field of corpus linguistics has progresssed its definition has gradually changed to suit the methodology of the day.  The Brown Corpus of Standard American English %CITE

% Examining the reasons why linguists first chose to use corpus methods reveals a number of important traits.  The first of these should be seen as its reliability---one of the major problems with assessing linguistic features by interrogation or directed research is that is it hard to establish known bounds of variability.  This inherent `fuzziness' in the method of inquiry, coupled with the context-dependent complexity of language itself, makes replication difficult.


% ----
% Brown's unique status afforded it status as a {\sl de facto} standard, and many subsequent corpora were built with similar foci.  Though the design of corpora has moved on significantly since those first efforts, this standard persists in its principles, and Brown's influence may be felt in the building and coding efforts of many general purpose corpora, to the point that many linguists are tempted to define corpora in terms of them. % TODO: move this elsewhere, but it's a good point

% Brown is widely regarded as the first of the modern age of corpora: its scale and electronic format differentiating it from previous collections and allowing automated analysis on a scale never before practical.




\subsubsection{Representativeness and Transferability}
Representativeness is, in effect, the goal of any sample.  It is the property that I chose to use as the loose starting condition above, and it is to be maximised by selection of those properties covered below.  By virtue of this holism, it is also dependent upon enough factors to be poorly defined in the literature.
% (by virtue of the difficulty of doing so).

The concept of representativeness is based strongly on philosophies of inference, and epistemology in general.  These are highly dependent on correlation in variables external to those measured by a sample, and any rigorous definition will involve the properties we wish to generalise, the purpose of such generalisation, and the population we wish to generalise about.  Users of reference corpora (those general-purpose corpora designed to represent a whole language) may find, for example, that their claims to validity are wildly variable compared to previous studies, simply due to the nature of their research question.


Much has been written on the subject of representativeness\td{cite loads}, both in linguistics and in other fields that are dependent upon complex sources of data.  As with psychology or sociology, the opacity of the mechanisms that generate the object of study is such that significant philosophical disagreement as to the underlying nature of language occurs.  This disagreement has, in many ways, limited efforts to formalise and reason about representativeness in corpus design: taken quantitatively, one person's adequate corpus is another's woefully biased one.

The LOB manual hints at the fluid nature of ``representativeness'' in corpus linguistics, and the degree to which corpus design is expert-guided\cite{johansson1986tagged}:
\begin{quote}
The true “representativeness” of the present corpus arises from the deliberate attempt to include relevant categories and subcategories of texts rather than from blind statistical choice. Random sampling simply ensures that, within the stated guidelines, the selection of individual texts is free of the conscious or unconscious influence of personal taste or preference.
\end{quote}


This ambiguity could be seen as an argument against the concept of a general purpose corpus, however, current progress indicates that this would be hasty: whilst special purpose corpora are often burdened by fewer procedural and pragmatic difficulties, they are necessarily limited in scope.  It is still rational and useful to identify speakers of a language as a homogenous group at some level, especially for smaller linguistic features.  On the other hand, only a sample that properly encompasses a large amount of inter-language variation may be used to describe many effects of interest.

With this in mind, many of the studies into representativeness have focused on examining existing corpora, with a view to testing their internal variation against some known re-sample.  This approach has been taken most famously by Biber~\cite{biber1993representativeness}, who performed a series of studies in which he compared the content of 100-word samples from the LLC and LOB corpora.  Each sample was paired with another from the same text (LOB samples points are $2000$ words each, and LLC's are $5000$).  Biber went on to extract some deliberately small-scale linguistic features from each sample, before examining the difference in frequency between each.

% \til{more detail on this paper, since it's so crucial}

Biber concluded that existing corpora were sufficient for examination of smaller linguistic features, however, in the process he saw fit to reject the notion of representativeness used here (and, notably, everywhere else)\td{quote?}.  Biber's argument is that we should not aim for any degree of proportional representation of linguistic features, for this would produce a corpus that is mostly one kind of text (due to a conjectured Zipfian distribution of text types).  In many ways, this is an argument in parallel to the aims of stratified sampling, however, stratification of samples (rather that with adjusted weights) is normally performed as a compromise, where we are aware from previous sampling efforts that we cannot adequately sample randomly.  Biber's presentation of this idea has seemingly led many to reject the common wisdom of sociological sampling, leading to corpus building to be seen as a fundamentally new activity, and consequently as something of a black art.
% black art within the community.


%-----
A further aspect clouding the waters of quantitative representativeness assessment is disagreement over how to parsimoniously stratify language.  This is in part due to the difficulty in defining a taxonomy for genre, which is often the most controlled variable within a general purpose corpus' sample design.  Ultimately, I consider that the answer to this is, as mentioned in the LOB manual, down to the individual research question.

%-----


In part for the reason that Biber's work was taken early on as a validator of current practices in corpus building, the notion of representativeness has remained an almost entirely philosophical concept within corpus linguistics.% \td{mention efforts to establish it automatically, but retain a working definition here so more discussion may be freed up later}

% "There is a broad consensus among the participants in the project and among corpus linguists that a general-purpose corpus of the English language would ideally contain a high proportion of spoken language in relation to written texts. However, it is significantly more expensive to record and transcribe natural speech than to acquire written text in computer-readable form." -- (http://www.natcorp.ox.ac.uk/docs/URG/BNCdes.html / composition)


%----

% 
%  TODO: put me in the ideal corpus stuff.
% The concept of transferability % taken from sociology
% is an alternative often applied to qualitative analyses by those in the social sciences \td{find introductory citation, Dornyei?}.  It describes the likelihood that findings may apply to other members of a given, defined, population, without speculating as to the probability that a member of said population may be suitable for such a comparison.  This concept allows us to find evidence-based theories which hold true for groups specified by prior knowledge of the sample and human judgement of its relationship to the population.
% 
% It could be said that, prior to corpus methods, the best linguistics could hope to achieve was qualitative transferrability.  Indeed, some may argue that the theoretically infinite population of utterances language makes possible means that any sample is incapable of being representative of language as a whole, and that we are only able to generalise to observations of language [within finite time periods].  
% 

\til{ Discuss representativeness in a more "linguistic" way including balance and choice of sampling frame.  Refer to web as corpus section for "we also discuss what we're sampling in WaC terms below...}

% \til{Also, refer to the books blow-by-blow (use notebook + quote)}

\til{Include NLP representativeness stuff?}








\subsubsection{Size}
% \til{This is the primary driver of the above, no-one really pins this down but survey some corpora and some more modern approaches (Kilgarriff's google-ology comments at the end would be good.  Link to web corpora for extreme bigness and ref to lower sections at end}

The size of any sample is a crucial and often hard-to-determine property, and corpora do not differ in this respect.  In many ways the question of corpus size is a primary constituent of the representativeness property above, and though it has long been recognised as such there remains little consensus on just how large is large enough.

This disagreement is in part because language exhibits a number of properties that make it hard for us to gauge variability in the population, meaning that most sample size estimation methods (which rely on random sampling) are poorly suited.

%-

The first of these is the inability to accurately measure the complexity of language, which, as used and applied by humans, has unknown degrees of freedom.  This effect applies itself at many levels:

\begin{itemize}
    \item Lexicon---Vocabulary, even when restricted to a given demographic, time period or person, is difficult to define with any certainty.  Many people are capable of recognising entirely novel words, simply by virtue of their morphology. %Jabberwocky
    \item Syntax---The meaning of words and phrases is heavily context based, but the context affecting each aspect of language is variable and, in some cases, wide-reaching.  This makes it hard to determine if we should be sampling 2000-word texts, single sentences, or the whole thing.  This unknown sampling unit drives one of the key trade-offs in corpus design, as a 1-million word corpus comprised of single sentences will be capable of embodying more variation than will one made of two 500,000-word samples.
    \item Semantics---Variation in our understanding of langauge in context is poorly understood.  This is the driver behind any models of the above, but also affects how we should sample external variables such as socioeconomic factors and even the direct circumstances in which a text is used.  %\td{[socio/neurolinguistics has this job]}
\end{itemize}

This ambiguity produces a tradeoff that has been identified by relatively few in the corpus-building community \td{(notably biber, Greis, Evert with the library)}---that corpora of equivalent size may yield significantly different inferential power due to their differing number of data points.  This may be seen as an issue of depth vs. coverage: corpora including large snippets of text are capable of supporting more complex models and deeper inference, at a cost to the generality of their results.

There remains disagreement upon the extent to which these two aspects should be traded off, though it is notable that the NLP and linguistics fields vary greatly in their treatment of the data, with linguistics typically focusing on frequency and immediate collocation, and NLP being skewed towards more complex, instrumentalist, models.  I would suggest that, realistically speaking, researchers should be examining their experimental design with respect to the size and/or complexity of the features they are working with in order to select a corpus that matches most closely.  It is also quite clear that this does not happen: the BNC contains a small number of large samples, yet is often used to analyse small-scale linguistic features.

% \til{mention Biber's contribution in passing, and say how this may also be seen as a property of the clumpiness of language and that work on dispersion is able to help somewhat}

%- 

Secondly, establishing the boundaries of a given language is difficult.  Users of a general-purpose corpus wish for two conflicting properties to be satisfied---the population must be wide enough to provide a useful set of persons about which to infer (and, more loosely, this should align with those persons we can say informally, for example, `speak English'), yet the corpus must provide sufficient coverage to represent that population in the first place.

Generally it seems that this problem, though having been recognised, has received too little effort for pragmatic reasons.  Issues of balancing demographics in corpus design have typically taken a curiously detached form, that of selecting a sample of language as a proxy for demographics (e.g. selecting the bestsellers over unpopular books or sampling more popular newspapers).

\til{mention brown, eagles}

% This concept has extended itself to the point where corpus building is seen as a problem phrased in terms of which texts to select, rather than how one might select them.  
This approach has led to a situation where each and every corpus carries with it the expert judgement of linguists not only in selecting a wide variety of texts from within the population, but also their socio-linguistic opinions.

This reliance on expert opinion to overcome practical challenges associated with text retrieval has lead to the sampling policy being somewhat opaque to end users.
Those using a given corpus are not necessarily able to rigorously define about whom they infer a given result.  In practice, this often manifests as annotations and comments on the demographics of texts analysed, such as noting that much of the text in the corpus was from one source, or seemed to cover a weird circumstance\td{examples?}.


%-

Finally, disagreement on how we should extract features from language samples (i.e. which dimensions of variability are interesting) means that, aside from the immediate and obvious properties such as genre (about which there is even less agreement), any efforts to stratify language are met with suspicion.  This often leads to a researcher performing a corpus analysis using an informally-subsampled general-purpose corpus, with questionable correspondance between the categories used to select texts and their research goals.
It is the author's opinion that this is unanswerable except for individual studies: in order to know variation in features affecting one's use of a corpus, it is necessary to define the covariates and evaluation strategy.

% This disagreement robs corpus builders of the power to perform true multi-phase stratified sampling, though it is worth noting that the efforts of the 90's led to a general agreement on how to differentiate between some of the major covariates.\td{rewrite and clarify}

%-

Further to these problems of defining the nature of the sample, there is the resultant problem of determining a sample size even where these are known.  

Taking the first of these issues in the extreme, it may be said that the only ``fully representative'' corpus must contain all context for each text (something that would include at best an abridged history of the world) in order to satisfy inquiry from many different perspectives.  Given the limitations in analysing properties of language beyond a given scale, and the clear impracticality of extending that scale's upper bound, it seems reasonable to conclude that current corpus efforts are sized so as to be useful for small-scale features, and that our inspection of language is currently somewhat shallow.

Taking into considerations problems of proportional, stratified sampling, it seems possible to establish a corpus size and composition that is widely agreed upon.  Nonetheless, issues of selecting a sampling unit mean that the resultant corpus may either be a refinement of current efforts (not necessarily a bad thing) or utterly colossal and beyond the capacity of even modern processing systems.

Since sample size and composition are intimiately related both to one another and to the concepts of representativeness and transferrability, this issue will be one of primary importance to the rest of the thesis.





\subsubsection{Purpose}
% \til{examples of purpose.  General/special.  Cover qualitative issues and quantitative issues of parameter estimation from biased statistics.}
The reason for sampling a given population is a crucial feature of corpora.  Not only does it define the level and type of metadata available (and the arbitrary definitions used therein), the expert selection of sample designs means it has a large influence on the sampling frame used, and thus the validity of any generalisations made using said data.

The condition given by McEnery \& Wilson\cite{} here is that a corpus must have been built with some degree of linguistic inquiry in mind, for example, the collected works of Shakespeare would be counted, but not one's personal book collection (even if it happens to include the complete works of Shakespeare and nothing else).

This distinction seems to be little more than a way of stating that one's ability to infer things from a corpus is relative to its external properties, which is true of any sample.

This requirement calls into question two things: the generality of a corpus, and the extent to which it is documented.

General-purpose corpora sample a large population, of interest to many users.  This requires that they remain fairly unbiased and cover a large amount of variation (which in turn necessitates very large sample sizes).  Since they are re-used many times, the quality of their documentation is key to their gross value---each user will require particular variables in order to generalise from the text.

Special purpose corpora avoid this challenge by being re-used for less disparate aims (and, generally less frequently).  Because of the relative focus, their documentation is often able to be significantly more detailed, allowing for deeper insight.  This approach, however, is not transferrable to the sample sizes used in general purpose corpora.

To contextualise this, both examples above merely constitute special-purpose corpora, that offer answers to different research questions.  For one, we may answer those about the nature of Shakespeare's language use, whereas the other allows us to find knowledge about a given person's literary preferences.  It is of no direct consequence that more people are likely to care about the former.

If we extend the example to include others' book collections, the ability to generalise changes: we know significantly more about Shakespeare than many other authors, and it is thus possible to annotate the Shakespeare corpus with details of his life, times when works were written etc.  The same could not be said of a corpus where our aim is to investigate reading habits (even if some people exclusively read Shakespeare).

\paragraph{}
This `purpose' requirement can be phrased entirely in terms of external variables.
A group of texts about which nobody knows anything do not offer any opportunity for inquiry (except about themselves), and so it is reasonable to require documentation of the context in which those texts occur (or any external property that demarks a homogenous group).

By stating that a corpus must have a defined purpose, we include a set of reasonable assumptions about the corpus and its external properties: a corpus built for study of Italian newspaper text is unlikely to heavily sample the Guardian, for example.  In many ways, statement of purpose offers a shorthand for many decisions and assumptions inherent in construction of a large corpus, and a simple way to assess how well matched a corpus may be for another---related---purpose.

It is noteworthy, however, that selecting a corpus by its original purpose greatly complicates any subsampling that is possible---one must be able to subsample texts not only according to external data of interest, but also taking into account the original interactions and assumptions made by the constructors.  As mentioned above, it is unlikely that any corpus is capable of being documented `fully' enough to avoid these issues, however, this is a compelling argument for focus on detailed metadata being provided (rather than detailed rationales for sampling), expecially in the case of general-purpose corpora.


% see http://www.natcorp.ox.ac.uk/docs/URG/BNCdes.html


%----


\subsubsection{Data Format}
Many authors stipulate that a modern corpus should be electronic.  To generalise this position, they require that it is in some way processable by machines, i.e. that it must be in some way regular.
This defines the format of not only the basic textual content, but also the availability of metadata at all levels (category/strata, document, word).

Both of these have been addressed to some extent, especially the problem of representing the text itself, which has largely been solved by UTF-8 at the character level, and XML/SGML at the markup level.  Standards derived from efforts such as the TEI\cite{ide1995tei} have some penetration, though there is often still the need to include proprietary extensions if other concerns on this list are to be maintained.

Though earlier work on corpus construction focused on data formats\cite{atkins1992corpus,EagTcwgCtypeaglespreliminary}, with a view to sharing corpora for local analysis, modern approaches tend towards corpora `as a service'\cite{hardie2012cqpweb,ferraresi2008introducing} --- providing a front-end to query and analyse text directly whilst still hosting it at the original institution.  This approach is largely taken to mitigate licensing issues, and to work around the high level of technical skill necessary to work with large-scale data.

This thesis takes the view that representation is largely a solved problem: UTF-8 and XML are both capable of storing international characters and complex annotations in a way that is easily mined for many uses, and advances in database technology and distributed processing offer many ways to process and retrieve structured data.




\subsubsection{Classification}
Efforts have been made to address the ambiguity of some often used strata such as text-type and genre.  One of the more notable was EAGLES~\cite{EagTcwgCtypeaglespreliminary}, which produced a number of recommendations designed to be applied to new corpora whilst retaining general compatibility with existing designs:


To some extent, the form a corpus takes is defined by its content---any meaningful taxonomy is arbitrary and theory-laden.  For this reason, recommendations made by EAGLES were driven by a review of the theoretical basis for existing taxonomies and work at the time:

%CITE EAGLES http://www.ilc.cnr.it/EAGLES96/intro.html
% \til{Describe EAGLES briefly, but t'is not a main focus.  Quote below is from \url{http://www.ilc.cnr.it/EAGLES96/intro.html}}
\begin{quote}
Any recommendation for harmonisation must take into account the needs and nature of the different major contemporary theories.
\end{quote}

The need to provide useful metadata is particularly challenging for general-purpose corpora, which have very loosely-defined aims and must maintain a high level of neutrality whilst also proving useful to many people.



%--

Internal text distinctions may be drawn using bottom-up (often termed `corpus-driven') methods, however, these are frequently difficult to operationalise.  A prime example of this is Biber's multidimensional analyses of corpora\cite{biber1992complexity}, which focus on feature extraction and principal component analysis in order to identify primary dimensions of variation within corpora.

Biber's approach has been held aloft by many as one of the only `unbiased' analyses of variation around, however, this is not the case.  Without an authoritative underlying theory of language use by humans, any features used to construct the model will, however neutrally treated thereafter, exert pressure on the results.  Given the subtlety with which such analyses extract information (and the difficulty in interpreting resultant factors), this is likely to lead to favouring one set of conclusions over another.

This is also true of techniques such as latent dirichlet allocation (LDA), which is usually trained using token frequencies.  These straight frequencies are still entirely bereft of context, and tokenised using procedures that embody particular theoretical decisions.

Sharoff\cite{sharoff2007classifying} presents a more transparent clustering methodology that relies on keyword metrics to identify salient features.  This provides some connection to other keyword literature, along with a mechanism for inspecting the resultant categories.

Arguably, further problems with the approach of finding natural strata of variation lie with sampling and linguistic problems: many of the practical issues surrounding corpus building involve enumeration of easily identifiable groups of texts, something that would be hard to compromise or `trade off' if working from factor loadings.  Further, many analyses will find subsampling data difficult when defined in terms of many external variables, though this is more of a challenge for the linguistic community (and providers of its tools).

%--

Through reasoned examination of existing efforts, Lee\cite{lee2003bnc}
was able to re-form the BNC's classification by adding external data and classifying documents manually.  This is arguably the most prominent effort to apply the multi-phase `examination and re-appropriation' method that Biber and EAGLES recommend.

His methodology, though labour-intensive, offers a defensible way to trade off the various interests of users.  One key aspect to this is the fact that it was built after the corpus, and thus may take into account common usage when making distinctions between texts.  Though useful, the extent to which this is scientifically defensible is arguable, as convenient distinctions are not necessarily those made by the language users themselves, and such a manual approach is afflicted by the various biases of expert opinion.

% \til{Describe taxonomy defining efforts and perhaps stratification in a way that is more tied to original corpus documentation, esp. stratification w.r.t. brown/brown-inspired corpora}


In summary, the problem of producing a meaningful and operationalisable taxonomy for corpus organisation is actually one of community agreement: for any given user of a corpus, the task is simpler.  This indicates that an a transparent, well-documented approach should be taken in order to allow end users to decide upon their level of agreement with the pre-applied categories, or that differing levels of confidence should be applied to aid subsampling.


% When it comes to large corpora (such as general-purpose ones), there is good reason to require that they are electronic in order for them to be usable, since smaller, `hand-processable' versions would not be sufficiently representative.  As with purpose, then, it seems reasonable to require a standardised format, and increasingly reasonable to require that a corpus be machine-readable.







\subsubsection{Dissemination and Collaboration}
The ability for multiple researchers to access a corpus is one of the main benefits of corpus methods---corpus-based studies may be replicated and compared with absolute certainty of the empirical aspect of the research.

The process of building a large, multi-purpose sample for use by a whole field of research mirrors that used in other fields, many of which have similar probems gathering data.  Many examples of this approach exist, such as the BHPS and BCS\cite{taylor1996british,hough1983british}.

Although corpora are generally less sensitive on ethical grounds, many legal challenges remain to distributing and using such a large quantity of data.  This has historically greatly limited both the source and form of data gathered for corpora, and was one of the reasons behind sampling 2000-word samples (rather than whole texts) in Brown's corpus.

Navigating copyright law remains one of the primary tasks of a corpus building effort, though the increasing dependence on digital sources has dulled this somewhat, since they are typically less controlled when being published.  Nonetheless, corpora often come bundled with restrictive licensing, something that limits the ability of the wider community to participate in their use.



%-



The heavy re-use of corpora, then, exerts both positive and negative forces upon the scientific community.  On one hand, it is necessary to share resources in order to lower costs, increase awareness of rare data, and pool efforts to create better samples.  On the other, the ubiquity of a corpus may damage its scientific value, and starve the community of more up-to-date or relevant resources.  In the worst case, widespread use of a single corpus by the community may lead to partial circularity of hypothesis derivation and testing.

In an ideal world, it would be possible to replicate studies with one's own samples.  As I shall cover later, increasing digitisation of documents (and use of the web) offers a way to make this scenario possible, at least for some corpora and purposes.



\subsubsection{Sample Type}
Corpora span many types of sample, from simple cross-sectional ones (`synchronic' corpora) to those that aim to report language use through time (`diachronic' corpora) and a hybrid of the two, which is designed to follow language use and update on-the-fly (`monitor' corpora).  Further to this, there are parallel corpora, intended to match texts across some variable such as language\td{cite %http://www.statmt.org/europarl/
}
, and many other designs that combine properties of these to satisfy various sample designs.

These approaches represent various use-cases, and are often significantly less `general-purpose' than large synchronic reference corpora such as the BNC.








\subsubsection{Summary---A working Definition}
The above discussion illustrates the wide variety of samples that may be called corpora.  The focus of this thesis is on mechanisms for making, operationalising and documenting the above choices, and as such the working definition here opts to not apply any clear restrictions.

One obvious requirement, however, is that a corpus must be a suitable sample \textsl{for its intended purpose}.  This means that the sample design (external variables) and the document classification (internal variables) must be clearly defined in terms of the research questions given.

For the purposes of this thesis, a corpus constitutes a body of text that must be:

\begin{itemize}
    \item Representative of some stated population;
    \item Sufficiently large to satisfy that representation (for a given set of purposes);
    \item Explicit in its coverage of external variables;
    \item Of a regular, machine-readable format.
\end{itemize}


% 
% 
% % TODO: some kind of blockquote thing
% ``A collection of text samples, subject to the following conditions:
% 
% \begin{enumerate}
%     \item \label{enum:corpusdef-external} External (contextual) data is sufficiently well documented to prove useful to research (i.e. the population is well defined, texts are annotated);
%     \item \label{enum:corpusdef-internal} Texts are sampled in an internally-consistent manner, relative to external proportions (i.e. the corpus is externally and internally valid, and thus representative);
%     \item The data are recorded in a format suitable for automated processing (i.e. the corpus is not intended for direct manual inspection, and will be sized accordingly);
%     \item Documentation is provided in order to describe the veracity of claims made for \ref{enum:corpusdef-external} and \ref{enum:corpusdef-internal} above.
% \end{enumerate}
% 
% ''
% 












% -------------------------------------------------------------
\subsection{Validity Concerns in Corpus Building}
\til{This gets a bit noty here as I dump stuff from other sources, todo clean up and order.}




The requirements discussed above are satisfiable in a number of ways, each of which will exclude and promote certain uses of the result.  Generally, threats to the validity of inquiry based upon these samples may be broken down into three areas:

\begin{itemizeTitle}
    \item[External Validity] How relevant the sample is to the population about which we wish to infer something.  These are likely to limit the generality of a study, or lead to under/overestimation of its effects.
    % (Representativeness vs. transferrability, coverage of population, size [sampling unit]) sampling design issues
    \item[Internal Validity] How much a study can rely on document annotations and data in order to draw conclusions.  Issues here are likely to cause false results.
    % (internal proportions/categorisation, stratification, covariates, comparability)
    \item[Practical Issues] Limitations on the mechanics of sampling.  These issues may cause either or both of the above.
    % that prevent gathering an ideal sample
\end{itemizeTitle}

This section identifies potential issues with corpus sampling methods.  The majority of these are practical issues for which fixes must be designed carefully and on a case-by-case basis.


% Not listed here are cognitive biases. It's worth noting that one of the main advantages of statistical and quantitative methods are their objective properties: humans are exceptionally poorly suited to reasoning about large numbers, and our ability to digest them mathematically into smaller quantities of information is crucial.% This is embodied in my original working defintion of a corpus, that it is not meant to be read by a human.  



\til{
30-06-13 I've not edited this since I want to go through and consolidate all the sections, find refs for each, etc when I have access to my notes.
}





\subsubsection{Population and Covariate Specification}
This category largely describes the manner in which certain important covariates are selected for sampling, or sampled. Though language contains an unknown and very large number of possible dimensions of variation for study, many of them are judged to be of particular importance in describing variation across texts.

Atkins et al.~\cite{atkins1992corpus} provide a series of recommendations for covariate selection, which I have used to guide this list.




\paragraph{Poor definition of population}
% \til{see [this](http://www.natcorp.ox.ac.uk/docs/urg/)}
The population for whom a corpus describes language use is relatively difficult to describe, as it is largely a self-referential problem. Though efforts are often made in speech corpora to balance content according to populations of language speakers, the same cannot be said for written samples, which are often selected from various indexes.

The utility of linguistics, and linguistic studies using corpus methods, is in their capacity to infer truths about a given population. Any well constructed sample should adequately represent each member of this population, and hence any study based upon the data is necessarily limited by the scope of this sample. Though there is doubtless great value in imperfect samples, much of that value is extracted by reasoning qualitatively about a `typical' user who is properly represented. Poor definition of the population in terms used within analyses renders this reasoning unreliable.

Seemingly, one reason why corpora are not specifically targeted is because their selection processes rely on lists that are compiled using data for which it is difficult to determine comparable bounds.  The BNC's policy for selecting written materials, for example, is segmented into the following sources:

% \til{Insert more detailed overview, more critical analysis thereon}
\begin{itemize}
 \item books, selected from bestsellers, literary prices, library loans, additional texts;
 \item periodicals and magazines (including newspapers);
 \item other media (essays, speeches, letters, internal memoranda).
\end{itemize}

Of the lists used in the books category, they specify the following criteria:

\begin{quote}
``Each text randomly chosen was accepted only if it fulfilled certain criteria: it had to be published by a british publisher, contain sufficient pages of text to make its incorporation worthwhile, consist mainly of written text, fall within the designated time limits, and cost less than a set price''.
\end{quote}

It is often difficult, using other sources of data, to establish the details of an author's nationality, the age of a text, or the suitability of the time limits\cite{dollinger2006oh}.
% Indeed, it seems likely that these bounds are likely to vary according to which type of text is sampled, but this is a relatively minor issue which would complicate use of the corpus).
These issues are typically better addressed by more specialist corpora, which are subject to easier-to-determine bounds such as social role, context or text type\cite{kucera2002czech,przepiorkowski2008towards,kyto1993manual}.

Note that the problem of specifying a population is quite apart from that of determining what proportion of each stratum should be included in a corpus.  The relative proportions of each of the above categories (as well as the problem of selecting proportionally from each list) are a problem of the validity of the inference made from a corpus, i.e. internal consistency.  Use of a corpus to evidence truths about a given population, where this is either unknown or different, leads to internally consistent, but externally irrelevant conclusions.

Ambiguity in population definition has a number of direct influences on other issues of corpus validity. Selection of stratum sizes, for example, must be based on the relative proportions of language used by the given population.  If a population is defined using purely internal (linguistic) properties, this becomes a reflexive and circular task---we end up selecting proportions of language to match proportions seen in language.  Use of auxiliary data to augment sampling policies (such as social demographics taken from other, large-scale surveys) must also be matched to the population in question.

Where selection of genres is defined by linguistic content, it is necessary to ensure that the frequencies used do not have systematic correlations with features being studied.  This is impossible to automate, and must be assumed based on experience and theoretical reasoning.

%-- 

Speech corpora are often specified in a significantly less ambiguous manner (though not always with more proportional sampling). This seems to be due to the direct nature of speech sampling, which involves the person actually performing an utterance (rather than the language use itself being a persistent and concrete artefact).  Essentially, this is nothing more than a paradigm shift: there is functionally no difference in sampling a work that is synchronously `performed' and received to one asynchronously.

This difference is identified by Leech~\cite{leech2006new}, and will be inspected in greater detail below, as it is a potential major source of disagreement between construction and use. % [section about production/consumption]\td{comment more}









\paragraph{Time Distribution}
The temporal properties of a corpus will vary greatly depending on other design decisions within this list: corpora that aim to sample use will necessarily contain many older texts, whereas those based on production may be stricter.
% The Brown and LOB corpora, for example, sampled texts produced in 1961 only.
Sampling online, or sampling from historical texts, merely identifying the age of a text is also challenging.

Though the concept of sampling texts in a single time frame makes perfect sense when sampling language \textit{production} only, many corpora (including, explicitly, Brown and the BNC, aim to trade-off both production and reception of texts in order to balance the zipfian popularity of many kinds of text.  For example, the vast majority of published works are seldom read, yet certain books might be read millions of times.

The differences between distribution of production and consumption of texts leads to an interesting question: for a given population, how old are texts used on a day-to-day basis?

This question, with its direct relation to the original ideals of corpus sampling, seems largely unaddressed in the literature. Some efforts have been made to incorporate linguistic change over time into corpus design, however, these do so in an abstract manner and under the assumption that synchronic sampling is fundamentally valid in order to represent language use (use here refers to the proportional mix of production and consumption).

% \til{Mention various efforts to specify this, if i can find any. Talk more about monitor copora and specifically historical/diachronic corpora.}






\paragraph{Metdata Comparability}
Efforts such as EAGLES identified this issue as a significant one, and much effort has since been spent on minimising the problem of classifying texts.  Nonetheless, the difficulty in identifying a widely-agreed-upon classification scheme means that this remains an important design decision.

This occurs in two ways. The former of these is the problem of selecting texts according to external, non-textual, variables about which we may wish to infer findings, such as level of education of the author, nationality, target audience of text, etc. This is perhaps the least agreed-upon, partially because of the focus on sampling using existing lists as proxies for proper demographic specification.

The latter of these is selection and stratification according to external, but textual features such as genre, text type, medium, etc. these are fairly well agreed upon and specified in a fair amount of detail in efforts such as EAGLES and Atkins, et al.  

A secondary, yet related, issue is that of metadata completeness: often, texts are sourced from very different places, and come with differing levels of metadata.  Normalising and homgenising these metadata is particularly challenging, and may include a loss of resolution for many texts.






\subsubsection{Practical Issues}

\paragraph{Copyright}
Copyright is one major problem with sampling large volumes of text from any source, especially those already available for-profit from large publishers, who are acutely aware that their entire business model is based on controlling access to their intellectual property.

This is stated in the original documentation for the Brown corpus as one reason for their choice of sampling unit, and often causes `black spots' in the sampling frame of a corpus, where certain publishers are known for their absolute declination in offering material.

\til{there are various quotes about this, but are any on-the-record?}

\paragraph{Lack of a central index}
The lack of a central index for a given set of documents makes random selection within strata difficult. Any indexes that do exist are highly heterogenous in purpose, form and extent. Digital indices such as those maintained by search engines are skewed by corporate forces and also only partially cover their indexed population.

Many indexes conflate the concepts of production and consumption.  Search engines are an ideal example of this, as their goal is not to return all results for a term, but to return the `most useful'.  This may actually be a desirable property if seeking to include real-world use in a sampling scheme.

% \til{Cover which lists *do* exist and relate back to the brown, BNC design (publishers' lists, libraries, etc).}



\paragraph{Time Spent Gathering Data}
Gathering data from physical resources is expensive and difficult.  

Paper documents must be scanned, digitised, manually corrected and converted into a normal form. Speech must be gathered in an ethically defensible manner, and metadata about the speakers must be gathered.  Electronic documents require significant format conversion.

Beyond these first efforts, any metadata and annotation must be added for the final form to be useful.

Digitisation technology, especially that used for paper documents, has recently progressed significantly due to efforts to preserve historical documents, and libraries' attempts to digitise older publications. In some cases, scanners are capable of scanning entire books without intervention.




\paragraph{Ethics}
For speech data, and written data that is not intended for public dissemination, issues of privacy must be considered.  Any auxiliary data is unlikely to suggest proportions for documents used within a private context (e.g. notes left on a fridge), or those controlled by formal social structures (such as documents used in many offices).

Some topics and contexts that ought to be represented are difficult to sample due to the cultural taboo surrounding them. This is especially relevant for speech corpora, since speech is often used for informal transactions, and because it is difficult to separate the sampling procedure from the speaker himself.



\paragraph{Ephemera}
Many documents encountered in everyday life are designed to be discarded. This means that many are difficult to obtain after-the-fact.  Also, the presumption that out-of-date ephemera are difficult to obtain forms a significant part of the privacy people take for granted, raising further ethical concerns (indeed, this concept is enshrined in law in germany, who have `the right to be forgotten').


\sepline


\subsubsection{Stratum Selection [external validity?]}

% \paragraph{ most covariates not considered, or considered in a haphazard manner}
% Though it is clearly impossible to stratify a sample according to all of its main covariates (especially in the case of language, where this set is both large and unknown), the selection of covariates upon which to stratify a selection is somewhat haphazard and seldom justified in terms of the research value of the corpus.

% In some cases, one part of a corpus is stratified one way (books, for example from index lists), with another part (typically speech, sampled demographically) being stratified another. This must clearly lead us to cast suspicion on the validity of any comparisons between these parts (and the relevance of subsamples from each to the original corpus documentation's statement of representativeness and purpose).

\paragraph{Lack of Mutual Exclusivity}
Though a problem for categorisation and taxonomy selection, the `fuzzy edges' of genre, medium, popularity and other covariates often leads to ambiguity surrounding which category a text should belong to.

Though this will always persist in some form, a reasonable solution is to precisely specify objective conditions for selection. These may then be inspected by a researcher in order to assess the value of the sample to their particular questions.

Some classification schemes apply multiple labels to a text in order to avoid this\td{which}.  This approach may provide a way of producing a more accurate overall classification, but complicates many analyses, particularly quantitative ones relying on regression.


\paragraph{ lack of variability/saturation analysis/multi-phase}
Biber and others\cite{leech2006new,biber1993representativeness}\td{check Leech ref} recommend that sampling should occur in an iterative process, with the contents of a corpus being used as evidence to weight selection of strata from the next version.

As Varadi~\cite{varadi2000corpus}\cite{varadi2001linguistic} notes, this simply doesn't happen.  indeed, the simpler option of using a previous corpus to guide one's stratum selection is also ignored.  Reasons for this may include the shift in classification priorities, and the time required to re-code and align a previous corpus' annotation structure to that which is to be built. 

Having acquired a first iteration of a corpus, there are many methods for assessing the 'completeness' of a sample.  some of these are explored by Biber, and others (including Evert, Greis) go on to describe further internal measures\cite{evert2004simple}.

Note that this is quite aside from the problem of assessing corpus proportions using external variables, which is easier to inform using other studies in a multi-phase fashion (using demographic proportions, for example).



\paragraph{Coverage of Population}
An issue of external validity, and distinct from the problem of specifying a population.  Having specified a population, it is necessary to establish the connection between this definition and the population actually retrieved.  Ideally this would be based on auxiliary data and large-scale studies of language use, however, in practice there is much expert opinion involved.

This is mainly a problem of ensuring that the strata selected, in whatever proportions, cover all language use in the given population.  This is distinct from the quality of each stratum, or the relevance of its weighting, and is a problem of omission: the opinions of a group of researchers (though skilled) are unlikely to fully cover language use of highly disparate social groups within a society.

Auxiliary data from social surveys may be used as a rough indicator for this, though in reality no data source exists that can describe, in social terms, the 'types' of language used (w.r.t. the primary dimensions of sampling for corpora).  Questionnaires, ethnography, and/or direct sampling may be the only ways to establish a ground truth for this, but for now it remains an open research question.% [until the personal corpus section ;-)]





% \paragraph{External demographic information}
% this is a problem of weighting strata to fit the population.  

% in the case of speech data, there is often some attention given to the deompographics of the ext producers.  in the case of written data, the popularity of items on an index is often taken as a proxy for this.  

% check: brown and lob, iirc, 'balance' their demographics by choosing equal proportions of some major variables, which is not really balancing.  the bnc may also do this.  there is simply unknown balance for the books, and it's worth noting that different people buy books vs. go to libraries, making the populations for each quite hard to compare scientifically.



\paragraph{Stratum Sizing}
This problem can be seen two ways, depending on the sampling policy.  In an ideal world, strata are entirely proportional, and the problem of identifying the minimum size is one of ensuring that all variability within the smallest stratum is adequately represented.  In practice, one of the reasons for stratified sampling is to, with philosophical rather than mathematical justification, boost the prevalence of a stratum by artificially oversampling it.

Both approaches are correct from their respective viewpoint, however, the deliberate oversampling seen in corpus construction is often informal.  Algorithms for internal saturation/sufficient sample size may be used for both the formar and latter, though it's worth noting that the sample is less random if used in the latter way.  The highly Zipfian distribution of language will, for most practical purpose, necessitate the use of inflated stratum sizes for small strata.



\subsubsection{Randomness}

% \paragraph{ most covariates not considered (see stratum selection)}
% see 'insufficient consideration of external demographic information' for a specific case of this.  

% one of the key reasons for using stratification is that it allows one to correct for the biases implicit in sampling technique which may be caused by difficult-to-predict practical concerns such as patterns of behaviour through time (a classic example being the disproportionate sampling of women through going door-to-door during working hours).  ignoring a multitude of covariates relinquishes them from this control, meaning that we should generally stratify according to any we believe to be particularly crucial to corpus integrity and purpose.

% whereas some covariates, such as genre, date of authorship, etc, are well considered in the design of corpora, many are selected in accordance with availability or legality.  this is one cause of the large disparity between spoken and written corpora in most general-purpose offerings (for example, the bnc is just [n%] spoken).  if we were to control for this proportion, analyses relating this to other important variables would be possible.

% practically speaking, the importance of variables to the 'balance' of a corpus may be seen as zipfian: a few key features, such as genre or formality, are going to heavily influence content and vary widely across the population in ways we wish to investigate, whereas most variables are going to exert only a small influence on the resultant selection of linguistic data.  much of this decision must be made in a qualitative manner, and there will always be a small population of variables we *should* have sampled for a given study.  this problem is one of degree.



\paragraph{Availability Sampling}
The pragmatic issues surrounding corpora do not apply equally to all genres, media, social demographics, or settings.  Interactions between the key variables of interest and the sampling method cause gross over-sampling of things that are easy to access.  Technically speaking, this is a composite of many other biases, though the final effect is easy to see.

A prime example of this is the proportions of written and spoken texts in many corpora, and even the proportion of elicited spoken vs. 'natural' spoken texts, due to the difficulty in obtaining consent.  Another example may be seen in the BNC's proportion of academic or newspaper texts, both of which comprise a very large proportion of the corpus, yet are read by a relatively small propotion of the population.

% [interestingly, corpus builders seem to be willing to spend umpteen hours manually coding a book, yet refuse to properly sample something if it doesn't already have a centralised index.  perhaps this is because they can hire grad students for the former ;-) ]


% \paragraph{ lack of variance/saturation analysis (see stratum selection)}
% see 'size.no methods for saturation/internal measures'


\paragraph{Nonrandom Sampling}
% (i.e. web crawls)
Though much effort is made to randomly sample where possible, some forms of corpus are particular susceptible to the siren-like call of availability sampling.

In many cases, such as web crawling, nonrandom sampling strategies are used due to the lack of an authoritative (or reliable) central index.  In others, such strategies may be the result of other practical issues, such as the location of researchers or legal concerns surrounding certain contexts.

It's possible to augment and correct for a lot of the problems that are introduced through nonrandom sampling, for example, Schafer and Bildhauer do this in their web-scraping corpus building tools, which attempt to stratify their samples by top-level-domain \textsl{after} selection\cite{schafer2014focused}.

Nonrandom sampling is an unavoidable truth of corpus building in almost all contexts, and with care should not unduly influence the result of a corpus building effort.
%With proper adjustment, this problem is to be of equal or lesser magnitude to that of omitting some variables from stratification efforts.



% \paragraph{ incomplete coverage within each stratum [use of proxy vars]}
% note that this is distinct from specifying the population, or ensuring that the strata cover the population.

% this is todo.  Stuff was written here, but it was poorly differentiated from some other sections.

% TODO this!

%ignore the blockquote below, it's a bit conflated.:
%> a prime example of this is the sampling of books from the bnc (as stated above, perhaps re-paste?).  this strategy selects from a number of different lists under requirements of the books being popular and written in a given year.  though the bnc aims to represent language use in britain synchronically, this portion of it could more accurately be described as a sample of authors' language use in that year.  it seems irrational to suppose that authors are representative of the general british-english speaking population, or even that those *reading* the works (who may or may not 'agree' with the language used within) are representative of that wider population.  indeed, if we believe the taboids, the percentage of people who read books is fairly small.  further to this, among book readers, it is important to consider that many of them will be reading classics or mixing their reading through time, something deliberately excluded by the bnc's sampling strategy.






\sepline
\subsubsection{Size}
Sample size calculations are a complex topic, particularly where the variance for a given population is difficult to determine.  Due to limitations in power analysis methods for mixed-methods designs, many of the sample size judgements required during corpus construction will necessarily include some expert opinion.


% \paragraph{ no overall size established }
% (w.r.t strata, and randomness above)

\paragraph{Internal Saturation Measures}
% (see randomness)
Within strata, efforts should be made to ensure that a corpus contains sufficient data to represent linguistic variation.  The obstacles to constructing such an internal measure are great, since they relate back to problems of establishing the limits of variation within language.

Evert's (zipfr) LNRE models offer possibly the most advanced generative method for this, and can be seen as a progression upon Biber's variation analyses of the early nineties.  The issues with using such models to determine corpus variability sit neatly in two categories:

\begin{enumerate}
 \item Selection of interesting variables to measure sufficiency (this, in reality, would be part of the corpus documentation: we can say it is sufficiently representative for frequency counts, grammatical analyses up to n tokens, etc. with little risk)
 \item Decisions on how much data is enough data.  This is fairly easy to approximate with a sufficiently accurate language model, and even simple binomial approximations prove useful in judging the likely frequency of features.
\end{enumerate}

In spite of the practicality of the techniques surrounding measures of internal feature saturation, few currently do such analyses.  This is perhaps because the linguistic researcher himself must usually perform the analyses in terms of his own study criteria, rather than the corpus builder.

\til{notably, [someone, ahrg, mentioned in greis 'exploring variability' iirc], has looked into applying these methods to corpus stuff... find+review paper.}



% \paragraph{ no multi-phase/iwp design}
% *todo: this really belongs elsewhere, since this point repeats some of the others and the discussion would suit there instead*
% perhaps due to biber's dismissal of the design, little work has been done on constructing truly proportional corpora.  biber's primary objections were phrased in statistical terms:

% \begin{quotation}
% Biber's objections
% \end{quotation}

% Yet are motivated by entirely practical considerations. Since his description of representativeness served to flatter many corpora of the time, it seems to have been widely accepted by those without particular interest in statistics. Nevertheless, some have challenged particularly this assumption.  varadi (2001) flat-out rejects his definition, stating that it is particularly unwise to re-approriate such a widely used and recognised term.  [check greis' discussion, get quote from leech's paper].

% ultimately, i am willing to entertain biber's point in relation to stratified sampling in the general case: it is often desirable and necessary to inflate the smaller strata in order to properly represent the variation within. This, however, is purely a *practical* issue, and should not be confused with the ideal case of pure random sampling.

% biber goes on to advocate a watered down (non-proportional) multi-phase design, including the figure:

% [figure from p256 biber93]

% which has been seen by many (and recommended by eagles) as a wise strategy for corpus building. Nevertheless, as is noted above, it is seldom followed even roughly.


\subsubsection{Sampling Unit}
% \til{Might be worth clarifying the two approaches to this (models vs sampling) and perhaps link in Sean Wallis' general bent of establishing a baseline.}

\subsubsection{Sampling / Analysis Unit Mismatch}
% i.e. For a given corpus size in words, the size in units may be too small.
Any sample is, strictly speaking, best analysed in terms of its sampling unit. This issue has been inspected many times, as it is particularly visible during corpus analyses.

Biber's initial assessments of language variation in 1988~\cite{biber1988variation} assessed the suitability of the $2000$-word sampling unit by splitting each sample and comparing the relative frequency of features in each half.  He determined that, if they were the same, then the sampling unit size was sufficient to represent a given feature.  This lead to the conclusion that corpora were adequate for inspection of 'small-scale grammatical features', something that seems to be bourne out by the success of corpus methods in this area (POS tagging, etc.).

This issue is often phrased in terms of dispersion, i.e. the tendency for a feature to be represented evenly in all documents throughout the corpus.  Dispersion is something that manual inspection of concordances and corpus data renders particularly transparent, as it is often possible to see that all instances of a particular idiom are tracable to a given author, or all coverage of a certain topic is from one publication.  This is a prime example of the unit of analysis being far smaller than the sampling unit, a problem that is receiving increasing recognition.

Evert, with his library metaphor~\cite{evert2006random}, describes a sampling policy for corpus linguistics that avoids this disparity.  In it, he describes randomly sampling progressively smaller units in a virtual library, moving from books through pages to sentences.%  todo: check if he stops short of words, iirc he does.

Since many statistical problems arise from the disparity between sampling and analysis units, a particularly problematic instance of this issue is the use of bag-of-words (frequency) models.  these model language without taking into account order, and as such would be best applied to a corpus sampled at the word level: any section of text beyond this is going to exhibit 'clumpiness' effects that are beyond the comprehension of the model.

The prevalence of BOW models is such that many people choose to phrase their objections in terms of the accuracy of binomial models of language frequency\cite{kilgarriff2005language,evert2004simple,evert2007zipfr}.  Undoubtedly they have a significant point---more complex LNRE models are capable of far more useful inferences, however, it's worth noting that only a model that can integrate the linguistic influence of word 1 upon word $2000$ in a sample will truly justify a $2000$-word sample\footnote{This is a quantitative form of Hoey's argument that whole texts are necessary to truly describe human expectations of word use.}.

%--

One part of the problem is caused by a fixation on word lengths. A given corpus, $100,000$ words in length, may comprise just $50$ texts. For the purposes of many analyses involving person-person variation, it can be said that we really only have $50$ data points. Though care if often taken to sample these texts broadly, the idea of targeting a corpus in terms of its word length, rather than the number of samples, leads to extremely poor suitability for analysis of many small-scale features.

The problem of quite what sample size to choose is a trade-off: if we were to sample single sentences for our $100,000$ word corpus, we would soon require thousands of samples, each from a randomly selected and carefully-stratified source. If we wish to perform a complex analysis of narrative structure within those sentences, we'd find there isn't sufficient data.

Except in certain cases, there is a plateau of difficulty for sample size: sampling $2000$ words from a book incurs very similar levels of practical obstacles than does sampling $100$. Sampling units should be chosen in accordance with the complexity required by researchers of the time---for example, those working in information retrieval and NLP fields will demand relatively large sample units by comparison to many researchers in linguistics.

Corpora should, ideally, be defined in terms of the number of datapoints, rather than words.  This is one area where compatibility with existing corpora and techniques is arguably damaging to the final result, and where documentation should be clearer in guiding valid use.



% \paragraph{conventionalised to a weird/large number}
% % (2k) (biber)
% As mentioned above, the sampling unit for general-purpose corpora is conventionalised to a figure of 2000. This figure has undergone almost no refinement since it was originally determined by the brown designers, and has (again, as mentioned above) been reinforced by biber's determination that it is sufficient to represent small-scale grammatical features.

% Though 2000 words is far from an unsuitable selection, people's targeting of word lengths, and use of word-frequency analyses, leads me to believe that it should be significantly shorter in order to maximise the utility of a corpus.



% \paragraph{heterogenous between corpora }
% (see comparison.  as mentioned there, this isn't much of an issue really)



% \paragraph{"clumpiness" of language is not considered}
% see discussion on "too large" above.  this is merely another way of phrasing it.


% \paragraph{not formally specified or justified using semantic models}
% as discussed in "too large", the sampling unit should be selected according to, ideally, the internal properties of language.  philosophically speaking, language is a set of conventionalised symbols with which we may communicate with other people---the limit beyond which one word no longer meaningfully associates with others should be the point at which we draw the sampling unit.

% given context of a document, this may be un-nervingly large.  strictly speaking, it probably encompasses the whole history of the universe (in a manner similar to bayesian priors), however, that's only if we reason ad nauseum.

% a more practical limit, at least until neurolinguistics establishes the latter in a meaningful manner, is to sample at the maximum size for which we have models of language that may practically be applied.  for many black-box nlp techniques, this means we wish to have large sampling units in order to tease apart subtle interactions.  for many manual inspections of corpora for [especially qualitative] linguistic research, we will want to select a shorter sampling unit.



% \subsubsection{comparability}

% \paragraph{sampling unit}
% see also 'sampling unit'
% the differing size of the sampling unit between corpora renders many of them incomparable.  this is a fairly minor point, since many use the brown-standard 2k-word unit, chosen randomly from the text.

% \paragraph{poor documentation of intent, sampling policy, population bounds}
% see 'population'

% in order to perform scientifically meaningful comparisons, one must be sure that the populations between corpora differ in known ways.  though it is often possible to identify corpora that claim to be comparable in this manner, and use strikingly similar selection policies due to this, the smaller undocumented assumptions made by builders often cause minor differences, which may be crucial to certain forms of inquiry.

% prime examples of this are sampling from index lists compiled by different companies, or using different cultural definitions of formality.  more with some proper examples...

% the issue with this is not that corpora are very heterogenous and likely to be incomparable at a conceptual level, but that the lack of documentation on things such as the objective bounds of strata (see stratum selection) makes reasoned comparison by a user of the corpus impossible.















% -----------------------------------------------------------------
\subsection{Validity Concerns in Corpus Analysis}
One of the primary reasons for using quantitative methods in research is their objectivity and empirical basis.  This is especially true of linguistics, which seeks to generalise about a social property that is difficult to quantify or relate to other users.

In addition to the above procedural concerns, manual inspection and summarisation of corpora (or collections of features extracted by corpora) often leads to situations where errors of human judgement may imply certain findings.  These cognitive biases are widely recognised in many cases, and have been identified in other fields as common causes of error\cite{jain2012does,lieberman2009type}.

Many of these biases are fairly minor, and scientific methods are designed to counter-act their effect.  Nonetheless, qualitative analysis, poor corpus design, and presentation of certain features once extracted for inspection, are possible cause.

% \til{Until I decide to keep this section, here's an informal list:}

\begin{itemizeTitle}
    \item[Insensitivity to Sample Size] People are liable to underestimate variation in small samples.  Manual inspection of particularly intricate features extracted due to their grammatical form, especially from subsamples of the corpus (such as the spoken part) are likely to qualitatively imply false results\cite{rabin2000inference}.

    \item[Clustering Illusion] A tendency to find patterns where theye are none (aphophenia).  This is more of an effect when seeing larger data sets, such as when inspecting frequency lists or concordances.

    \item[Prosecutor's Fallacy] Assuming conditional probabilities indicate unconditional ones, and vice-versa.  This might be less prevalent, but when conditioning on a certain feature, subsample of the corpus, etc. it is common to assume that the trends identified are indicative (or anti-indicative) of similar trends in the rest of the corpus.

    \item[Texas Sharpshooter Fallacy (post-hoc theorising)] Testing hypotheses on the data they were derived from, i.e. inspection then derivation then testing.  Also called 'data dredging', this is a risk of large-scale community reuse of corpora.  A solution to this is to reuse sample \textsl{designs}, but not the actual data itself, fetching new data each time.

    \item[Availability Heuristic] The tendency for people to mentally overestimate the probability of features which they can immediately recall examples for\cite{tversky1973availability,schafer2014focused}.  This effect is likely to occur when examining corpora qualitatively, particularly if features are returned in response to searches on particular features.
\end{itemizeTitle}

Many of these concerns are difficult to address without fully quantifying or automating analysis stages that are currently performed manually, something that is often a technical challenge.  Though reduction of their incidence is the goal of a corpus builder, there is often little that can be done directly prior to analysis.


% see notes... 
% 
% \subsubsection{Categorising Reference Corpora}
% \til{Discuss the problem of meaningfully separating parts of a corpus for subsampling and description.}
% \begin{itemize}
%     \item Comparison to other corpora
%     \item Balance, breadth and generalisability (inc. efforts to establish standards for these)
%     \item Supcorpora, special-purpose-from-single-purpose
%     \item Parallel and comparable corpora
% \end{itemize}
% \subsubsection{Dissemination}
% \til{Descriptions of how to best spread corpora and the importance of sharing.  Cover the single-sample problem, open source corpora, twitter+news redactions, etc}
% 
% 
% \subsubsection{Balance}
% \til{Critiques of attempts to balance corpora against language (genres, special purpose corpus selection) and against one another (parallel corpora, brown, ae06/be06)}
% \subsubsection{Replicability and Reliability}
% \til{Linguistic reviews on `scientific issues' that do not centre on issues of text selection.  Include Open source corpora here, if they are not to be in the web section.}
% 
% 







% \til{The review of literature begins with a discussion of the current state of affairs w.r.t. sampling in corpus linguistics: how do people go about acquiring corpora, what their motivation and rationale is, and what people are currently developing in terms of sampling methodology.  I'll attempt to include some historical rationale too here.}
% 
% \subsection{Criticism of Current Data}
% \til{Here I will go on to discuss the failings of current widely-used lexical resources from the standpoint of their common use --- the section will focus on the representativeness of studies based on corpora as a method of commenting on their quality, and it's therefore necessary to draw the distinction between}
% \begin{itemize}
% 	\item General purpose corpora, and;
% 	\item Specialist corpora (and therefore some methods people use to build them).
% \end{itemize}
% 
% 
% 
% \subsection{Current Discussions of Representativeness}
% \til{Here I'll focus on work that is in the same vein as the thesis, i.e. how can we go about improving things?}



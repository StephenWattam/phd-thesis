

The field of linguistics is one concerned with description and formalisation of a particularly ethereal social concept.  The paucity of philosophical agreement upon the nature of language has led to many different approaches being taken through the years, many of which have accomplished great things in advancing our capacity to reason about, and derive conclusions from, language.


The most obvious method for inquiring about the nature of language seems to be to sample real-world use.  This process is followed in many other sciences concerned with social phenomena, and offers a tried-and-tested methodology for inferring results.  It is perhaps unsurprising, then, that this method has been used to a varying degree by many linguists throughout history.

In their 2001 book~\cite{macenery2001corpus}, McEnery and Wilson characterise the pre-Chomskian linguistic inquiry as primarily taking this form:

% page 2
\begin{quote}
The dominant methodological approach to linguistics immediately prior to Chomsky was based upon observed language use.
\end{quote}

They point to a number of studies using systematic analysis of samples to make their conclusions~\cite{kaeding1897haufigkeitsworterbuch,preyer1889mind,stern1924psychology,eaton1940semantic,west1953general}, and decribe Chomsky's influences on the field, which served to lead it to all but abandon corpus techniques in the 1950's, as a turn towards explanatory, rationalist theories of language.

These rationalist theories were often verified using experimentation or elicitation, in an effort to gather data that is detailed and reliable.  It's arguable just how valid and philosophically defensible these methods are, especially given the nature of language as a part-mental, part-tangible concept.%\td{rewrite this paragraph}

% These early corpora are of a quite different form to those of the `modern era', and will not be discussed in any great detail here except for historical context.


In a pre-digital world, collection and analysis of large-enough-to-be-useful samples of language was extremely difficult, making this rationalist approach a useful alternative.
Small samples are fundamentally unable to reveal some of the details examined by the structuralists, and construction and analysis of sufficiently large corpora in a non-digital era would prove a practical hard limit on the power of corpus studies.  To frame the focus on rationalist inquiry as an alternative to the empirical is a disservice to both: empiricism had supported the rationalist theories of the 50's, and would itself go on to be supported in turn as it once again rose to prominence.

The revival of corpus linguistics may be attributed almost solely to the availability of programmable computing machinery.  This offered a solution to the problems of scale encountered during earlier corpus analyses, making empirical data once more viable for detailed inspection of language.

This revival, often termed the `modern era' of corpus linguistics, has yielded what we would commonly call a corpus today: a large, machine-readable, annotated collection of texts sampled in order to represent some population of real-world language use.

Corpus studies are now widely relied upon across linguistics, and are often the method chosen to test theories derived from structuralist approaches.  This may be seen as a validation of their methods, as the two complementary philosophies of scientific inference are once more able to use one another without methodological suspicion.

For the purposes of this section I will be focusing on the design of `modern' corpora with respect to their use in validating linguistic theories, and for training automated Natural Language Processing (NLP) and information retrieval systems.  For this reason I will be covering mainly general purpose corpora: those that purport to represent such a large population as to cover a whole language.  This `style' of corpus is built so as to be useful to many research questions and researchers, in part to dilute practical issues surrounding sampling.  This is in contrast to special-purpose corpus, which are designed to represent a restricted context.  Special-purpose corpora may be selected according to demographic or linguistic properties, and are typically much smaller.  Because of this they are often built for a given study, or by re-sampling a general-purpose corpus.


% \til{More focus on sampling/representativeness}

% The collection of large volumes of real-world language is one method of increasing the empiricism of this process, and as such has been followed as a parallel stream of inquiry for a long time: [mention 1870's corpus studies, 1950's inquiries].  





% --

% 
% 
% 
% The use of corpora for linguistic analysis is a long one---one justification for this is that the alternative to using some kind of corpus is either to manually seek evidence for a linguistic feature (something that very easily leads to pseudoscientific, unfalsifiable theories), or to inspect the "idea" of language that a native speaker (or speakers) posesses.
% 
% Whilst, doubtless, much good work has been done using these alternatives, they lack the objective and empirical epistemological possibilities that define the scientific process.  Examination of one's idea of language is likely to be influenced by the knowledge of a linguist, and seeking evidence for a theory in a language with unknown or poorly-defined bounds is likely to yield it regardless of the reality. % when observed from other contexts
% 
% Corpus methods, then, free linguistics from the alchemy of human understanding, providing a convenient empirical truth against which we may assess hypotheses.
% 
% The value of any inferences evidenced by this empiricism is, however, limited to the extent to which a corpus is associated to the language upon which we wish to comment.  It is this requirement that has raised most objections, for the concept of language is little understood and poorly defined even where people claim to understand it.  Regardless of controversies over form, it is accepted that any corpus representative of useful portions of a language must be very large indeed (though the definition of 'large' is also debated!).
% 
% Until recently, the task of gathering, processing, and inspecting large volumes of text was arduous enough to prevent its widespread appeal: some efforts were made before the era of computerisation, %CITE
% , for example, did X etc \til{mention 1870s work, 1950's popularity}..% and there was a time when corpora were rather popular even without the benfits of computation
% 
% The introduction of programmable computing machinery, and (fast) electronic storage, opened the floodgates for easy, large-scale analysis of text.  This brought the second\td{does that mean we're on the third given its lull, or still second?} wave of corpus linguistics... \textsl{~~wavey wavey screen fades into next section like a flashback on 60's TV~~}
% 



\subsection{A Brief History of Modern Corpora}
The Brown Corpus of Standard American English~\cite{francis1961brown}% http://icame.uib.no/brown/bcm.html
is widely regarded as the first of the modern age of corpora.  Built in the 60's, Brown's corpus was the first electronic-format general purpose corpus and was roughly one megaword in size.  % 1,014,312
It contains 500 samples, each roughly $2,000$ words in size, that were taken to represent a cross-section of works published in the United States in 1961.  The proportions and sizes of samples were selected in order to trade off pragmatic concerns with the possible kinds of analysis that could be performed at the time.

The `Standard' in its name referred to Kucera and Francis' intent that the corpus represents their judgement of `standard' English use.  Brown became a \textsl{de facto} standard for American English, and the design was carried forward into many other corpora, mostly regional versions designed to be comparable to Brown\cite{hundt1999manual,johansson1986tagged,hundt1998manual,shastri1988kolhapur,collins1988australian,bauer1993manual,mcenery2004lancaster}.  In order to maximise the value of comparisons within studies, other general purpose corpora chose to mirror Brown's sampling policies.

% --- 

\begin{table}[Ht]
    \centering

    \begin{tabular}{llrr}
    \hline
    Categories & Texts in each category & American corpus & British corpus \\ \hline
    A & Press: reportage & 44 & 44 \\
    B & Press: editorial & 27 & 27 \\
    C & Press: reviews & 17 & 17 \\
    D & Religion & 17 & 17 \\
    E & Skills, trades, and hobbies & 36 & 38 \\
    F & Popular lore & 48 & 44 \\
    G & Belles lettres, biography, essays & 75 & 77 \\
    H & Miscellaneous & 30 & 30 \\
    J & Learned and scientific writings & 80 & 80 \\
    K & General fiction & 29 & 29 \\
    L & Mystery and detective fiction & 24 & 24 \\
    M & Science fiction & 6 & 6 \\
    N & Adventure and western fiction & 29 & 29 \\
    P & Romance and love story & 29 & 29 \\
    R & Humour & 9 & 9 \\ \hline
      & Total & 500 & 500 \\ \hline
    \end{tabular}

    \caption{The basic composition of the British and American corpora}
    \label{table:litreview:corpora:lobdist}
\end{table}


The Lancaster-Oslo-Bergen (LOB) corpus~\cite{johansson1986tagged} was built as a British counterpart to Brown.  
It uses the same stratification and sampling strategy (with one or two more texts in certain categories) and thus comprises roughly a megaword of British English, as published in 1961.  % See  http://clu.uni.no/icame/manuals/ Table 1 for a table of comparisons
Though the manual does note:

\begin{quote}
    The matching between the two corpora is in terms of the general categories only. There is obviously no one-to-one correspondence between samples, although the general arrangement of subcategories has been followed wherever possible.
\end{quote}

Table~\ref{table:litreview:corpora:lobdist} shows the proportions of texts in each genre, relative to Brown (the `American corpus'), as reproduced from the corpus manual.

% ---
% \til{ Not 100\% sure why I mention LLC here. }
% http://khnt.hit.uib.no/icame/manuals/londlund/index.htm
%  The corpus consists of 100 texts, each of 5000 words, totalling 500.000 running words of spoken British English. Information about the compilation of the corpus and explanation of the symbols (prosodic, phonetic, etc.
The London-Lund Corpus~\cite{greenbaum1990london} was released in 1990, and contains transcribed spoken text, annotated with a number of different markers to indicate intonation, timing and other extra-textual information.  
The corpus consists of $100$ texts, each of $5,000$ words, totalling $500,000$ running words of spoken British English.
The annotation scheme used in LLU is much more in-depth than that in many written corpora, reflecting the different nature of research questions for speech.



% --- 
Collins' requirement for a corpus upon which to base their dictionaries spawned the COBUILD and its `representative' subset, the Bank of English~\cite{Jarvinen1994AMW991886.991985,sinclair1987looking}.  COBUILD uses a slightly different approach to corpus building: that of the monitor corpus.  Monitor corpora are continually added to, using a fixed sampling policy but an ongoing sampling process.  At the time of writing, the BoE is 650 million words in size (The whole COBUILD family used by Collins is 4.5 billion)\cite{collinswebcorpus}.

The approach taken by the BoE opens many possibilities for analysis of language over time (something also covered by diachronic/historical corpora using more conventional sampling).  Even so, such comparisons are complicated by the irregular additions (c.f.\ diachronic corpora), and this remains the only major corpus built in this fashion prior to automated web retrieval.


% ---





The de-facto standard of the day is currently the British National Corpus, which comprises $100$ million words of British English\cite{leech1993100}.  The BNC's design was influenced heavily by discussions on corpus building that centred around creating a standard, reliable approach to taxonomy, sampling and annotation that occurred around the early nineties.

The BNC aims at being a synchronic `snapshot' of British English in the early 1990s.  It consists of samples of text up to $45,000$ words each, and is deliberately general-purpose, containing a wide range of genres as well as a sizable spoken portion.  It was released in 1994, but has since been re-coded and augmented, particularly notably by Lee~\cite{lee2003bnc}, who constructed a significantly more detailed (and principled) taxonomy for its texts in 2003.

Lee's genre taxonomy is outlined in Appendix~\ref{sec:appx:sample}.  Lee's decisions to code genres using this system were partially based on fostering compatibility with the ICE-GB\cite{greenbaum1996international} and LOB\cite{johansson1986tagged} corpora.

The prominence and ``whole population'' coverage of the BNC's sampling frame spawned many compatible corpora, though these often show more variation in sample design than the Brown clones.  Xiao, in his survey of influential corpora, notes the existence of national reference corpora for American, Polish, Czech, Hungarian, Russian, Hellenic, German, Slovak, Chinese, Croation, Irish, Norwegian, Kurdish, and many more\cite{xiaoz2008}.  \td{mention BNC2014 if I can work out how to cite it}

% 1.2 General definitions ( from http://www.natcorp.ox.ac.uk/docs/URG/BNCdes.html )
% 
% The British National Corpus is:
% a sample corpus: composed of text samples generally no longer than 45,000 words.
% a synchronic corpus: the corpus includes imaginative texts from 1960, informative texts from 1975.
% a general corpus: not specifically restricted to any particular subject field, register or genre.
% a monolingual British English corpus: it comprises text samples which are substantially the product of speakers of British English.
% a mixed corpus: it contains examples of both spoken and written language.


% ---
The rise of electronic communications has led to a reduction in the effort required to gather corpus data.  This has resulted in a great increase in the number of special-purpose corpora built for specific studies~\cite{westlabusenet2013,baroni2006building,Mair20060101T00000009215034355}.  These corpora are more focused in that their construction methods restrict them to electronic data, yet  their large sample size may make them suitable for the study of smaller-scale features in a more general context.


% Move? [These corpora are often less widely used, and serve to illustrate one extreme discussed below, that the purpose of a corpus is important to its construction]

This thesis is focused on sampling using automated, technical mechanisms to overcome some of the challenges facing conventional methods, and as such focuses on web-based methods (known as `Web as Corpus').  WaC is concerned with sampling the web itself, as well as constructing samples that are representative of other data, yet are retrieved primarily from the web.  This approach, and those using similar methods, are covered below in Section~\ref{sec:litreview:newtech}.






















% ---------------------------------------------------
\subsection{What Makes a Corpus?}
The use of general purpose corpora as large monoliths, reused in many studies and systems, has led to much debate over the nature of a corpus.  This, as we shall see throughout the thesis, is a question unworthy of concrete answers---each purpose will exert certain demands upon corpus design criteria, and any widely-used corpus is likely to be a compromise around these.

This section exists primarily to define a corpus as used by the author: it is unlikely that the definition derived below is universal, however, it is designed to reflect most use-cases, across corpus linguistics and NLP.

% This discussion is separated to cover a number of properties of corpora, and their parameters.  These 



In order to establish the important traits of corpora, it is wise to have an understanding of the motivation behind their existence.  Corpus methods are generally contrasted against two other methods of linguistic investigation: direct elicitation from a language speaker, and directed research into a linguistic feature under controlled conditions.  Both of these pose significant scientific challenges---both are reliant on at least one linguist's intuitive view of language (one that could hardly be said to be representative of most language users), and both require the acquisition of data without its usual context, something that is especially difficult given the varied and context-dependent complexity of language.

Corpora provide limited solutions to both of these issues.  In the former case, they provide an objective record of linguistic data that is free from all but the initial builders' influences (which, in the ideal case, may be documented and provided along with the data itself).  In the latter, they are as portable as any large volume of text, and may be annotated with context sufficient for a given linguistic task.


%------
\paragraph{}
At its most basic level a corpus is a sample of text.  Given research questions surrounding a body of text, it is perhaps necessary only to stipulate that a corpus must be represent known population\cite[p. 22]{mcenery2001corpus}:

\begin{quote}
\ldots{}a body of text which is carefully sampled to be maximally representative of a language or language variety.
\end{quote}

Further to this, the modern definition of a corpus has undergone a series of significant refinements thanks largely to the increase in both the ubiquity and power of computing machinery.  Corpora are, with very few exceptions, electronic (with an increasing number documenting texts of electronic origin), multi-modal (covering a wide variety of methods of communication and their linguistic features), and annotated with linguistic data.

Many authors go further by stating that a corpus should be machine-readable, annotated with information useful to linguistic inquiry, built for a specific purpose or methodology, available for use in other studies, finite in size or stratified to provide multiple possible analysis methods with internally valid data\cite{bennett2010using,bowker2002working,leech1992corpora}.
%The majority of present-day corpora are “balanced” or “systematic”. This means that the texts are collected (“compiled”) according to specific principles, such as different genres, registers or styles of English (e.g. written or spoken English, newspaper editorials or technical writing); these sampling principles do not follow language-internal but language-external criteria. For example, the texts for a corpus are not selected because of their high number of relative clauses but because
Whilst I do not consider many of these to be requirements for a scientifically useful corpus, they may contribute greatly to corpus utility due to their alignment with common methodologies and uses.  There is undoubtedly a case for this---the utility of a corpus is often limited by its format, however, the extent to which this applies varies wildly by purpose makes it unreasonable to include many resources missing some of the above under the term ``corpus''.


%Awesome quote from http://www.linguistics.ucsb.edu/faculty/stgries/research/2009_STG_CorpLing_LangLingCompass.pdf
%However,
%since corpus data only provide distributional information in the sense mentioned earlier,
%this also means that corpus data must be evaluated with tools that have been designed to
%deal with distributional information and the discipline that provides such tools is statistics.
%And this is actually completely natural: psychologists and psycholinguists undergo comprehensive
%training in experimental methods and the statistical tools relevant to these methods
%so it’s only fair that corpus linguists do the same in their domain. After all, it would be kind
%of a double standard to on the one hand bash many theoretical linguists for their presumably
%faulty introspective judgment data, but on the other hand then only introspectively eyeball
%distributions and frequencies. 

This review, then, shall start with the most basic of definitions, that of `a body of text sharing some property that may be interrogated for some linguistic information'.  This takes into account the claims of generalisability that cannot be made for more haphazard collections of texts (often called \textsl{libraries} or \textsl{archives} with no relevant common features) which are not demarked by the boundaries of some notional property.










\subsubsection{Representativeness} % and Transferrability
Representativeness is, in effect, the goal of any sample.  It is the property that I chose to use as the loose starting condition above, and it is to be maximised by selection of those properties covered below.  By virtue of this holism, it is also dependent upon enough factors to be poorly defined in the literature.
% (by virtue of the difficulty of doing so).

The concept of representativeness is based strongly on philosophies of inference, and epistemology in general.  These are highly dependent on correlation in variables external to those measured by a sample, and any rigorous definition will involve the properties we wish to generalise, the purpose of such generalisation, and the population we wish to generalise about.  Users of reference corpora (those general-purpose corpora designed to represent a whole language) may find, for example, that their claims to validity are wildly variable compared to previous studies, simply due to the nature of their research question.


Much has been written on the subject of representativeness\cite{biber1993using,varadi2001linguistic,varadi2000corpus,leech1992corpora,rapp2014using}, both in linguistics and in other fields that are dependent upon complex sources of data.  As with psychology or sociology, the opacity of the mechanisms that generate the object of study is such that significant philosophical disagreement as to the underlying nature of language occurs.  This disagreement has, in many ways, limited efforts to formalise and reason about representativeness in corpus design: taken quantitatively, one person's adequate corpus is another's woefully biased one.

The LOB manual hints at the fluid nature of ``representativeness'' in corpus linguistics, and the degree to which corpus design is expert-guided\cite{johansson1986tagged}\td{can only find online version, no page nums}:
\begin{quote}
The true “representativeness” of the present corpus arises from the deliberate attempt to include relevant categories and subcategories of texts rather than from blind statistical choice. Random sampling simply ensures that, within the stated guidelines, the selection of individual texts is free of the conscious or unconscious influence of personal taste or preference.
\end{quote}

I consider this a false dichotomy: a high quality random sample is essentially the gold standard to which expert designs should be aspiring, and both are aiming for the same notional goal.  The key benefit to random sampling is, as observed above, the immunity from `unconscious' bias\cite{barnett1991sample}. %p.19

This ambiguity could be seen as an argument against the concept of a general purpose corpus, however, current progress indicates that this would be hasty: whilst special purpose corpora are often burdened by fewer procedural and pragmatic difficulties, they are necessarily limited in scope.  It is still rational and useful to identify speakers of a language as a homogeneous group at some level, especially for smaller linguistic features.  On the other hand, only a sample that properly encompasses a large amount of variation may be used to describe many effects of interest.

With this in mind, many of the studies into representativeness have focused on examining existing corpora, with a view to testing their internal variation against some known re-sample.  This approach has been taken most famously by Biber~\cite{biber1993representativeness}, who performed a series of studies in which he compared the content of 100-word samples from the LLC and LOB corpora.  Each sample was paired with another from the same text (LOB samples points are $2,000$ words each, and LLC's are $5,000$).  Biber went on to extract some deliberately small-scale linguistic features from each sample, before examining the difference in frequency between each.

% \til{more detail on this paper, since it's so crucial}

Biber concluded that existing corpora were sufficient for examination of smaller linguistic features, however, in the process he saw fit to reject the notion of representativeness used here (and, notably, everywhere else)\cite[p. 247]{biber1993representativeness}:

\begin{quote}
Language corpora require a different notion of representativeness, making proportional sampling inappropriate in this case. A proportional language corpus would have to be demographically organized (as discussed at the beginning of Section 3.2), because we have no a priori way to determine the relative proportions of different registers in a language.
\end{quote}

Biber's argument is that we should not aim for any degree of proportional representation of linguistic features, for this would produce a corpus that is mostly one kind of text (due to a conjectured Zipfian distribution of text types).  In many ways, this is an argument reflected by stratified sampling with artificially boosted proportions, however, stratification of samples (rather that with adjusted weights) is normally performed as a compromise, where we are aware from previous sampling efforts that we cannot adequately sample randomly.  Biber's presentation of this idea has seemingly led many to reject the common wisdom of sociological sampling, leading to corpus building to be seen as a fundamentally new activity, and consequently as something of a black art.
% black art within the community.


%-----
A further aspect clouding the waters of quantitative representativeness assessment is disagreement over how to parsimoniously stratify language.  This is in part due to the difficulty in defining a taxonomy for genre\cite{lee2001genres}, which is often the most controlled variable within a general purpose corpus' sample design.  Ultimately, I consider that the answer to this is, as mentioned in the LOB manual, down to the individual research question: a corpus sufficient to represent one feature may easily be massively biased for those with greater variation in the population.

%-----


In part for the reason that Biber's work was taken early on as a validator of current practices in corpus building, the notion of representativeness has remained an almost entirely philosophical concept within corpus linguistics.% \td{mention efforts to establish it automatically, but retain a working definition here so more discussion may be freed up later}

% "There is a broad consensus among the participants in the project and among corpus linguists that a general-purpose corpus of the English language would ideally contain a high proportion of spoken language in relation to written texts. However, it is significantly more expensive to record and transcribe natural speech than to acquire written text in computer-readable form." -- (http://www.natcorp.ox.ac.uk/docs/URG/BNCdes.html / composition)


%----

% 
%  TODO: put me in the ideal corpus stuff.
% The concept of transferability % taken from sociology
% is an alternative often applied to qualitative analyses by those in the social sciences \td{find introductory citation, Dornyei?}.  It describes the likelihood that findings may apply to other members of a given, defined, population, without speculating as to the probability that a member of said population may be suitable for such a comparison.  This concept allows us to find evidence-based theories which hold true for groups specified by prior knowledge of the sample and human judgement of its relationship to the population.
% 
% It could be said that, prior to corpus methods, the best linguistics could hope to achieve was qualitative transferrability.  Indeed, some may argue that the theoretically infinite population of utterances language makes possible means that any sample is incapable of being representative of language as a whole, and that we are only able to generalise to observations of language [within finite time periods].  
% 


The BNC, though often relied upon as a reference corpus, makes a number of bold assumptions regarding representativeness---to the point of explicitly stating its deviation from being a solid representation of its population\cite[p.6]{lou1995users}:

\begin{quote}
There is a broad consensus among the participants in the project and among corpus linguists that a general-purpose corpus of the English language would ideally contain a high proportion of spoken language in relation to written texts. However, it is significantly more expensive to record and transcribe natural speech than to acquire written text in computer-readable form. Consequently the spoken component of the BNC constitutes approximately 10 per cent (10 million words) of the total and the written component 90 per cent (90 million words). These were agreed to be realistic targets, given the constraints of time and budget, yet large enough to yield valuable empirical statistical data about spoken English.
\end{quote}

This implies that the corpus should not be taken as a single sample at all, or that large adjustments should be made during analysis to avoid generalisations on purely quantitative bounds.

A similarly pragmatic approach is taken to the sampling of written material according to production and reception statistics, with a balance being brought between the two based on publication, library lending, and magazine circulation statistics.  Further, some samples were taken purposively\cite[p.10]{lou1995users}:

\begin{quote}
Half of the books in the ‘Books and Periodicals’ class were selected at random from Whitaker's Books in Print 1992. This was to provide a control group to validate the categories used in the other method of selection:  the random selection disregarded Domain and Time, but texts selected by this method were classified according to these other features after selection.
\end{quote}

The spoken portion contains a `demographically sampled' portion (comprising 50\%), so called because it makes an attempt to represent speakers according to their sex, age, and social class.  Individuals were randomly distributed, and each provided recordings of their conversations over a two to seven day period.  The major limitation noted here is the short period of time, and thus the low probability of capturing certain, rare, interactions on tape.  In order to remedy this, half of the spoken component was devoted to genre-based stratification, with the intent being to capture a greater breadth of data.



% \til{ 
%     Mention balance? esp. in BNC demographic spoken portion
%     % Mention sampling frame selection.  Describe BNC, relate to others using Xiao's tables.
% }
% % \til{Also, refer to the books blow-by-blow (use notebook + quote)}

\til{Include NLP representativeness stuff?
\\
So, I definitely found some papers back in 2013, but can't dig them out now.  Is this todo droppable?
}








\subsubsection{Size}
% \til{This is the primary driver of the above, no-one really pins this down but survey some corpora and some more modern approaches (Kilgarriff's google-ology comments at the end would be good.  Link to web corpora for extreme bigness and ref to lower sections at end}

The size of any sample is a crucial and often hard-to-determine property, and corpora do not differ in this respect.  In many ways the question of corpus size is a primary constituent of the representativeness property above, and though it has long been recognised as such there remains little consensus on just how large is large enough.

This disagreement is in part because language exhibits a number of properties that make it hard for us to gauge variability in the population, meaning that most sample size estimation methods (which rely on random sampling) are poorly suited.

%-

The first of these is the inability to accurately measure the complexity of language, which, as used and applied by humans, has unknown degrees of freedom.  This effect applies itself at many levels:

\begin{itemize}
    \item Lexicon---Vocabulary, even when restricted to a given demographic, time period or person, is difficult to define with any certainty.  Many people are capable of recognising entirely novel words, simply by virtue of their context or morphology (and the inverse, e.g.\ \textsl{Jabberwocky}).
    \item Syntax---The meaning of words and phrases is heavily context based, but the context affecting each aspect of language is variable and, in some cases, wide-reaching.  This makes it hard to determine if we should be sampling $2,000$-word texts, single sentences, or the whole thing\cite{hoey2005lexical}.  This unknown sampling unit drives one of the key trade-offs in corpus design, as a 1-million word corpus comprised of single sentences will be capable of embodying more variation than will one made of two $500,000$-word samples.
    \item Semantics---Variation in our understanding of language in context is poorly understood.  This is the driver behind any models of the above, but also affects how we should sample external variables such as socioeconomic factors and even the direct circumstances in which a text is used\cite{sinclair1991corpus,Stubbs19950101T0000000929998X23}. % sinclair p70 for collocation stuff
    %\item Discourse---
    %\item Pragmatics---
\end{itemize}

This ambiguity produces a tradeoff that has been identified by relatively few in the corpus-building community\cite{evert2006random,kilgarriff2005language,gries2011corpus}\td{Which ref from Baayen?  The one I have in bibtex doesn't seem suitable}% \td{(notably biber, Greis, Evert with the library)}
---that corpora of equivalent size may yield significantly different inferential power due to their differing number of data points.  This may be seen as an issue of depth vs.\ coverage: corpora including large snippets of text are capable of supporting more complex models and deeper inference, at a cost to the generality of their results.

There remains disagreement upon the extent to which these two aspects should be traded off, though it is notable that the NLP and linguistics fields vary greatly in their treatment of the data, with linguistics typically focusing on frequency and immediate collocation, and NLP being skewed towards more complex, instrumentalist, models.%\td{cite? how?}
I would suggest that, realistically speaking, researchers should be examining their experimental design with respect to the size and/or complexity of the features they are working with in order to select a corpus that matches most closely.  It is also quite clear that this does not happen: the BNC contains a small number of large samples, yet is often used to analyse small-scale linguistic features.

% \til{mention Biber's contribution in passing, and say how this may also be seen as a property of the clumpiness of language and that work on dispersion is able to help somewhat}

%- 

Secondly, establishing the boundaries of a given language is difficult.  Users of a general-purpose corpus wish for two conflicting properties to be satisfied---the population must be wide enough to provide a useful set of persons about which to infer (and, more loosely, this should align with those persons we can say informally, for example, `speak English'), yet the corpus must provide sufficient coverage to represent that population in the first place.

Generally it seems that this problem, though having been recognised, has received too little effort for practical reasons.  Issues of balancing demographics in corpus design have typically taken a curiously detached form, that of selecting a sample of language as a proxy for demographics (e.g.\ selecting the bestsellers over unpopular books or sampling more popular newspapers).


% This concept has extended itself to the point where corpus building is seen as a problem phrased in terms of which texts to select, rather than how one might select them.  
This approach has led to a situation where each and every general-purpose corpus carries with it the expert judgement of linguists not only in selecting a wide variety of texts from within the population, but also their socio-linguistic opinions.  Sinclair makes this explicit in ``Developing Linguistic Corpora'', where representativeness is described in entirely subjective terms\cite[p.5]{wynne2005developing}:

\begin{quote}
A corpus that sets out to represent a language or a variety of a language cannot predict what queries will be made of it, so users must be able to refer to its make-up in order to interpret results accurately. In everything to do with criteria, this point about documentation is crucial. So many of our decisions are subjective that it is essential that a user can inspect not only the contents of a corpus but the reasons that the contents are as they are.
\end{quote}


This reliance on expert opinion to overcome practical challenges associated with text retrieval has lead to the sampling policy being somewhat opaque to end users.
Those using a given corpus are not necessarily able to rigorously define about whom they infer a given result.  In practice, this manifests as a need to qualify results by reasoning about the likely impact of any ambiguity.


%-

Finally, disagreement on how we should extract features from language samples (i.e.\ which dimensions of variability are interesting) means that, aside from the immediate and obvious properties such as genre (about which there is arguably less agreement~\cite{lee2001genres,aston2001text,sharoffs2015}), any efforts to stratify language are met with suspicion.  This may lead to researchers performing corpus analysis using informally-subsampled general-purpose corpora, with questionable correspondence between the categories used to select texts and their research goals.
It is the author's opinion that this is unanswerable except for individual studies: in order to know variation in features affecting one's use of a corpus, it is necessary to define the covariates and evaluation strategy.

% This disagreement robs corpus builders of the power to perform true multi-phase stratified sampling, though it is worth noting that the efforts of the 90's led to a general agreement on how to differentiate between some of the major covariates.\td{rewrite and clarify}

%-

Further to these problems of defining the nature of the sample, there is the resultant problem of determining a sample size even where these are known.  

Taking the first of these issues in the extreme, it may be said that the only ``fully representative'' corpus must contain all context for each text (something that would include at best an abridged history of the world) in order to satisfy inquiry from many different perspectives.  Given the limitations in analysing properties of language beyond a given scale, and the clear impracticality of extending that scale's upper bound, it seems reasonable to conclude that current corpus efforts are sized so as to be useful for small-scale features, and that our inspection of language is currently somewhat shallow.

Taking into considerations problems of proportional, stratified sampling, it seems possible to establish a corpus size and composition that is widely agreed upon.  Nonetheless, issues of selecting a sampling unit mean that the resultant corpus may either be a refinement of current efforts (not necessarily a bad thing) or utterly colossal and beyond the capacity of even modern corpus processing systems.

Since sample size and composition are intimately related both to one another and to the concepts of representativeness and transferability, this issue will be one of primary importance to the rest of the thesis.





\subsubsection{Purpose}
% \til{examples of purpose.  General/special.  Cover qualitative issues and quantitative issues of parameter estimation from biased statistics.}
The reason for sampling a given population is a crucial feature of corpora.  Not only does it define the level and type of metadata available (and the arbitrary definitions used therein), the expert selection of sample designs means it has a large influence on the sampling frame used, and thus the validity of any generalisations made using said data.

The condition given by McEnery \& Wilson\cite{mcenery2001corpus} here is that a corpus must have been built with some degree of linguistic inquiry in mind, for example, the collected works of Shakespeare would be counted, but not one's personal book collection (even if it happens to include the complete works of Shakespeare and nothing else).

This distinction seems to be little more than a way of stating that one's ability to infer things from a corpus is relative to its external properties, which is true of any sample.

This requirement calls into question two things: the generality of a corpus, and the extent to which it is documented.

General-purpose corpora sample a large population, of interest to many users.  This requires that they remain fairly unbiased and cover a large amount of variation (which in turn necessitates very large sample sizes).  Since they are re-used many times, the quality of their documentation is key to their gross value---each user will require particular variables in order to generalise from the text.

Special purpose corpora avoid this challenge by being re-used for less disparate aims (and, generally less frequently).  Because of the relative focus, their documentation is often able to be significantly more detailed, allowing for deeper insight.  This approach, however, is not transferable to the sample sizes used in general purpose corpora.

To contextualise this, both examples above merely constitute special-purpose corpora, that offer answers to different research questions.  For one, we may answer those about the nature of Shakespeare's language use, whereas the other allows us to find knowledge about a given person's literary preferences.  It is of no direct consequence that more people are likely to care about the former.

If we extend the example to include others' book collections, the ability to generalise changes: we know significantly more about Shakespeare than many other authors, and it is thus possible to annotate the Shakespeare corpus with details of his life, times when works were written etc.  The same could not be said of a corpus where our aim is to investigate reading habits (even if some people exclusively read Shakespeare).

\paragraph{}
This `purpose' requirement can be phrased entirely in terms of external variables.
A group of texts about which nobody knows anything do not offer any opportunity for inquiry (except about themselves), and so it is reasonable to require documentation of the context in which those texts occur (or any external property that demarcates a homogeneous group).

By stating that a corpus must have a defined purpose, we include a set of reasonable assumptions about the corpus and its external properties: a corpus built for study of Italian newspaper text is unlikely to heavily sample the Guardian, for example.  In many ways, statement of purpose offers a shorthand for many decisions and assumptions inherent in construction of a large corpus, and a simple way to assess how well matched a corpus may be for another---related---purpose.

It is noteworthy, however, that selecting a corpus by its original purpose greatly complicates any subsampling that is possible---one must be able to subsample texts not only according to external data of interest, but also taking into account the original interactions and assumptions made by the constructors.  As mentioned above, it is unlikely that any corpus is capable of being documented `fully' enough to avoid these issues, however, this is a compelling argument for focus on detailed metadata being provided (rather than detailed rationales for sampling), especially in the case of general-purpose corpora.


% see http://www.natcorp.ox.ac.uk/docs/URG/BNCdes.html


%----


\subsubsection{Data Format}
Many authors stipulate that a modern corpus should be electronic.  To generalise this position, they require that it is in some way processable by machines, i.e.\ that it must be in some way regular.
This defines the format of not only the basic textual content, but also the availability of metadata at all levels (category/strata, document, word).

Both of these have been addressed to some extent, especially the problem of representing the text itself, which has largely been solved by UTF-8 at the character level, and XML/SGML at the markup level.  Standards derived from efforts such as the TEI\cite{ide1995tei} have some penetration, though there is often still the need to include proprietary extensions if other concerns on this list are to be maintained.

Though earlier work on corpus construction focused on data formats\cite{atkins1992corpus,EagTcwgCtypeaglespreliminary}, with a view to sharing corpora for local analysis, modern approaches tend towards corpora `as a service'\cite{hardie2012cqpweb,ferraresi2008introducing} --- providing a front-end to query and analyse text directly whilst still hosting it at the original institution.  This approach is largely taken to mitigate licensing issues, and to work around the high level of technical skill necessary to work with large-scale data.

This thesis takes the view that representation is largely a solved problem: UTF-8 and XML are both capable of storing international characters and complex annotations in a way that is easily mined for many uses, and advances in database technology and distributed processing offer many ways to process and retrieve structured data.




\subsubsection{Classification}
Efforts have been made to address the ambiguity of some often used strata such as text-type and genre.  One of the more notable was EAGLES~\cite{EagTcwgCtypeaglespreliminary}, which produced a number of recommendations designed to be applied to new corpora whilst retaining general compatibility with existing designs.


To some extent, the form a corpus takes is defined by its content---any meaningful taxonomy is arbitrary and theory-laden.  For this reason, recommendations made by EAGLES were driven by a review of the theoretical basis for existing taxonomies and work at the time\cite{EagTcwgCtypeaglespreliminary}\td{page unknown (online)}:

%CITE EAGLES http://www.ilc.cnr.it/EAGLES96/intro.html
% \til{Describe EAGLES briefly, but t'is not a main focus.  Quote below is from \url{http://www.ilc.cnr.it/EAGLES96/intro.html}}
\begin{quote}
Any recommendation for harmonisation must take into account the needs and nature of the different major contemporary theories.
\end{quote}

The need to provide useful metadata is particularly challenging for general-purpose corpora, which have very loosely-defined aims and must maintain a high level of neutrality whilst also proving useful to many people.



%--

Internal text distinctions may be drawn using bottom-up (often termed `corpus-driven') methods, however, these are frequently difficult to operationalise.  A prime example of this is Biber's multidimensional analyses of corpora\cite{biber1992complexity}, which focus on feature extraction and principal component analysis in order to identify primary dimensions of variation within corpora.

Biber's approach has been held aloft by many as one of the only `unbiased' analyses of variation around, however, this is not the case.  Without an authoritative underlying theory of language use by humans, any features used to construct the model will, however neutrally treated thereafter, exert pressure on the results.  Given the subtlety with which such analyses extract information (and the difficulty in interpreting resultant factors), this is likely to lead to favouring one set of conclusions over another.

This is also true of higher-level techniques such as latent dirichlet allocation (LDA), which is usually trained using token frequencies\cite{Blei2012PTM1338062133826}.  These straight frequencies are still entirely bereft of context, and tokenised using procedures that embody particular theoretical decisions such as the importance of punctuation\cite{pentheroudakis2006tokenizer} (especially true for languages lacking word delimiters, such as Chinese\cite{Wu1994ICT974358974399}).  

Sharoff\cite{sharoff2007classifying} presents a more transparent clustering methodology that relies on keyword metrics to identify salient features.  This provides some connection to other keyword literature, along with a mechanism for inspecting the resultant categories.

Arguably, further problems with the approach of finding natural strata of variation lie with sampling and linguistic problems: many of the practical issues surrounding corpus building involve enumeration of easily identifiable groups of texts, something that would be hard to compromise or `trade off' if working from factor loadings.  Further, many analyses will find subsampling data difficult when defined in terms of many external variables\cite{aston2001text}, though this is more of a challenge for the linguistic community (and providers of its tools).

%--

Through reasoned examination of existing efforts, Lee\cite{lee2003bnc}
was able to re-form the BNC's classification by adding external data and classifying documents manually.  This is arguably the most prominent effort to apply the multi-phase `examination and re-appropriation' method that Biber and EAGLES recommend.

His methodology, though labour-intensive, offers a defensible way to trade off the various interests of users.  One key aspect to this is the fact that it was built after the corpus, and thus may take into account common usage when making distinctions between texts---something that Lee relies on in an effort to define categories using `external' definitions (as opposed to Biber's internal variance measures).  This may prove more easy to operationalise in some circumstances, but still involves the subjective expert opinion of someone who may or may not agree with the user's perspective\cite{aston2001text}.

%nal \til{Describe taxonomy defining efforts and perhaps stratification in a way that is more tied to original corpus documentation, esp. stratification w.r.t. brown/brown-inspired corpora}


In summary, the problem of producing a meaningful and operationalisable taxonomy for corpus organisation is actually one of community agreement: for any given user of a corpus, the task is simpler.  This indicates that a transparent, well-documented approach should be taken in order to allow end users to decide upon their level of agreement with the pre-applied categories, or that differing levels of confidence should be applied to aid subsampling.


% When it comes to large corpora (such as general-purpose ones), there is good reason to require that they are electronic in order for them to be usable, since smaller, `hand-processable' versions would not be sufficiently representative.  As with purpose, then, it seems reasonable to require a standardised format, and increasingly reasonable to require that a corpus be machine-readable.







\subsubsection{Dissemination and Collaboration}
The ability for multiple researchers to access a corpus is one of the main benefits of corpus methods---corpus-based studies may be replicated and compared with absolute certainty of the empirical aspect of the research.

The process of building a large, multi-purpose sample for use by a whole field of research mirrors that used in other fields, many of which have similar problems gathering data.

Many examples of this approach exist, such as the British Household Panel Survey and British Crime Survey\cite{taylor1996british,hough1983british}, both of which demand[ed] significant investment over a long period in order to overcome the practical issues of large-scale demographic sampling.  Nonetheless, the quality this yields has lead to their widespread use, for example, both are used extensively by governments to assess social policy impact.

Although corpora are generally less sensitive on ethical grounds, many legal challenges remain to distributing and using such a large quantity of data.  This has historically greatly limited both the source and form of data gathered for corpora, and was one of the reasons behind sampling 2000-word samples (rather than whole texts) in Brown's corpus.

Navigating copyright law remains one of the primary tasks of a corpus building effort, though the increasing dependence on digital sources has dulled this somewhat, since they are typically less controlled when being published.  Nonetheless, corpora often come bundled with restrictive licensing, something that limits the ability of the wider community to participate in their use.

Many countries' fair use exemptions apply to research, though this may apply only to copies taken for private study, as laid out in the UK's Copyright, Designs and Patents Act 1988
%\td{citation: http://www.legislation.gov.uk/ukpga/1988/48/introduction}
.  The concept of `Fair Dealing' covers many uses of extracts in research, and a recent exemption was added that additionally allows the use of\cite{intellectualpropertyofficeuk2014}\td{nopage, online}:

\begin{quote}
\ldots{}automated analytical techniques to analyse text and data for patterns, trends and other useful information.
\end{quote}

% Similar fair use clauses are relied upon in the USA to distribute some web corpora, 
%-



The heavy re-use of corpora, then, exerts both positive and negative forces upon the scientific community.  On one hand, it is necessary to share resources in order to lower costs, increase awareness of rare data, and pool efforts to create better samples.  On the other, the ubiquity of a corpus may damage its scientific value, and starve the community of more up-to-date or relevant resources.  In the worst case, widespread use of a single corpus by the community may lead to partial circularity of hypothesis derivation and testing.

\til{I don't know enough linguistics to pick out features that have been hot topics enough to do this.\\
Also I fear this is hard to find due to the lack of distinct exploratory/confirmatory roles in CL papers.}

In an ideal world, it would be possible to replicate studies with one's own samples.  As I shall cover later, increasing digitisation of documents (and use of the web) offers a way to make this scenario possible, at least for some corpora and purposes.



\subsubsection{Sample Type}
Corpora span many types of sample, from simple cross-sectional ones (`synchronic' corpora: Brown, BNC) to those that aim to report language use through time (`diachronic' corpora: ICE, Longman/Lancaster) and a hybrid of the two, which is designed to follow language use and update on-the-fly (`monitor' corpora: BoE, COCA)\cite{francis1961brown,burnard1995users,greenbaum1996international,summers1993longman,Jarvinen1994AMW991886.991985,davies2010corpus}.  Further to this, there are parallel corpora, intended to match texts across some variable such as language\cite{koehn2005europarl,mcenery2007parallel}, and many other designs that combine properties of these to satisfy various sample designs.

These approaches represent various use-cases, and are often significantly less `general-purpose' than large synchronic reference corpora such as the BNC.








\subsubsection{A working Definition}
The above discussion illustrates the wide variety of samples that may be called general-purpose corpora, and some of the issues that affect their utility.  The focus of this thesis is on mechanisms for making, operationalising and documenting the above choices, and as such the working definition here opts to not apply any clear restrictions.

One obvious requirement, however, is that a corpus must be a suitable sample \textsl{for its intended purpose}.  This means that the sample design (external variables) and the document classification (internal variables) must be clearly defined in terms of the research questions given.

For the purposes of this thesis, a corpus constitutes a body of text that must be:

\begin{itemize}
    \item Representative of some stated population;
    \item Sufficiently large to satisfy that representation (for a given set of purposes);
    \item Explicit in its coverage of external variables;
    \item Of a regular, machine-readable format.
\end{itemize}


% 
% 
% % TODO: some kind of blockquote thing
% ``A collection of text samples, subject to the following conditions:
% 
% \begin{enumerate}
%     \item \label{enum:corpusdef-external} External (contextual) data is sufficiently well documented to prove useful to research (i.e. the population is well defined, texts are annotated);
%     \item \label{enum:corpusdef-internal} Texts are sampled in an internally-consistent manner, relative to external proportions (i.e. the corpus is externally and internally valid, and thus representative);
%     \item The data are recorded in a format suitable for automated processing (i.e. the corpus is not intended for direct manual inspection, and will be sized accordingly);
%     \item Documentation is provided in order to describe the veracity of claims made for \ref{enum:corpusdef-external} and \ref{enum:corpusdef-internal} above.
% \end{enumerate}
% 
% ''
% 













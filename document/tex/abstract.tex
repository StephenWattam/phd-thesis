Current efforts in corpus linguistics and NLP centre around the use of corpora---large language samples that are intended to describe a whole population of language users.

% Historically these items have had their utility abrogated by a number of economic, social and practical limitations.

Perhaps because of the hard-won nature of the first corpora, many characteristics of their design have proven intransigent.  This yields many benefits for science---many studies are comparable and interaction between those in the field is simplified, however, many corpus-building efforts serve to replicate these older designs, rather than adapting them to suit available technological innovations.

The rise of web as corpus (WaC), and more recently big data, has provided us with the tools needed to build corpora with very little supervision.  This allows us to re-examine and, in places, exceed the limitations of conventional corpora.  Even so, many corpora are compared against conventional ones due to their status as a de-facto gold standard of representativeness.

Though the processes involved in building web corpora introduce their own limitations, this ease of access allows for improved adherence to the theoretical ideals of sampling, and thus a complementary strategy for corpus design.

%---

In this thesis I'll provide justification for the development of new sampling procedures guided less by concepts of linguistic balance, intuition and coverage, and more by statistical sampling theory.  I apply these novel methods to the problem of improving WaC samples (whilst avoiding their novel pitfalls), and present ways in which the whole scientific process of corpus comparison, dissemination and replicability may be assessed---itself something that demands an alternative view of corpus goals.  I go on to present tools that make these tasks easy to apply in a scientific context, in order to construct usable and practical web-derived corpora.



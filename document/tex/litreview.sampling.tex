% This will be a review of more formal sampling theory, comparing it to methods for acquiring language.


Many of the problems listed in section~\ref{sec:litreview-corpora} may be mitigated with careful sampling.  There are a great number of available sampling strategies, and the purpose of this section is to review those which are suitable for use with text data.


%- *NB: I don't wish for this to get too prescriptive.  It's intended as an overview of an idealised procedure, rather than a "how to", which will basically come later in a far more practical form*

\subsection{Population Definition}
The population covered by a corpus is likely to be defined in terms of social demographics, linguistic features, or media type.  The manner in which each of these may be estimated, enumerated, and sampled is highly variable mainly due to the availability of auxiliary data.

An accurate population definition is necessary for two purposes.  At one level, specification of this will define the bounds of external validity, guiding use of the corpus and defining the set of problems for which it is useful.  A second reason for accurate population specification lies in estimation of its size, which is necessary for power analysis and sample size calculation.

% Discuss justification of external validity, how this must be strict for use with general-purpose corpora
Ultimately, any statement of population coverage will be unique to the corpus being built.  In the case of general-purpose corpora, documentation of this population is particularly important, as the corpus is likely to be reused and possibly subsampled.  Further, the relationship each sample point within the corpus has to the overall population should be annotated, so that any resampling is defensible.

% Discuss causal theories of language, and how these must inform a choice
If sampling from linguistic features, the population items are taken from may be purely theoretical.  This initially would seem to violate the principles of corpus linguistics sampling language `as it is used', however, each example is necessarily also covered by a demographic population from which those data were taken.  It is particularly important that both are documented, especially given disagreements between theories regarding language use (for example, some researchers may consider two features related where others do not)\td{examples}.


\til{p'raps examples of how corpora do this right now, and other others do this?}

\til{mention external nature of definition, homogeniety of population (though the latter might be less important given the purpose comments in the corpus section)}



% 
% \begin{itemize}
%  \item Applications to language (not sure of any in the abstract, todo: find some.)
%  \item Applications to the web (google/yahoo estimation papers from DA bibtex)
% \end{itemize}








\subsection{Sampling Frame}
Definition of a sampling frame is a very similar problem to the population definition above, except that, where a population definition may relate to the aspects being studied, definition of a sampling frame is necessarily performed in terms of texts themselves.

One significant challenge here is that a well defined population may yield a very poorly defined sampling frame (or one which only matches under certain strong assumptions).  The difficulty of acquiring texts from many different sources means this is a particularly pertinant issue to corpus validity.

\til{say what a sampling frame is :-)}

\til{Enumeration of sampling frame, is it necessary for each sampling method?}



\subsubsection{Selection of a Sampling Unit}
\til{Linguistic: Mention dispersion, size needed, counts usually happening in words.  Statisitcal: foreshadow power and size debate below.  Discuss the relative complexity of multi-level data.}


\subsubsection{Auxiliary Information}
Auxiliary information may be used from a wide variety of sources to inform the specification both of a population and of a sampling frame.  The source of this information will vary depending on the focus of the corpus, however, there are some obvious sources of authoritative data:

\begin{itemize}
    \item Social surveys (for demographic, attitudinal information)
    \item Previous corpus building efforts
    \item Smaller-scale research on specific issues
\end{itemize}



\til{ More detail.  Methodological stuff.
    use of auxiliary information iteration to inform selection of texts}









% Much of this list belongs in the sampling methods bit below.
% \begin{itemize}
%     \item Subsampling with/without weights
%     \item Using auxiliary information to weight strata
%     \item artificial inflation of interesting sections of the population (modelling, stratification)
% \end{itemize}

\subsection{Sampling Method}

\til{A discussion of what each does, and where it seems to be used, split up into nonrandom:}

\til{Difficulties in random sampling due to the above problems with specifying a sampling frame}

\til{Opportunities in nonrandom sampling.  Cover nonrandom sampling used elsewhere where similar problems exist}


\subsubsection{Random}


\begin{itemize}
    \item SRS (more special purpose focus, mention use in re-sampling)
    \item Stratified (focus on this, mention IPW, use of external data to find proportions, primary dimensions of variation and link to taxonomic stuff)
    \item Multi-stage (link to above, perhaps belongs in that section)
    \item Cluster (may simplify some practical issues)
    \item Adaptive (possibly introduce later since it's a plausible solution to some issues, but not currently used.  Some parallels with the iterative method)
\end{itemize}

\subsubsection{NonRandom}

\begin{itemize}
    \item Snowball sampling (web crawlers)
    \item 'Purposive' in general (applies slightly to many things, perhaps worth mentioning but not really labouring)
\end{itemize}












\subsection{Size and Power}
Methods for selecting sample sizes, (stratum sizes), sampling unit sizes [last of these is mainly linguistic, but might be worth mentioning].

\subsubsection{Population Size Estimation}
\til{Write up notes, cover various methods and assess suitability.  Cover existing efforts from SE literature (perhaps later in web stuff)}
% http://www.healthknowledge.org.uk/public-health-textbook/health-information/3a-populations/methods-population-estimation-projection
% http://www.jstor.org/discover/10.2307/3797301?uid=3738032&uid=2&uid=4&sid=21102138383343
% http://en.wikipedia.org/wiki/German_tank_problem
% http://www.cals.ncsu.edu/course/fw353/Estimate.htm


\subsection{Sample Size and Power}
\til{Discuss methods for estimating sample size.}
\til{Debate suitability for various uses, esp. for general purpose corpora.  Focus on Zipfianness.}
\til{Cover saturation measures, 'new information' approaches}

% \begin{itemize}
%     \item Criteria for selection
%     \begin{itemize}
%         \item purpose-based models [should be big enough for x]
%         \item sufficiency/internal measures [should contain n xs]
%         \item breadth/variation [should contain n types of x]
%     \end{itemize}
%     \item Big data's approach
%     \item Size of auxiliary data for multi-stage designs
% \end{itemize}









% TODO: this section might not belong, or it might be best off later
\til{ The section below is probably best off being placed later, or at least where its focus is more well-defined}
\subsection{Post-hoc/Reweighting(/representation)}
\til{this seems to conflate the stratification stuff, restructure}
Methods for upping weights in line with auxiliary data, even of existing corpora.

\begin{itemize}
    \item Establishing weights using auxiliary data
    \begin{itemize}
        \item possible sources of valid data
        \item existing manual resampling by selection of categories (compare to above method)
    \end{itemize}
\end{itemize}












\subsection{Bias Estimation and Evaluation}
Methods for evaluating and comparing corpora

% http://en.wikipedia.org/wiki/Mean_signed_difference ?
% http://en.wikipedia.org/wiki/Mean_absolute_error

\begin{itemize}
    \item Bootstrapping
    \item Comparison to other corpora (validity, meaning, can we trust previous corpus to be a gold standard?)
    \item Comparison to humans (heuristics, summarising, "getting to know your corpus")
\end{itemize}





% Below is OLD as of 02/02/13
% 
% As we have seen in section~\ref{sec:sampling-corpus-linguistics}, corpus building efforts generally seek to solve a number of practical problems with quantitative linguistics, namely:
% \begin{enumerate}
%     \item Defining a finite and reliable linguistic resource (one that is static and entirely accessible);
%     \item Replicating, comparing, and disseminating results.
% \end{enumerate}
% 
% As such, the focus of many corpus building efforts \td{which corpus building efforts?}
% has been largely on problems of procuring texts, selecting sufficiently broad categories of texts to be useful to many researchers, and managing the practicalities of text selection (i.e. by taking snippets of published and copyright works rather than their whole inclusion).
% 
% The use of corpora availed many statistical quantitiative techniques, each bringing a series of assumptions regarding not only the internal nature of the corpus, but also its relation to the larger population.  Whilst the validity of internal models has been discussed at length\td{where discussed?}
% , little has been done to address the problems of external validity in this regard.
% 
% Therefore, in order to improve (and to assess) the value of a corpus to the wider field, it is necessary to inspect not only its empirical advantages (now well-determined for many frequently-used corpora) but also the potential value any given corpus building stratgy may yield in the ideal case.
% 
% In this section, I will be offering a critical description of how linguistic data may be sampled using existing, conventional, statistical techniques.  This comparison shall act as a gold standard against which existing (and future) efforts may be judged, as well as offering a stance from which to assess the successes and failings of current corpus-building efforts.
% 
% 
% 
% \subsection{The ideal Corpus}
% \label{sec:sub:ideal-corpus}
% \til{Based on current usage, but notably not restricted by methods and practicalities, review 2.1 with a view to extracting what we want from a corpus---address size, randomness, validity (internal + external), replicability, reliability, and flexibility.}
% 
% \subsubsection{Nature of Sample}
% \til{Conventional corpora are guided by experts.  Do we want pure random corpora?  What does this even mean, what would the population be? language use or language exposure?  Defer some of these until the later sections of the thesis.}
% \subsubsection{Validity}
% \til{Following on from the above.  What does external validity become for a general/specific purpose corpus?  Do common corpora have any claim to external validity when used for qualitative/quantitative analyses of varying types?}
% \subsubsection{Replication and Reliability}
% \til{Corpora are hailed as being a basis for collaboration.  This is true of the most broken corpus, but a shared and accepted broken sample is worse than everyone sampling from the same population.  Address subsampling problems and issues of 're-usable bias' for very popular corpora.  To what extent are studies based on the same corpus comparable when using massively  different methodologies?}
% \subsubsection{Size}
% \til{How many words should a piece of string have in it?}
% \subsubsection{Flexibility}
% \til{Selection of data for qualitative research, or subsampling for quantitative, has the potential to undo all of the balancing above if not done properly.  How can this risk be minimised (guidelines for researchers, corpus size, specific corpus design, distributing tools with corpora)?}
% % ---
% \subsubsection{Summary}
% \til{A simple, short and refer-able list of important things to work for in the below section.  Really, this might section up to here might be a good paper for CL or something.}
% 
% % \subsection{Paradigms for Sampling Language} % Kuhnian
% % \til{The aim in this section is to draw out prominent linguistic concepts and justify sampling methods examined above in terms of their suitability to the theory of how we use language.  This is the tie to the personal corpus stuff, but will mention other ideas of what language is.}
% 
% 
% 
% \subsection{Comparison of Methods}
% \til{Here I'll draw parallels between sampling methods and comment critically on their value.}
% \subsubsection{Population}
% \til{This is a largely linguistic issue of external validity, and will point back at section 2.1 a lot}
% Linguistic issues (p'raps best taken from 2.1?):
% \begin{itemize}
%     \item How can we select a population closest to that discussed in section~\ref{sec:sub:ideal-corpus}?
% \end{itemize}
% Statistical theory:
% \begin{itemize}
%     \item How other fields with very complex parameter spaces do sampling
%     \item Methods for population estimation given constraints from section~\ref{sec:sub:ideal-corpus}
% \end{itemize}
% \subsubsection{Sampling Frame}
% \til{This is largely an issue of internal validity and reliability.}
% Enumeration or bounding of a population:
% \begin{itemize}
%     \item General and special purpose corpora
%     \item Sources of data (web, books)
%     \item Time
% \end{itemize}
% Issues surrounding subsampling [general purpose corpora], and how this ought to be done
% 
% Sources of valid auxiliary information for sampling
% \subsubsection{Sampling Methods}
% \til{An overview of statistical sampling methods relevant to ling.}
% Nonrandom methods, primarily included to compare against current ones:
% \begin{itemizeTitle}
%     \item[Purposive] Compare against current techniques.  Critique use of inferential stats on such corpora.  Big data perhaps illustrates flaws?
%     \item[Snowball Sampling] Relevant due to web crawlers
% \end{itemizeTitle}
% Random sampling methods, as possible approaches
% \begin{itemizeTitle}
%     \item[Simple Random] Ideal case.  Focus on what makes it hard for linguistics.  Attempt to identify ways around this (empirical estimates of P()). Detasil practical issues with selection relative to stats.  
%     \item[Stratified] How to select strata (w/ling. relevance)? Multi-dimensional strata. Inverse probability weighting.  PPS possibilities with web/offline.
%     \item[Multistage ~and~] IPW, how to balance samples and...
%     \item[Cluster] Website-wide clusters.  Relate to the structure of data, with publishers/websites forming clusters.
%     \item[Adaptive] Perhaps too nonrandom for this section?  Offers a way to balance corpora with linguistic sampling frame.  Select frame using multistage sampling?
% \end{itemizeTitle}
% 
% \subsubsection{Size and Power}
% \til{Methods for selecting size and power.}
% Linguistic criteria for selecting sizes (relate to ling. lit above) [common models, sufficient representation of odd features, breadth of coverage].
% 
% Existing corpora's approach (refer to 2.1 mainly).  Focus on the rise of big data+simple models showing that p'raps small data is biased ("bad smells").
% \subsubsection{Reweighting}
% \til{Multistage sampling and bias estimating with and without using auxiliary data.}
% For multistage designs, how large should the initial corpora be? Is it small enough to be driven by the user's data or selection of medoid items?
% 
% Auxiliary Data:
% \begin{itemize}
%     \item Sources of valid auxiliary data (without inheriting the bias of existing corpora, ideally)
%     \item How broken manual resampling of corpora is at the moment, and how people *want* to do it.
% \end{itemize}
% \subsubsection{Bias Estimating}
% \til{This covers the problem of evaluation, how do we know that a corpus is better than another without a gold standard?  Does a gold standard actually exist, but we are not using it as such?  How can we ensure change goes in the correct direction?}
% Repeated resampling and benchmarking.
% 
% Comparison to auxiliary data, suitable distance metrics, and criteria for acceptance.
% 
% 


% This will be a review of more formal sampling theory, comparing it to methods for acquiring language.










Many of the problems listed in section [ref] above are fairly minor, and careful sampling easily mitigates their effect.  A number of sampling strategies are available for selection of data, and the purpose of this section is to review them in the abstract case, in order to identify their suitability for linguistic sampling.

Though I will stop short of defining, in concrete terms, the specification of an 'ideal' general-purpose corpus, this section begins with a discussion of the aims of a corpus, and how they impact desirable properties which we may wish to maximise through proper selection of a sampling strategy. 

%- this is motivation for a later chapter really, and this subsection may want moving to after the discussion of sampling policies, or the intro to that chapter.

\subsection{Desirable Properties of a Sample}
\til{Remove the linguistic stuff in preference for more theoretical side.  Cover power.}


\subsubsection{Sample Size}
This is a tricky but crucial property that is much discussed elsewhere relative to linguistic properties.  In seeking a sampling strategy, it is important that we are able to accurately assess the necessary size from, at worst, modest amounts of auxiliary data.  Any method that requires significant over-sampling or repeated iteration will incur the costs and efforts associated with physically gathering the data, which is most undesirable.



\subsubsection{Randomness}
This is a particularly thorny issue that has been little-addressed by linguists.  Upon initial examination of the problem, a simple random sample (SRS) may seem like the ideal---we know little about the data we're sampling, and use of any existing corpora to improve sampling proportions will be subject to the biases in their original design.  

Though stratified, random sample designs seem to offer the most obvious way of representing populations of texts.  This method is advocated by the Brown documentation, explained at length by Evert and others [Dunning, IIRC, covers this], and samples are treated as random by those analysing text (Biber's assessments don't weight things, for example).

The problem with random sample designs remains their attempt to be proportional.  Biber, rejecting the idea of a proportional corpus says

 > "if we were to sample proportionally, we'd just get a whole load of speech" (find in '93 paper)

This sentiment is contradicted by Varadi (2001), who calls for proportional sampling in order to ease analysis, however, Biber's point is a valid one.  The complexity of language, and its distribution over other interesting covariates, may mean that it is impractical to sample a sufficiently-sized proportional corpus.

The issue of proportionality, and sample sizes needed to acquire a useful level of significance from analyses thereon, is intimately related to the problem of selecting categories for selection, and as such should be assessed in context of a particular corpus.  If non-random sampling is to be used, it should be considered carefully and explicitly based on preliminary samples and other sources of auxiliary data.]

% --
in summary:

\begin{itemize}
 \item for randomness --- easy analysis without weights, easy subsampling, clear docs on demographics, equal 'resolution' of data for each covariate
 \item for nonrandomness --- better representation of small populations, smaller sample needed for many purposes, faster analyses, easier acquisition
\end{itemize}



\paragraph{Validity}
The concept of validity is only meaningful relative to a certain task, however, we may clarify the issues by breaking up the link between the theories we wish to test and their underlying physical manifestations (which we wish to sample):

\begin{itemizeTitle}
 \item [Criterion/predictive validity] the degree of association between the concept measured and the value of the data acquired, i.e. are we sampling in the correct way?
 \item [Content validity] the extent to which measurement of a given 'thing' and the theory, i.e. are we sampling the correct thing?
 \item [Construct Validity] The extent of agreement between theories and their implications and other [currently accepted] theories. \td{This should probably be removed, it's not terribly relevant.}
\end{itemizeTitle}

Alternatively, from 'SAGE's dictionary of quantitative management research' \til{find better sources before cementing this section, check library's obscenely expensive books section}:

> "Internal validity is the quality of an experimental design such that the results obtained can be attributed to the manipulation of the independent variable, whereas external validity is the quality of an experimental design such that the results can be generalised from the original sample and, by extension, to the populatrion from which the sample originated."\td{make a blockquote LaTeX thing}

Both of these are impossible to fully determine without first having an experimental design, however, a poorly designed corpus will yield fewer valid designs and uses than will an equivalently-specified well-sampled one.  Further to this, poor documentation of corpora may lead people to incorrectly assess the validity of their application.

\til{ include discussion on transferrability/generalisability; decide which of the above formalisms to use (probably internal/external, the former offers little but might be handy for discussing links between proxy variables and sample selection)}




\subsubsection{Replicability and Reliability}
The replicability of studies based on the same corpus is one main reason for using general-purpose corpora.  One major concern with the use of larger corpora is that they are quickly rendered out of date for certain purposes: those studying technology-related neologisms, for example, are likely to call upon web corpora and more recent publications, rather than look to more established works, because their hypotheses centre on a rapid change which is poorly represented by a decade-old corpus, however balanced.

[perhaps it'd be nice to look through some journals and count who's using corpora of what age here, might be handy]

Synchronic corpora are also, given their very narrow temporal distribution, more likely to represent fashions and short-term affectations.  Whilst this may initially be desirable, the relevance of its data to other corpora, and to everyday life, diminishes in a difficult-to-predict fashion (indeed, change through time is a whole area of inquiry in its own right).

[ talk about how technology might help, but link to future sections instead of going too far into it]




\subsubsection{Flexibilty/Generality}
One of the aims of a general purpose corpus is to foster re-use of data for all those studying a given population, across a multitude of research questions.  Though, as the above discusses, it is necessary to restrict the useful domain of a corpus during the design stages, care should be taken to sample a sufficiently general population of language users (and a sufficiently general set of variables on each one) so as to be useful in many different study designs.

Efforts in this area have primarily centred around two foci---the former of these is the proper selection of corpus variables and sampling, discussed in collaborative efforts to standardise corpus design:

> There's a quote from EAGLES where they say, in effect, "one of the major contributions of this is that we've got people talking about standardisation and collective use"

\til{ also see efforts to cover corpus design in general, seek from Bibtex list. }

The latter of these surrounds subsampling corpora for given uses.  Subsampling is a relatively common operation for general purpose corpora, and many commonly-used corpus analysis tools such as SketchEngine and CQPWeb [bncweb?] support simple mechanisms for constructing subcorpora.

The representation of the [ahrg, can't find the paper, austrian? national] corpus was designed such that text was stored in "chunks", aligned with powers of two.  This, along with an interface that allowed selection of these chunks to form a subcorpus, allowed a user to subsample the corpus with high levels of granularity for each of the variables he was interested in.  Perhaps due to the focus on having fairly large text samples, this approach does not seem to have caught on [yet...].




\subsection{Sampling Strategy}
*NB: I don't wish for this to get too prescriptive.  It's intended as an overview of an idealised procedure, rather than a "how to", which will basically come later in a far more practical form*

\subsubsection{Population Definition}
Defined in terms of

\begin{itemize}
 \item aim of corpus \& science, who would find a given set useful for study?
 \item External validity justification
 \item Causal systems of language, variation within them
\end{itemize}

How: Population estimation methods

\begin{itemize}
 \item Applications to language (not sure of any in the abstract, todo: find some.)
 \item Applications to the web (google/yahoo estimation papers from DA bibtex)
\end{itemize}




\subsubsection{Sampling Frame}

\begin{itemize}
 \item Enumeration/definition
    \begin{itemize}
    \item taxonomy specification work, p'raps?  this is possibly too 'linguistic' for this section though
    \end{itemize}
 \item Subsampling with/without weights
 \item Using auxiliary information to weight strata
 \item artificial inflation of interesting sections of the population (modelling, stratification)
\end{itemize}



\subsubsection{Sampling Method}
A discussion of what each does, and where it seems to be used, split up into nonrandom:

\begin{itemize}
 \item Snowball sampling (web crawlers)
 \item 'Purposive' in general (applies slightly to many things, perhaps worth mentioning but not really labouring)
\end{itemize}

and random:

\begin{itemize}
 \item SRS (more special purpose focus, mention use in re-sampling)
 \item Stratified (focus on this, mention IPW, use of external data to find proportions, primary dimensions of variation and link to taxonomic stuff)
 \item Multi-stage (link to above, perhaps belongs in that section)
 \item Cluster (may simplify some practical issues)
 \item Adaptive (possibly introduce later since it's a plausible solution to some issues, but not currently used.  Some parallels with the iterative method)
\end{itemize}


\subsubsection{Size}
Methods for selecting sample sizes, stratum sizes, sampling unit sizes [last of these is mainly linguistic, but might be worth mentioning].

\begin{itemize}
 \item Criteria for selection
    \begin{itemize}
    \item purpose-based models [should be big enough for x]
    \item sufficiency/internal measures [should contain n xs]
    \item breadth/variation [should contain n types of x]
    \end{itemize}
 \item Big data's approach
 \item Size of auxiliary data for multi-stage designs
\end{itemize}


\subsubsection{Post-hoc/Reweighting(/representation)}
Methods for upping weights in line with auxiliary data, even of existing corpora.

\begin{itemize}
 \item Establishing weights using auxiliary data
    \begin{itemize}
    \item possible sources of valid data
    \item existing manual resampling by selection of categories (compare to above method)
    \end{itemize}
\end{itemize}


\subsubsection{Bias Estimation/Evaluation}
Methods for evaluating and comparing corpora

\begin{itemize}
 \item Bootstrapping
 \item Comparison to other corpora (validity, meaning, can we trust previous corpus to be a gold standard?)
 \item Comparison to humans (heuristics, summarising, "getting to know your corpus")
\end{itemize}




















% Below is OLD as of 02/02/13
% 
% As we have seen in section~\ref{sec:sampling-corpus-linguistics}, corpus building efforts generally seek to solve a number of practical problems with quantitative linguistics, namely:
% \begin{enumerate}
%     \item Defining a finite and reliable linguistic resource (one that is static and entirely accessible);
%     \item Replicating, comparing, and disseminating results.
% \end{enumerate}
% 
% As such, the focus of many corpus building efforts \td{which corpus building efforts?}
% has been largely on problems of procuring texts, selecting sufficiently broad categories of texts to be useful to many researchers, and managing the practicalities of text selection (i.e. by taking snippets of published and copyright works rather than their whole inclusion).
% 
% The use of corpora availed many statistical quantitiative techniques, each bringing a series of assumptions regarding not only the internal nature of the corpus, but also its relation to the larger population.  Whilst the validity of internal models has been discussed at length\td{where discussed?}
% , little has been done to address the problems of external validity in this regard.
% 
% Therefore, in order to improve (and to assess) the value of a corpus to the wider field, it is necessary to inspect not only its empirical advantages (now well-determined for many frequently-used corpora) but also the potential value any given corpus building stratgy may yield in the ideal case.
% 
% In this section, I will be offering a critical description of how linguistic data may be sampled using existing, conventional, statistical techniques.  This comparison shall act as a gold standard against which existing (and future) efforts may be judged, as well as offering a stance from which to assess the successes and failings of current corpus-building efforts.
% 
% 
% 
% \subsection{The ideal Corpus}
% \label{sec:sub:ideal-corpus}
% \til{Based on current usage, but notably not restricted by methods and practicalities, review 2.1 with a view to extracting what we want from a corpus---address size, randomness, validity (internal + external), replicability, reliability, and flexibility.}
% 
% \subsubsection{Nature of Sample}
% \til{Conventional corpora are guided by experts.  Do we want pure random corpora?  What does this even mean, what would the population be? language use or language exposure?  Defer some of these until the later sections of the thesis.}
% \subsubsection{Validity}
% \til{Following on from the above.  What does external validity become for a general/specific purpose corpus?  Do common corpora have any claim to external validity when used for qualitative/quantitative analyses of varying types?}
% \subsubsection{Replication and Reliability}
% \til{Corpora are hailed as being a basis for collaboration.  This is true of the most broken corpus, but a shared and accepted broken sample is worse than everyone sampling from the same population.  Address subsampling problems and issues of 're-usable bias' for very popular corpora.  To what extent are studies based on the same corpus comparable when using massively  different methodologies?}
% \subsubsection{Size}
% \til{How many words should a piece of string have in it?}
% \subsubsection{Flexibility}
% \til{Selection of data for qualitative research, or subsampling for quantitative, has the potential to undo all of the balancing above if not done properly.  How can this risk be minimised (guidelines for researchers, corpus size, specific corpus design, distributing tools with corpora)?}
% % ---
% \subsubsection{Summary}
% \til{A simple, short and refer-able list of important things to work for in the below section.  Really, this might section up to here might be a good paper for CL or something.}
% 
% % \subsection{Paradigms for Sampling Language} % Kuhnian
% % \til{The aim in this section is to draw out prominent linguistic concepts and justify sampling methods examined above in terms of their suitability to the theory of how we use language.  This is the tie to the personal corpus stuff, but will mention other ideas of what language is.}
% 
% 
% 
% \subsection{Comparison of Methods}
% \til{Here I'll draw parallels between sampling methods and comment critically on their value.}
% \subsubsection{Population}
% \til{This is a largely linguistic issue of external validity, and will point back at section 2.1 a lot}
% Linguistic issues (p'raps best taken from 2.1?):
% \begin{itemize}
%     \item How can we select a population closest to that discussed in section~\ref{sec:sub:ideal-corpus}?
% \end{itemize}
% Statistical theory:
% \begin{itemize}
%     \item How other fields with very complex parameter spaces do sampling
%     \item Methods for population estimation given constraints from section~\ref{sec:sub:ideal-corpus}
% \end{itemize}
% \subsubsection{Sampling Frame}
% \til{This is largely an issue of internal validity and reliability.}
% Enumeration or bounding of a population:
% \begin{itemize}
%     \item General and special purpose corpora
%     \item Sources of data (web, books)
%     \item Time
% \end{itemize}
% Issues surrounding subsampling [general purpose corpora], and how this ought to be done
% 
% Sources of valid auxiliary information for sampling
% \subsubsection{Sampling Methods}
% \til{An overview of statistical sampling methods relevant to ling.}
% Nonrandom methods, primarily included to compare against current ones:
% \begin{itemizeTitle}
%     \item[Purposive] Compare against current techniques.  Critique use of inferential stats on such corpora.  Big data perhaps illustrates flaws?
%     \item[Snowball Sampling] Relevant due to web crawlers
% \end{itemizeTitle}
% Random sampling methods, as possible approaches
% \begin{itemizeTitle}
%     \item[Simple Random] Ideal case.  Focus on what makes it hard for linguistics.  Attempt to identify ways around this (empirical estimates of P()). Detasil practical issues with selection relative to stats.  
%     \item[Stratified] How to select strata (w/ling. relevance)? Multi-dimensional strata. Inverse probability weighting.  PPS possibilities with web/offline.
%     \item[Multistage ~and~] IPW, how to balance samples and...
%     \item[Cluster] Website-wide clusters.  Relate to the structure of data, with publishers/websites forming clusters.
%     \item[Adaptive] Perhaps too nonrandom for this section?  Offers a way to balance corpora with linguistic sampling frame.  Select frame using multistage sampling?
% \end{itemizeTitle}
% 
% \subsubsection{Size and Power}
% \til{Methods for selecting size and power.}
% Linguistic criteria for selecting sizes (relate to ling. lit above) [common models, sufficient representation of odd features, breadth of coverage].
% 
% Existing corpora's approach (refer to 2.1 mainly).  Focus on the rise of big data+simple models showing that p'raps small data is biased ("bad smells").
% \subsubsection{Reweighting}
% \til{Multistage sampling and bias estimating with and without using auxiliary data.}
% For multistage designs, how large should the initial corpora be? Is it small enough to be driven by the user's data or selection of medoid items?
% 
% Auxiliary Data:
% \begin{itemize}
%     \item Sources of valid auxiliary data (without inheriting the bias of existing corpora, ideally)
%     \item How broken manual resampling of corpora is at the moment, and how people *want* to do it.
% \end{itemize}
% \subsubsection{Bias Estimating}
% \til{This covers the problem of evaluation, how do we know that a corpus is better than another without a gold standard?  Does a gold standard actually exist, but we are not using it as such?  How can we ensure change goes in the correct direction?}
% Repeated resampling and benchmarking.
% 
% Comparison to auxiliary data, suitable distance metrics, and criteria for acceptance.
% 
% 


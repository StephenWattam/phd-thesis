
The evaluation of corpus construction is a particularly tricky area: without a gold standard or real-world task to frame the results, it is often difficult to tell quantitatively what differences between corpora constitute improvements.


The way seed-based methods work offers an ideal source of gold standard data, nonetheless, only experience and repeated use can reveal the manner of the differences identified, and its impact on experimental results.  This evaluation will use this reflexive method to identify overall responsiveness to the seed corpus, along with an examination of individual components of the system as a method for revealing specific strengths and weaknesses.

This evaluation is based around one of tasks identified in Chapter~\ref{?}---that of rebuilding a corpus from scratch based upon a seed's proportions.  This task is effectively a super-task of reconstructing or repairing a corpus, and is equivalent to many scenarios involving disseminating sensitive corpora with minor restrictions on the metadata dimensions used.



This chapter begins with a description of the use cases covered, and a detailed rationale of how these form testable, objective research questions.

Section~\ref{sec:evaluation:method} details the two main approaches to evaluation: those applied to individual components, and those applied to the results of running the implementation as a whole.


\til{Write the rest of this when it's more carefully structured}






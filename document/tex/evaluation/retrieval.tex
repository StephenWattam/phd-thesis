The value in the resampling process above largely lies in the ability to present well-defined document prototypes to a retrieval stage.  There are many potential sources for document retrieval according to this prototype, including the original input corpus, a separate `super-corpus', or, ideally a separate population (that is itself not a sample).



\subsection{Method}
\label{sec:evaluation:method}

The proof-of-concept evaluated here implements a mechanism similar to that of BootCaT~\td{cite}, using a search engine to retrieve results based on key n-grams.  This mechanism was selected due to the pertinence of its method to other corpus building efforts --- there is no technical limitation imposed by the software tool, and users are free to select any other source of data or mechanism of retrieval.

The similarity of the retrieval system described here to BootCaT, along with the use of concrete prototype documents, allows us to inspect the `measurement error' of using search engines to retrieve data based on key terms.  This evaluation is, therefore, primarily a qualitative assessment of that bias and a preliminary assessment of its sources.

% ---

\begin{figure}[Ht]
    % ./analysis/loglik-significance.r
    \centering
    \includegraphics[width=0.8\textwidth]{evaluation/retrieval-overview}
    \caption{An overview of the retrieval mechanism used in the proof-of-concept implementation.}
    \label{fig:evaluation:retrieval:outline}
\end{figure}


The retrieval process (as summaried in Figure~\ref{fig:evaluation:retrieval:outline}) is performed iteratively using the components evaluated in the previous sections:

\begin{enumerate}
    \item Sample a prototype document from the corpus profile
    \item Retrieve candidate links using the Microsoft Bing web service~\footnote{\url{https://datamarket.azure.com/dataset/bing/searchweb}}.
    \item Download, remove boilerplate (using justext~\td{cite}), and classify each document using heuristics from the corpus profile.
    \item Measure distance in the resulting vector space according to each heuristic's value.
\end{enumerate}

For this data set, both the direct links from Bing and the first level of hyperlinks in each document were downloaded.  This is done in order to ensure that the sampled area of metadata is a supersample of the desired distribution, as well as to maximise the chances of finding suitable points in other, uncontrolled, dimensions.

Documents with fewer than 100 words were discarded as they were unlikely to be classified accurately.

For this evaluation, the written portions of the BNC were used as the seed corpus.  Keywords used for search were generated using log likelihood scores, and no cutoff was used: instead, the random selection algorithm was weighted by the resulting score.

The spoken and \texttt{w\_Misc} categories were omitted for a number of reasons.  Spoken data was omitted on the conjecture that it is difficult to find online~\footnote{Retrieval of transcriptions may be facilitated by searching for genre-specific features in addition to keywords, and this form of specialism is a potential avenue for improving the accuracy of all genres.}, leading to a predictable gap in the resulting corpus.  \texttt{W\_misc} was omitted due to the need for an accurate classification step, and because keyword-based retrieval requires that said keywords are highly representative of a given genre.  Both of these issues are potentially solvable in future work, yet lie outside the scope of this thesis.


The retriever must, as far as possible, retrieve documents according to \textsl{all} prototype metadata dimensions, that is, it must perform the equivalent to an agglomerative query.  This is a particular challenge for search engines due to their generality.  Since the Bing API lacks tools to filter by any of the three dimensions used in this evaluation, the primary focus was on genre --- word count and reading ease were both free to vary.  Language was specified as \texttt{en\_GB}.

% TODO: Write in discussion about how this is challenging but that a hybrid system might work best, say retrieve from scholar if it's a paper.









% This process is deliberately agnostic of any internal variables: the \textsl{content} of the retrieved texts is also affected by the choice of retrieval mechanism, however, this is part of the variance desired in the dataset, and should form part of any motivating research question.

% The problem of when the output corpus is `sufficiently large' may not be solved using this method: whilst the convergeance of the output corpus to the input is known to some degree of certainty, the same cannot be said of the errors in retrieval.  Assuming that each text is entirely accurate to its prototype will yield a corpus showing the same distribution as the input, but this does not mean that the source used is presenting data in an unbiased manner, or that sufficient variation has been captured to generalise about \textsl{that} population.

% It is also possible to select documents that are not a perfect fit to the prototype: indeed, this may be necessary where continuous measures are used to characterise the sampling design.  In this case, the residual variation may be used to determine bias in the selection method by measuring how non-uniform the distribution of these residuals is.  Note, however, that even uniform nonzero residuals represent an overall increase in dispersion compared to the input distribution: it is almost certainly more useful to deliberately apply this to the input distribution by applying a smoothing method prior to resampling than it is to rely on the document retrieval stage (which may apply said dispersion in less predictable ways).

% \til{
% further work:\\
% Though not implemented here, it is possible to use this known error distribution as a correction mechanism for the sampler, deliberately seeking to `shore up' the residuals.  This kind of feedback loop is used in bootstrapping algorithms such as Gibbs and slice sampling, though its application to a procedure based on a fairly hard-to-predict search mechanism (web search engines) presents major engineering challenges.
% }

% % --



% The retrieval method used here (outlined in Chapter~\ref{sec:rebuilding} and Figure~\ref{fig:evaluation:retrieval:outline}) is based on the principle of heuristically seeking documents fitting the prototype, followed by a ranking stage and selection of the highest-ranked document.  This means that it has the potential to output imperfect documents but is maximally unlikely to do so.  It does not adjust for any accumulated error during execution, so has the potential to gradually accumulate errors in one particular direction if `perfectly matched' documents are not available within the parameters given.  This approach is largely taken to prevent the retriever from searching endlessly for a combination of metadata values that simply do not exist in the sources to which it has access: many specific uses of this technique will be able to use far more certain methods for their retrieval stage (up to and including manual selection of documents).








\subsection{Results and Discussion}
\label{sec:evaluation:results}

In total, $72540$ documents were downloaded after $6510$ requests to Bing.  After boilerplate removal using JusText~\td{cite} and discarding of short documents, $55790$ documents remained in the corpus.


This sample size is $\approx 21$ times the input corpus.  Estimating using graphical methods (as in Section~\ref{sec:evaluation:resampler}, this should be sufficient to provide an appropriately distributed sample of genres.  The worst case is that each Bing search returns very few documents with the desired genre, for example, if each search returns only a single document of interest then this ratio is reduced to $\approx 3N$.



% \subsubsection{Univariate Properties}
Before inspecting the results from the retriever, the limitations of this method should be noted.  Firstly, the ideal retrieval mechanism should target all dimensions of the prototype document provided, in this case Genre, reading ease, and word count).  Bing does not index the latter of these two, leaving them to vary naturally.

This variation is in interaction with the genre itself, and so it is not possible to generalise from the observations here to the wider web.  The distributions shown in Figures~\ref{fig:evaluation:retrieval:flesh} and \ref{fig:evaluation:retrieval:words} are, however, indicative of the documents found on Bing when searching for BNC terms.  Upholding the assumption that the written BNC represents general-purpose language use, this is an indication of users' exposure to documents online\footnote{This is a distinction drawn strongly amongst WaC, where many efforts attempt to describe the web itself rather than usage thereof.}.

From the point of view of one sampling documents to form a supercorpus, it is desirable that the uncontrolled dimensions are widely and evenly dispersed across the vector space, such that any subsequent rejection sampling is simplified.


\begin{figure}[Ht]
    % ./analysis/
    \centering
    \includegraphics[width=0.8\textwidth]{evaluation/retrieval-flesch}
    \caption{Prototype/document readability scores}
    \label{fig:evaluation:retrieval:flesh}
\end{figure}

\begin{figure}[Ht]
    % ./analysis/
    \centering
    \includegraphics[width=0.8\textwidth]{evaluation/retrieval-words}
    \caption{Prototype/document word counts}
    \label{fig:evaluation:retrieval:words}
\end{figure}

Figure~\ref{fig:evaluation:retrieval:flesh} shows the distribution of reading ease scores across the collected documents, the $y$ axis showing the values as downloaded from the web.  The red line is plotted with a gradient of $0.5$.


The readability dimension (Figure~\ref{fig:evaluation:retrieval:flesh}) is fairly well dispersed, indicating that there are no  of document fundamentally unreachable using the search engine method.  Though this dimension is the simplest, both the prototype and retrieved documents have similar standard deviations ($16.2$ and $17.9$ respectively) and means ($57.5$ and $57.4$ respectively).  There is very little correlation between prototype and retrieved values: with a PMCC of just $0.06$.  Generally, this indicates that most distributions of readability are `covered' online, and the one used in the BNC is easily represented.


Word counts are altogether more complex.  The word count is distributed in the BNC in a far more complex manner \td{need a plot?}, and the method of sampling can be reasonably expected to impact this more than readability.  Figure~\ref{fig:evaluation:retrieval:words} shows that the distribution of word counts online (conditioned on genre) is far from that in the BNC.
% TODO


\begin{figure}[Ht]
    % ./analysis/
    \centering
    \includegraphics[width=0.8\textwidth]{evaluation/retrieval-words-dist}
    \caption{Distribution of document lengths in the written BNC and downloaded corpora}
    \label{fig:evaluation:retrieval:words-dist}
\end{figure}

The distribution plots in Figure~\ref{fig:evaluation:retrieval:words-dist} display the obvious missing portion of the word count target distribution.  This is a clear example of the bias of online documents, which tend to be paginated even where the content is particularly long (a trend that is exacerbated by advertising-based funding models).

\til{
In the discussion section, mention that this bias can be worked around for most analysis by not sampling whole texts, but that it is an interesting finding that the web can't do long texts.  Perhaps refer to this in the keyword bit later.
}



\begin{table}[Ht]
    \centering

    \begin{tabular}{|r|l|r|r|r|r|}
        \hline
        & Genre & Prototype & Downloaded & TP & tpr (\%) \\ 
        \hline
        1 & W\_ac\_humanities\_arts & 2180 & 1089 & 181 & 8.30 \\ 
        2 & W\_ac\_medicine & 987 & 1162 & 415 & 42.05 \\ 
        3 & W\_ac\_nat\_science & 684 & 1258 & 160 & 23.39 \\ 
        4 & W\_ac\_polit\_law\_edu & 4424 & 1467 & 590 & 13.34 \\ 
        5 & W\_ac\_soc\_science & 3112 & 1074 & 290 & 9.32 \\ 
        6 & W\_ac\_tech\_engin & 466 & 1267 & 129 & 27.68 \\ 
        7 & W\_admin & 195 & 1038 & 23 & 11.79 \\ 
        8 & W\_advert & 856 & 4501 & 273 & 31.89 \\ 
        9 & W\_biography & 2979 & 1736 & 221 & 7.42 \\ 
        10 & W\_commerce & 2952 & 2058 & 464 & 15.72 \\ 
        11 & W\_email &  46 & 882 & 3 & 6.52 \\ 
        12 & W\_essay\_school &   1 & 871 & 0 & 0 \\ 
        13 & W\_essay\_univ & 246 &  23 & 0 & 0 \\ 
        14 & W\_fict\_poetry & 815 & 460 & 42 & 5.15 \\ 
        15 & W\_fict\_prose & 10487 & 1844 & 888 & 8.47 \\ 
        16 & W\_hansard & 161 & 114 & 39 & 24.22 \\ 
        17 & W\_institut\_doc & 572 & 1952 & 131 & 22.90 \\ 
        18 & W\_instructional & 111 & 941 & 54 & 48.65 \\ 
        19 & W\_letters\_personal & 124 & 611 & 9 & 7.26 \\ 
        20 & W\_letters\_prof & 227 & 396 & 9 & 3.96 \\ 
        21 & W\_newsp\_brdsht\_nat\_arts & 764 & 3470 & 147 & 19.24 \\ 
        22 & W\_newsp\_brdsht\_nat\_commerce & 792 & 484 & 100 & 12.63 \\ 
        23 & W\_newsp\_brdsht\_nat\_editorial & 212 & 909 & 23 & 10.85 \\ 
        24 & W\_newsp\_brdsht\_nat\_misc & 1544 & 896 & 45 & 2.91 \\ 
        25 & W\_newsp\_brdsht\_nat\_report & 854 & 925 & 73 & 8.55 \\ 
        26 & W\_newsp\_brdsht\_nat\_science & 235 & 284 & 6 & 2.55 \\ 
        27 & W\_newsp\_brdsht\_nat\_social & 881 & 259 & 6 & 0.68 \\ 
        28 & W\_newsp\_brdsht\_nat\_sports & 639 & 503 & 97 & 15.18 \\ 
        29 & W\_newsp\_other\_arts & 216 & 1345 & 49 & 22.69 \\ 
        30 & W\_newsp\_other\_commerce &  91 & 466 & 18 & 19.78 \\ 
        31 & W\_newsp\_other\_report & 756 & 933 & 52 & 6.88 \\ 
        32 & W\_newsp\_other\_science & 327 & 196 & 3 & 0.92 \\ 
        33 & W\_newsp\_other\_social & 904 & 705 & 41 & 4.54 \\ 
        34 & W\_newsp\_other\_sports &  26 & 540 & 4 & 15.38 \\ 
        35 & W\_newsp\_tabloid &  45 & 482 & 2 & 4.44 \\ 
        36 & W\_news\_script & 479 & 121 & 3 & 0.63 \\ 
        37 & W\_non\_ac\_humanities\_arts & 1610 & 1665 & 193 & 11.99 \\ 
        38 & W\_non\_ac\_medicine & 384 & 1605 & 142 & 36.98 \\ 
        39 & W\_non\_ac\_nat\_science & 1438 & 1534 & 258 & 17.94 \\ 
        40 & W\_non\_ac\_polit\_law\_edu & 2891 & 1428 & 463 & 16.02 \\ 
        41 & W\_non\_ac\_soc\_science & 2210 & 1516 & 134 & 6.06 \\ 
        42 & W\_non\_ac\_tech\_engin & 2578 & 807 & 513 & 19.90 \\ 
        43 & W\_pop\_lore & 3371 & 6918 & 662 & 19.64 \\ 
        44 & W\_religion & 838 & 1942 & 400 & 47.73 \\ 
        \hline
    \end{tabular}


    \caption{Document counts by genre for prototypes and downloaded documents.}
    \label{table:evaluation:retrieval:words-dist}
\end{table}


\begin{figure}[Ht]
    % ./analysis/
    \centering
    \includegraphics[width=0.8\textwidth]{evaluation/retrieval-genres-corr}
    \caption{Prototype genres frequencies and resulting downloaded document frequencies.  Shaded area indicates 'over-downloaded' categories.}
    \label{fig:evaluation:retrieval:genres-corr}
\end{figure}


\begin{figure}[Ht]
    % ./analysis/
    \centering
    \includegraphics[width=0.8\textwidth]{evaluation/retrieval-genres-diffdist}
    \caption{The distribution of differences in genre frequency between prototype and downloaded documents.}
    \label{fig:evaluation:retrieval:genres-diffdist}
\end{figure}



Genre was the only parameter explicitly controlled-for by the search engine retrieval method.  Precise counts for each of the prototype and retrieved genres are provided in Table~\ref{table:evaluation:retrieval:words-dist}.  

Figure~\ref{fig:evaluation:retrieval:genres-corr} shows the prototype genre frequency plotted against the downloaded.  When selecting a full corpus using the search engine lookup method, differences between the prototype and downloaded document classes should be minimal --- when selecting a supercorpus, it is desirable only that they fall into the shaded region of the plot.  The evidence here is not compelling for oversampling: just $19$ of the $44$ categories have been "over-sampled".

Evidence for the "directedness" of genre-based retrieval using ngrams is slightly stronger.  Rank correlation between prototype and retrieved documents is $0.45$, and a Mann-Whitney test is not significant ($U = 786.5; p \approx 0.13 > 0.05$), indicating that errors in retrieval are centred around the prototype's frequencies.  This is reinforced by inspecting the density plot for differences between category frequencies, shown in Figure~\ref{fig:evaluation:retrieval:genres-diffdist}

% --

Treating the searching process as a classifier allows us to examine the expectation of retrieving a genre successfully.  $7355$ documents were returned with the desired genre, leading to a true positive rate of $13.2\%$.  This implies retrieval results close to the worst case of one relevant document per search, and explains the low correlation between predicted and resultant class types.

Per-class retrieval rates are displayed in the right-most column of Table~\ref{table:evaluation:retrieval:words-dist}.  There is no correlation between the size of a genre and its true positive rate ($\rho = 0.84$), and inspection of the table indicates that performance is better predicted by the `linguistic specificity' of a given category (with religion, medicine and tech leading the recall rate statistics).


\til{ It's possible to investigate interactions between dimensions or genres here, but I think it might not add much to the discussion.  Perhaps might give a handle on some of the bing behaviour, but also might just reflect the classifier}

% -- 

Finally, keywords were computed relative to the original input corpus using log likelihood.  These offer insight into the specific biases of the data source and a handle on qualitative differences between sampling processes.  

% This is a comparison of the supercorpus against the input used to generate it.
% Should a 'fake' BNC be generated by rejection sampling the supercorpus too?  The comparison would arguably be fairer.


The top 200 keywords were inspected for the whole supercorpus relative to the seed corpus. Overrepresented tokens were scored separately to under-represented ones in accordance with \cite{}\td{cite Rayson et al}, and the lists were free-coded for prominent themes.

\til{Include in appendix?}

Overrepresented terms may be grouped into a number of obvious salient categories.  The first of these is technical terms: those that are side-effects of the tokeniser or otherwise are related specifically to the web.  This includes features such as protocols (`http'), and features of URLs (`co', `org'), as well as terms such as `email' and `posted' which have simply become more prevalent since the BNC was produced.  It's difficult to say whether or not a more recent seed corpus would eliminate most of these from the keyword list.

Temporal features dominate the rest of the list, with \td{N} terms related to date or proper nouns (`Obama', `Nike').  Dates feature prominently: 2001-2012 are all mentioned, along with `1991' and `1988'.  Genetics is also mentioned (`gene', `mutation'), along with education (`university', `pupils') and government (`government', `council').

The \td{N} remaining terms are pronouns or function words.  These, along with the large number of quantities and references to time/date imply consistency with other WaC studies, that is, an overrepresentation of news online.

This central theme is interesting to contrast against the BNC, which contains a large quantity of news material already, indicating that news genres are even more prevalent online, or that online news is ``newsier'' than the bulk of the BNC data.

Features that are significantly under-represented prodominantly fall into the category of proper nouns, and these are mostly political and from the UK (`kinnock', `gummer', `heseltine').  Since the search engine was instructed to search the `GB' marketplace, many of these are most likely explained by temporal effects --- the prevalence of these keywords also points at the strength of news within the BNC, along with places and organisations that featured in the news at the time (`TUC', `Maastricht', `DTI').

There is also a strong theme of legal terms (`offeror', `solicitor', `arbitrage'), which points at a clear under-representation of legal documents returned by search engines.  This is likely a technical limitation of the retriever, which avoids \.pdf files.

Some technical terms are also presented in the keyword list due to the speed of progress in that field: `80486' and `microcomputer' both showing their age.

Finally, there are a number of gastric terms (`oesophegeal', `pylori', `dueodenal'), which point to a very specific set of documents missing in the returned data.  These terms are from an academic context that, like legal documents, is under-represented for technical reasons, however, it is also likely that the specificity of this category indicates an over-representation within the BNC to some degree.


% TODO: Comparison of keywords from data (should be no sig. differences!)

% TODO: possibly also generate a false BNC from the supercorpus and analyse its keywords to see the differences.











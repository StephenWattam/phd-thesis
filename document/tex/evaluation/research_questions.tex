

The approach used in this evaluation is one of treating the system to be tested as a method of replicating the features of a corpus, passing statistical properties of the input corpus through to the output.

The success of this method is dependent on the structure of the input corpus, and the structure of the population new documents are drawn from, and as such this evaluation is just a single datapoint in the space of possible research questions.  If testing with another seed corpus, one must change the search strategy modules to fit, changing the overall results of the tool.  Essentially, this is a manifestation of `garbage in, garbage out': moving search strategies into the implementation merely makes this more explicit.  For that reason, this evaluation uses an extremely popular corpus, the BNC, to ensure some degree of transferrability.

One method of testing this is to see the output corpus as a model of the input, given certain assumptions that include the selection of metadata types and search strategies: providing those assumptions hold, much of the variation in the input corpus ought to be explained by variation in the output.  Measuring differences between the two allows us to identify specific areas of poor fit, which can then be improved either by altering the pluggable modules to fit the ground truth.

The purpose of this evaluation is to test both the search modules used for the particular corpus given, and the overall method: if no set of reasonable assumptions can be found, this is an indication either that the population of documents online is fundamentally different to that of the input corpora tested, or the method presented here is incapable \textsl{by design} of identifying appropriate documents.  Differentiating between these two is not possible quantitatively.


This evaluation will centre around replication of a large general-purpose input corpus that is sampled at the document level (that is, each datapoint is a document rather than a single word or sentence).  This design has been chosen as it represents the most common current design and because metadata at the document level are consistently both meaningful to humans and well researched.  It is also similar to the approach taken by existing search methods for building corpora, making results indicative of search engine behaviour.

% --

Research questions surrounding this method run to:

\begin{itemizeTitle}

    \item[Components] How valid are the assumptions of each of the retrieval methods and heuristics selected?

    \item[Overall Application] For general-purpose input corpora, to what extent (and in what manner) does the output corpus resemble the input `seed' corpus?

  %  \item[Feature Correlation] Do the differences between two input corpora match those between two corpora built using them as seeds?

    \item[Residual Variance] What consistent features remain variable between the output and input corpora, i.e.\ what data cannot be sought online using the heuristics/search methods selected.

\end{itemizeTitle}

The accuracy of each heuristic component is contributory to the excess dispersion in the output corpus.  By design these modules are unambitious, relying on existing methods and tools already tested in the literature, however, their performance upon the data used here will be evaluated in order to better explain sources of error.  As this system remains a proof of concept, the selection of these modules is limited.


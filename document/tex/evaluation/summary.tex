
The past two chapters have specified and tested a generalisation of many web based corpus building systems.  The intent is that this method is split into modules which are easily tested in isolation and which represent separable concerns within the corpus-building process.

This evaluation has focused on a common-case of constructing a corpus based on an existing set of metadata, following the stage of building a transferrable profile for the BNC, resampling a set of prototype documents, and then retrieving a corpus using the Bing search engine.  A white-box approach provides some insight into the transferrability and utility of each stage in a real-world context, and the selection of corpus and retrieval methods (in line with many existing methods) provides some transferrability.



The classification accuracy for the parameters chosen in the BNC was variable: the task of classifying genres is a challenge for current machine learning methods, and this seems set to continue, especially where fine-grained categories are used.  This challenge alone implies that the method is better suited to special-purpose corpora, or problems that do not require controlling for genre\footnote{Though doubtless very few such problems exist.}.

Bootstrapping to retrieve prototype documents is predictable, as expected, and indicates that corpus sizes are not intractably large for modestly complex input distributions.  The complexity (and thus the required number of documents) increases massively with the addition of new dimensions, meaning again that simpler designs are better suited to quick retrieval.  Manual simplification of the distribution (such as binning continuous distributions like word counts) offers a way to control this manually whilst managing losses in accuracy.

The final, retrieval, stage is particularly challenging.  The retrieval mechanism here exhibits similar properties to that used in BootCaT, mainly the over-representation of news sources, and this seems unlikely to change for other inputs.  The prototype-based approach, however, offers a mechanism for further investigation into the bias of the retrieval mechanism, and there is significant further work necessary to evaluate different methods.  Investigation of retrieval errors, repeated with different study designs, could eventually be used to build up a picture of search engines' suitability for a given task.

For the BNC-based task outlined here, the final output corpus was largely a suitable supercorpus.  The next stage for any user of this corpus would be to discard any documents with more error than is deemed damaging for a given study design, something I am unable to do here without assuming a large number of theoretical conditions.  Areas under-represented in the retrieved data are largely technical---these could be retrieved by sourcing data from places other than a general-purpose search engine.  %Evidence from Chapter~\ref{sec:personal} implies that these technical subjects are most likely over-represented in the BNC, that is to say that construction of a realistic corpus is unlikely to require them.

Ultimately, this method was able to produce a BNC-like corpus quickly and with minimal end-user interaction, in a manner that is described fully as a single, human-readable file.  This opens up opportunities to repeat sampling runs and produce sample designs without having to rely on existing corpora as a source of `seed' terms, and offers a way to evaluate the corpus construction process with known margins for error.  The externally-defined nature of the corpus definition also relieves users of many issues surrounding dissemination of corpora, easing documentation, licensing, and bandwidth requirements.





%\til{How well does the system work w.r.t. chapter 5 aims and objectives, and how well does this chapter answer its own RQs.}



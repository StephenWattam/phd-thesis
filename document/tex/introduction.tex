
% \dropcap{T}{he}
The history of linguistics, as with most `soft' sciences, is one that illustrates a gradual methodological progression.  This progression has continually moved from the conceptual to the empirical, relying ever more on objective and quantitative methodologies.  During this time, many detractors [cough chomsky cough] were disproven by the valuable insights yielded by the approach, and it is a trend likely to continue as the field of linguistic inquiry becomes increasingly important to governmental, commercial, and consumer interests.



Kuhn's view of scientific revolutions~\cite{kuhn1970structure}
holds that only empirical evidence, well applied, may provide the agreement upon the theoretical framework required to progress a field; as the intricacy and accuracy of theoretical systems evolves, this necessarily requires more accurate data.  Like many of the younger sciences linguistics faces a difficult dilemma: it must have high-quality empirical data in order to further develop widely-agreed-upon theories, yet its ability to establish the quality of samples is directly dependent upon said theoretical systems.
\til{I implicitly posit that linguistics is limited by its data}


Achieving a higher level of validity in corpus studies (and [thus] a higher level of utility for NLP systems) requires the ability to assess and improve upon existing resources without recourse to a single as-yet-undeveloped theoretical system.
One way of achieving this is iterative improvement of shared samples (indeed, if we are to solve the dilemma mentioned above this is a requisite part of evaluating any theoretical progression), something that has been advocated both by linguists and those who sample similarly awkward data in other fields.


Current corpus resources are often fashioned around a small subset of exemplar corpora---these have become a de-facto standard, and are thus often used as inspiration for new corpus designs.  This approach, whilst ensuring compatbility and interoperability, necessarily restricts the availability of revolution-causing data.
\til{This relates to the Kuhnian view that people agree on the theory and so plateau with the same data/methods}


The easy availability of data online, coupled with the increasing ubiquity of technology in everyday life, offers a solution to many of the problems that shaped these examplars.  Though this has, to some extent, brought corpus construction more in line with a conventional scientific model (where each researcher would build a sample tailored to test his specific hypothesis), many such methods have merely formed their own widely-used corpora, and these opportunities have not yet been fully exploited.


Corpora constructed using these methods offer a way to depart from the traditional view of validity, yet this raises a significant epistemological issue: without comparison to existing standards, how may one ensure quality?
%In order to answer this, I return to the original intent of sampling in the general case.


The design decisions that shaped existing corpora were largely made in a non-digital era.  They contain abrogations of sampling theory that represent compromises between the ideal method and the original data format, as well as social and economic limitations.  Many of these limitations have become conventionalised, and in many cases damage the inferential power of studies, and the accuracy of systems built upon their findings.
\til{i.e. The current practice of basing web corpora on their structure is likely to be lower quality than a first-principles approach.}


I posit that the current practice of basing `modern' web corpora on their traditional counterparts discards the main value of the method---that it is \textit{not} limited by the practical concerns of the original designs.

% ----
\til{after this I forget which tense to write in, and it sort of becomes signposting without the signposts.}

To illustrate this, I set about sampling from first principles, under the reasoning that an ideal sample is theory-neutral. \td{uhh, well, ish.  Probably remove this}


The first stage in this is to identify the limitations of traditional corpus design, and assess the extent to which these pragmatic concerns apply to WaC methods.  This process is again helped significantly by the availability of technology---where once one would have to carry bulky recorders and perform extensive manual transcription, much language use is natively digital, or may be processed from simple records.


Having established methods for assessing the ground truth of a corpus, the limitations of any new sampling technique must be identified, and the impact these have upon the sample itself must be assessed.
\til{This takes the form of a review of other literature, and the DA study.}


A sampling strategy is then devised that may mitigate these limitations, whilst ensuring that we remain as closely tied to the original theory as possible.  This takes the form of a series of tools which are able to build samples in response to a particular specification (one that is stated in theoretical terms familiar to those requiring the sample).


Evaluation of resultant samples relates once more to the problem of establishing a ground truth.  Since the construction of a new sampling method is predicated upon the unacceptable bias of traditional corpora, simple comparison is rendered meaningless---the sources of bias affecting the newly constructed corpus are unlikely to match those from the conventional one, and it is thus not possible to simply assess the change in magnitude thereof.  The only available resource for establishing validity in this context is the `ground truth' of human understanding.  

\til{i.e. we can't compare size of bias, we have to compare type too, like p values and effect sizes}




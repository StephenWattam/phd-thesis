
% \dropcap{T}{he}

Kuhn's view of scientific revolutions\cite{kuhn1970structure}
holds that only empirical evidence, well applied, may provide the theoretical framework required to progress a field; as the intricacy of theoretical systems evolves, this necessarily requires more accurate data.  Like many of the younger social sciences, linguistics faces a difficult dilemma: it must have high-quality empirical data in order to further develop widely-agreed-upon theories, yet its ability to establish the quality of samples is directly dependent upon said theories.

Current corpus resources are often fashioned around a small subset of exemplar corpora---these have become de-facto standards, and their sample designs are heavily re-used.
Whilst this approach ensures compatibility and interoperability, it also leaves many sample design decisions unspoken or assumed.

Many of these examplar corpora were built in the days before widespread use of technology such as the internet, before digital document creation became popular.  As a result they are structured around designs intended to minimise practical issues surrounding retrieval, coding, and transcription that may not apply in a modern context.  These challenges had the effect of prying corpus designs from their ideal statistical principles and into more expert-designed forms\cite{atkins1992corpus}.

This thesis presents three explorations of this gap: identifying methods and techniques for sampling in unconventional ways, and assessing their worth within the context of existing general-purpose samples for linguistic research.  For each, a generalisable method and supporting tools are produced, in order to make these efforts accessible to a wider linguistic audience.

At its core is the notion of representativeness, the central argument being that the key to improving the validity of corpus-based scientific research (in linguistics or NLP), is the improvement of the sampling method.

Research questions motivating this work seek to inform and extend current practice:
\begin{itemize}
    \item How can existing sampling theory be used to extend current general-purpose corpus construction techniques?
    \item How can these sample designs contribute to existing general-purpose corpus construction aims?
    \item How can WaC methods be used to simplify the corpus construction process in a manner agnostic of sample design?
    \item Can WaC methods be used to mitigate existing challenges in general-purpose corpus design?
\end{itemize}



\section*{Background\hfill{}Chapter~\ref{sec:litreview}}
One significant issue, which remains a topic for debate, is what goals linguistic samples should have.  As the focus in this thesis is on larger, general-purpose corpora, this requires some understanding of common use, and existing practice.

This chapter begins by reviewing the history of general-purpose corpus building, with a view to constructing a working definition of a corpus using existing terminology from corpus linguistics.  These designs are then compared to a number of relevant `ideal' sample designs informed by statistical sampling theory, and their similarities and limitations discussed.  The closing portion of the review forms a brief introduction to new technologies available, particularly the Web-as-Corpus paradigm.


\section*{Document Attrition\hfill{}Chapter~\ref{sec:longitudinal}}
As corpora involving web data become more prevalent, the need to understand the temporal properties of web publishing become increasingly important to the external validity of any scientific claims made using them.  Many others have already observed these changes, though with a focus on technical, rather than linguistic properties.

This chapter presents a motivating investigation into sources of link rot (the tendency for resources online to become unavailable over time) within corpora distributed as lists of URLs.  It finds that many corpora suffer significant loss when retrieved using naive methods, across a timescale likely to affect users of current corpora (some widely-used corpora are now decades old).

This motivates long-term and in-depth analyses, neither of which I am well placed to cover in this thesis: however, a comprehensive tool was developed in order to perform longitudinal sampling of web data.  This tool offers a mechanism for investigating properties as yet unaddressed by the literature, such as changes through time that do not affect availability (such as the editing of articles according to political whim), and network-level properties (such as the response time and location of hosting providers).

\paragraph{Contributions:}
\begin{itemize}
    \item A minor addition to existing literature on link rot, particularly in linguistic corpora.
    \item A publicly-accessible tool for longitudinal analysis of web resources, at a resolution and scale not possible with current software.
\end{itemize}

\section*{Personal Corpora\hfill{}Chapter~\ref{sec:personal}}
The issue of representativeness is examined here with respect to an unconventional research question: how representative is a general-purpose corpus of just one speaker's experience of a language?

This question is valuable primarily because it inverts the sample design commonly used for larger corpora, which cover many people in many contexts but with a low resolution for each individual.
Inverting the standard design allows for detection of a number of difficult-to-determine properties, such as what proportions of each genre are used at given times of day, or in different contexts.

At the same time, many of the challenges faced when executing such a sample design are novel, or have only been seen outside corpus sampling.  Here, too, technological benefits such as the ubiquity of smartphones and speed of logging software yield results that would have been impossible just a few years ago.

This section ends with a discussion of the potential uses of such a sample, both of itself (for use in special-purpose corpus construction) and at a wider scale to inform the sampling policy of future efforts (to identify missing components from the .

\paragraph{Contributions:}
\begin{itemize}
    \item A case-study relating general-purpose corpora to one person's idiolect; the practical and theoretical findings therefrom.
    \item A semi-automated method for producing personal corpora, including supporting software.
\end{itemize}


\section*{Profile-Guided Corpora\hfill{}Chapters~\ref{sec:rebuilding},~\ref{sec:evaluation}}
A central theme of this thesis is the research-question-oriented nature of any sample.  Without a clear and objective research question, any claim to external validity is subject to the judgement of the reader.

These two chapters detail the design and implementation of a method that is designed to unambiguously specify the important variables in a research design, and then retrieve corpora based upon these designs.  This serves to solve a number of significant practical issues with conventional corpus design, and takes sampling from the web in a new direction: away from a corpus of the web itself, and towards a general-purpose source of documents for all purposes.

This is accomplished by the encoding of procedures based on expert opinion into pluggable modules, that may be used to construct a corpus' sample design according to the whim of the user.  These designs may then be distributed free of any legal restriction and re-used at will.  The automated nature of the corpus retrieval process also serves to vastly speed up and simplify corpus construction.

Because there is no reliance on a gold-standard corpus, evaluation for this method is particularly difficult.  This thesis relies on two mechanisms: a white-box examination of each of the stages of the corpus building process is presented, and its rationality confirmed.  A black-box examination against the source corpus is then performed, in order to identify any subjective unintended bias in the result.  Though the results of this evaluation are difficult to generalise to all corpus designs, these indicate a practical level of success for the common design used.

\paragraph{Contributions:}
\begin{itemize}
    \item A method for quantitatively producing, reweighting, and describing corpora using a set of metadata distributions using bootstrapping.
    \item Software tools to summarise corpora into freely-reusable profiles.
    \item Software tools to construct corpora based upon said profiles, using fully-automated and semi-automated methods.
    \item A mechanism to mitigate existing ethical and legal issues surrounding corpus distribution.
\end{itemize}


\section*{Conclusions\hfill{}Chapter~\ref{sec:conclusions}}
This thesis concludes with a review of the methods presented, and an examination of the contexts in which they may be used.  The core themes of the thesis are reviewed, before a discussion of major items of further work.



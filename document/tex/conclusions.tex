
% Summary

This thesis has examined the problem of improving corpus sampling from a number of perspectives.  This chapter will serve to summarise these, before offering an overview of significant areas for further research.

Chapter~\ref{sec:longitudinal} presents a motivating study of link rot in open-source corpora, finding large variations in document half-life using a URL-seeking method.  This illustrates that temporal effects have a significant impact on the availability of data, and that such effects vary according to source.  Much literature exists that has related these to layout features (such as the role of navigation and landing pages), yet the impact this has upon linguistic content is less well understood.

In order to make such investigation possible, a cohort sampling approach is necessary, performed at a high resolution over a long period of time.  The LWAC tool automates this process, providing a mechanism to construct large-scale corpora that are indexed by time as well as URL.  Key to the utility of such a tool is its performance, which was shown to be adequate to construct corpora in the millions of documents, sampled daily.

This wealth of data enables analysis not only of `simple' availability, but also changes through time due to editorial processes or site redesigns.  This may allow sociolinguistic analysis surrounding specific events, or a more general picture of long-term language change through time.  It is hoped that the latter of these can be used as auxiliary data to inform stratification of future general-purpose corpora (or at least their web-based components).

% ---
Chapter~\ref{sec:personal} presents a case study detailing a short-term linguistic census of a single subject.  The sample design used therein is a `narrow and deep' design, orthogonal to the 'broad but shallow' coverage of the large national corpora.  This makes it particularly useful for judging rationally how well such data apply to individuals, and offers a way to retrieve metadata not found using conventional data gathering methods.

Such a detailed sample also provides a mechanism for reasoning about problems of corpus size: the data observed in just two weeks for a single subject yielded almost a million words.  Assuming significant inter-person variation for a given linguistic feature, this implies that a population of ~60 million people cannot be well represented by a corpus of just a few million words.  Quantitative knowledge of inter-person variation is needed to operationalise this, but such data could be gathered using some of the automation methods presented.

Such methods were largely taken from lifelogging, a field chosen due to its focus on unobtrusive, best-effort, and wide-ranging sampling techniques.  These proved to be significantly more troublesome than expected: though automation was assistive, much manual correction was necessary to operationalise the data, including the need for a human-interest model to correct summary statistics that were gathered at a low resolution.


% ---
Chapters~\ref{sec:rebuilding} and~\ref{sec:evaluation} cover a method for `profiling' a corpus purely based on independent variable specification.  This profile has many potential uses, both as human documentation for a corpus and as a machine-readable descriptor, and constitutes a way to gradually refine corpus designs over time.

This system is one designed to extend and generalise the approach taken by BootCaT in order to make its selection criteria easier to operationalise.  It is designed to be modular, allowing those building a corpus to re-use algorithms and taxonomies in a simple manner: essentially, the aim is that the tool should be working using the same concepts and distinctions as the desired sample design.

Monte Carlo techniques were used as a method for operationalising the profile, something that was shown to achieve convergence to the desired distribution within a tractable period for a BNC-like corpus of written text.  The classifiers required to identify documents for final selection were shown to be workable, but their performance was borderline for the more complex dimensions such as genre---this is in part due to the ambiguity in the test data's taxonomy, and in part due to the difficulty of large-scale classification tasks.  Such issues are of wide utility, however, implying that improvement in this area will not require specific study.

Retrieval of documents according to the prototypes was shown to be possibly the biggest challenge.  This is in part due to the classic corpus sampling problems of being unable to locate documents in a uniform and reliable manner, and in part due to the biases seen online.  A number of methods were presented to work around these issues, such as manual selection or use of manually-curated web indices, and evaluation of these remains as further work.




% --------
\paragraph{}
To revisit the problem of representativeness, this thesis has taken the view that any such question must be framed in terms of a larger study design.  This is not to say that general-purpose corpora are impossible to create: sampling larger populations is such a challenging and resource-intensive endeavour that it will most likely always have to be offloaded onto a third party.

The aim, then, is to provide a general-purpose corpus that is uniformly representative for a wide, and well-specified, set of research questions.  This corpus should be sampled using random sampling techniques, and this thesis' tools have been designed to inform stratification efforts with this in mind.  Such a corpus should also be unambiguously and comprehensively documented: assumptions should be made explicit in corpus documentation so that they may be compared against those made by researchers, and the limitations of the intended use of a corpus should be stated in quantitative terms.

The contributions within this thesis have been targeted to provide some insight into the underlying population that is not possible using current techniques.  I do not suppose that these alternative sampling designs will be used in their own right (though there is utility in this for some), but that the perspective they yield may be used to inform and improve upon existing corpus retrieval methods.


% Kilgarriff \& Grefenstette's issue with web corpora\cite[p. 343]{kilgarriff2003introduction}:

% \begin{quote}
% The Web is not representative of anything else. But neither are other corpora, in any well-understood sense.
% \end{quote}












% Further Work
\section{Further Work}
As much of this thesis has been exploratory, there is a wealth of further work necessary to maximise the value therein.  Whereas each chapter contains numerous notes in context, this is a high-level summary of the major avenues of inquiry.


% ====================================================================================
% \subsection{Further Work}
% Currently, workers in LWAC do not evaluate JavaScript or build a DOM (though they do send and receive cookies) --- increasingly these features are used by websites to display content, and extending workers to handle them broadens the set of pages it's possible to retrieve meaningfully.  Such an extension is expected to incur a heavy performance penalty, however, as it massively increases the work done by each client during downloads, and complicates the resulting data.

% The field of survival analysis offers a large number of techniques for studying the change in a cohort over time, however, this is largely focused on the existence of simple regressor variables.  This is suitable for technical properties such as those focused on by search engine designers, but less approproiate for digesting large quantities of text.  Techniques for time-series analysis of large corpora are developing, but are usually not based around a cohort-based design.

% The distributed nature of LWAC makes it possible to study geographical effects --- this is something that would require only modest outlay as virtualised infrastructure becomes cheaper.


% Further sampling and analysis is necessary to confirm the issues highlighted above.  This paper comprises a preliminary look at data sampled in a longitudinal study, which will go on to relate the influences of extrinsic document features (which may be used to inform sampling strategies) to their linguistic content.  

% This will involve, primarily, identifying the extent to which document attrition applies bias on linguistic content, rather than technical features, and how this varies through the sampling period.  Issues of particular interest include:

%





\subsection{Large-Scale Longitudinal Document Attrition Studies}
A large-scale longitudinal sample of WaC sources is one obvious extension to the work in Chapter~\ref{sec:longitudinal}\footnote{Indeed, one was underway, but due to technical failures the results were not ready for publication here.}.

Such a study would be able to answer questions not only on the rate of decay (as has been addressed elsewhere), but also the shape of the decay curve, hitherto presumed to be exponential for most types of data.

Further to this, there are many web page changes that do not constitute `link rot' in that the original information may still be present, but are still interesting for various linguistic and social questions, for example how often documents are changed in response to news events, or have their boilerplate changed in order to affect the context in which documents are presented.

Ultimately, developing a linguistic understanding of change through time will require significant work, however, a single large sample could provide useful information to those seeking to use large existing web or monitor corpora.

\subsection{LWAC Distribution}
LWAC's distributed design lends it particular powers in distinguishing web access issues from a geographical perspective.  With the low price of virtual machine rent, a network of LWAC clients could easily be constructed to access a number of websites from differing locations.

Such a configuration would reveal geographical changes made by site administrators for reasons of editorial control, censorship, and interference by service providers (Phorm\cite{clayton2008phorm}, for example).  This would open an avenue for investigation of a number of social factors, particularly as the (time-series) results could be compared and correlated with many real-world regressors, such as the incidence of political events.  Such approaches are already being widely used in social media analysis\cite{achrekar2011predicting,bollen2011twitter}, though the high number of societal covariates renders such studies difficult to validate.

\subsection{Slimmed-down Personal Corpora}
A natural progression of the work in Chapter~\ref{sec:personal} is the repetition with more subjects.  This is troublesome in part due to the in-depth coverage of language sampled, something that is likely to be less than critical for many research questions (such as those primarily concerned with `standard' day-to-day interactions, or even simply those using a computer).

There are many mechanisms which could be used to establish a provisional model of demographic linguistic variation based on partial sampling of each subject, perhaps elective using smartphone technology, down to the level of questionnaires.  This dataset could be added to gradually, and used to guide large diachronic sampling efforts as a form of auxiliary data.

This would require work to formalise and proceduralise the data gathering methods presented, in order to discount those with the least `effort efficiency' and maximise automation.  The continuing ubiquity of smartphone technology offers a vehicle through which to distribute such methods and collect resulting data.

Operationalising the results of such sampling is also an open question, though such work would largely require the application of existing stratification and re-weighting techniques.  This would allow an experimenter to select a small sample demographic, and then use a general-purpose corpus to augment it with larger quantities of linguistic information in a representative manner.  In order for these methods to be useful at a large scale, they should be informed by further work on power analysis, which is currently in its infancy in corpus linguistics.



\subsection{Improved ImputeCaT Modules}
The performance of the ImputeCaT corpus profile retrieval tool is currently constrained primarily by two factors:

\begin{itemize}
    \item The accuracy of classifiers used to identify document distributions and class membership during profile generation and candidate document selection;
    \item The specificity of retrieval methods.
\end{itemize}

The former of these is the subject of much ongoing research, and can be expected to progress gradually due to its utility in many areas.  The design of ImputeCaT is such that this may be leveraged easily to provide continuing accuracy improvements.

The latter is more challenging: though ultimately corpora can be retrieved manually (incurring only the error seen in inter-annotator scores for a given taxonomy\cite{sharoffs2015}), the scalability of automated corpus construction is born largely of its ability to retrieve documents online in an unsupervised manner.  This is currently limited by the search-engine-based retrieval mechanism demonstrated here.

Many options exist for this, such as the use of web directories, supercorpora, semi-supervised topic modelling, or crowdsourcing.  Each of these should be evaluated relative to typical corpus construction tasks in order to determine its suitability for given study designs: indeed, it is likely that some exhibit biases such that they are unable to replicate some samples.


\subsection{Bias Estimation}
Finally, the convergeance-based design of profile-guided retrieval offers the first few avenues through which to explore the bias of sources.  Currently, bias in retrieval methods is conflated with classification error, something that is also far from negligable.

The approach of seeking a series of known document properties, however, leads to the obvious method of simply measuring how `difficult'\footnote{Ideally measuring difficulty for a typical human.} it is to retrieve a document given certain parameters.  Differences between this difficulty-of-retrieval metric and an equivalent measured based on human performance should yield biases in online sources.

Such a measure would be highly dependent on many unstable variables, however, such as the retrieval methods used, sample design, and temporal/contextual factors.  Nonetheless, this offers a real-world take on representativeness without reliance on variance and homogeniety measures.

As ever, these methods are dependent upon finding a human analogue task that acts as a well-agreed-upon baseline: ultimately, without such a research question, the question of representativeness is unanswerable.




\begin{center}
\vfill
{\fontfamily{pzc}\selectfont C'est fini!}
\vfill
\end{center}




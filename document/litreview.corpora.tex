

The field of linguistics is one concerned with description and formailisation of a particularly ethereal social concept.  The paucity of philosophical agreement upon the nature of language has led to many different approaches being taken through the years, many of which have accomplished great things in advancing our capacity to reason about, and derive conclusions from, language.

Until recently, one notable property of linguistics relative to other social sciences was its tendency to use experimentally-derived or elicited data.  These sources of data yielded many theories of grammar, syntax and semantics that persist as useful frameworks, though their relationship to the 'ground truth' of real language use is, in some cases, questionable.

The collection of large volumes of real-world language is one method of increasing the empiricism of linguistic inquiry, and as such has been followed as a parallel stream of inquiry for a long time: [mention 1870's corpus studies, 1950's inquiries].  Before the advent of computers, however, the process of gathering and analysing this data was inhibitively expensive and time-consuming, rendering it out of the grasp of many studies.

The introduction of programmable computing machinery, along with fast electronic storage, opened possibilties for large-scale analysis of text.  This yielded the modern form of a corpus seen today: a large, machine-readable, annotated collection of texts sampled in order to represent some population of real-world language use.

Corpora may be split into two (rather poorly-defined) categories: the former of these is the general-purpose corpus, designed to represent a whole language (or much of one) such that any results can be generalised to most situations and subjects.  This 'style' of corpus is built so as to be useful to many research questions and researchers, in part to solve practical issues surrounding sampling.  The latter type is the special-purpose corpus, which is designed to represent a restricted context.  These special-purpose corpora may be selected according to demographic or linguistic properties, and are typically much smaller.  Because of this, they are often built for a given study, often by re-sampling a general-purpose corpus.



% --

% 
% 
% 
% The use of corpora for linguistic analysis is a long one---one justification for this is that the alternative to using some kind of corpus is either to manually seek evidence for a linguistic feature (something that very easily leads to pseudoscientific, unfalsifiable theories), or to inspect the "idea" of language that a native speaker (or speakers) posesses.
% 
% Whilst, doubtless, much good work has been done using these alternatives, they lack the objective and empirical epistemological possibilities that define the scientific process.  Examination of one's idea of language is likely to be influenced by the knowledge of a linguist, and seeking evidence for a theory in a language with unknown or poorly-defined bounds is likely to yield it regardless of the reality. % when observed from other contexts
% 
% Corpus methods, then, free linguistics from the alchemy of human understanding, providing a convenient empirical truth against which we may assess hypotheses.
% 
% The value of any inferences evidenced by this empiricism is, however, limited to the extent to which a corpus is associated to the language upon which we wish to comment.  It is this requirement that has raised most objections, for the concept of language is little understood and poorly defined even where people claim to understand it.  Regardless of controversies over form, it is accepted that any corpus representative of useful portions of a language must be very large indeed (though the definition of 'large' is also debated!).
% 
% Until recently, the task of gathering, processing, and inspecting large volumes of text was arduous enough to prevent its widespread appeal: some efforts were made before the era of computerisation, %CITE
% , for example, did X etc \til{mention 1870s work, 1950's popularity}..% and there was a time when corpora were rather popular even without the benfits of computation
% 
% The introduction of programmable computing machinery, and (fast) electronic storage, opened the floodgates for easy, large-scale analysis of text.  This brought the second\td{does that mean we're on the third given its lull, or still second?} wave of corpus linguistics... \textsl{~~wavey wavey screen fades into next section like a flashback on 60's TV~~}
% 

% TODO: a chronological history of corpora, [possibly just brown->lob->llc->bnc, p'raps include newswire mentions]

\subsection{A Brief History of Modern Corpora}
The Brown Corpus of Standard American English % CITE
is widely regarded as the first of the modern wave of corpora.  Built in the 60's, Brown's corpus was the first electronic-format general purpose corpus and was roughly one megaword in size.  It contains 500 samples, each roughly 2000 words in size, that were taken to represent a cross-section of works published in the United States in 1961.  The proportions selected, and sizes of samples, were selected in order to trade off pragmatic concerns with the possible kinds of analysis that could be performed at the time.

The `Standard' in its name referred to Kucera and Francis' intent that it become a regular feature across corpus linguistics---this was quickly realised, as Brown became a \textsl{de facto} standard for American English.  In order to maximise the value of comparisons within studies, other general purpose corpora chose to mirror Brown's sampling policies.  

% --- 

The Lancaster-Oslo-Bergen (LOB) corpus was built as a British counterpart to Brown.  %CITE
It uses the same stratification and sampling strategy (with one or two more texts in certain categories) and thus comprises roughly a megaword of British English, as published in 1961.  % See http://khnt.hit.uib.no/icame/manuals/lob/index.htm Table 1 for a table of comparisons
Though some minor differences exist (see table x?, p'raps?).


% ---
\til{ Mention LLC, perhaps other specific corpora}
% http://khnt.hit.uib.no/icame/manuals/londlund/index.htm
%  The corpus consists of 100 texts, each of 5000 words, totalling 500.000 running words of spoken British English. Information about the compilation of the corpus and explanation of the symbols (prosodic, phonetic, etc.
The London-Lund Corpus % CITE
has a greater focus on spoken text, and comes annotated with a number of different markers to indicate intonation, timing and other extra-textual information.  


% --- 
\til{ BoE/COBUILD }

% ---
\til{ BNC, mention how it's become the 'new' de-facto standard, describe sampling and composition in some detail}

% ---
\til{ }

% ---
The rise of electronic communications has led to a reduction in the effort required to gather corpus data.  This has resulted in a great increase in the number of special-purpose corpora built for specific studies.  These corpora are generally more focused than the larger ones built up until the 90's, but some still claim to be general purpose (or at least have a `wide remit').  

% Move? [These corpora are often less widely used, and serve to illustrate one extreme discussed below, that the purpose of a corpus is important to its construction]

In this thesis, UI will be focusing on a specific (though popular) form of these corpora based on `web-as-corpus' (WaC) methods.  Please see section REF for more.




\subsection{What Makes a Corpus?} % 'What makes a corpus'
\til{Define the usage of corpus to be used in the thesis, in terms of properties that are desirable and undesirable from the literature.  This will likely not be the same as those in section 2.2.  This section should focus on a vaguely chronological recantment of the reasons for significant choices when building large and popular corpora, finishing off with what people use them for.  i.e. intent -> newer intent -> reality}

% Redo intro
The precise definition of a corpus is something that has been debated at length within the corpus linguistic community---the purpose of this section is not to define definitively what a corpus shall be, but instead to identify traits that are desirable and useful to the process of scientific inquiry.



In order to establish the important traits of corpora, it is wise to have an understanding of the motivation behind their existence.  Corpus methods are generally posited against two other methods of linguistic investigation: direct elicitation from a language speaker, and directed research into a linguistic feature.  Both of these pose significant scientific challenges---both are reliant on at least the linguist's intuitive view of language (one that could hardly be said to be representative of most language users), and both require the acquisition of data from within its context, something that is especially difficult given the varied and context-dependent complexity of language.

Corpora provide solutions to both of these issues.  In the former case, they provide an objective record of linguistic data that is free from all but the initial builders' linguistic choices (which, in the ideal case, may be documented and provided along with the data itself).  In the latter, they are as portable as any large volume of text, and may be annotated with context sufficient for a given linguistic task.


%------
The modern definition of a corpus has undergone a series of significant refinements thanks in part to the meteoric rise in both ubiquity and power of computing machinery.  Corpora are, with very few exceptions, electronic (with an increasing number documenting texts of electronic origin), multi-modal (covering a wide variety of methods of communication and their linguistic features), and annotated with linguistic data.  

% TODO: Meyer, McEnery and Wilson definitions

A corpus, then, is typically seen to be `a body of text sharing some important property that may be interrogated for some linguistic information'.  This is a fairly general definition, but takes into account the separation often made between haphazard collections of texts (often called \textsl{libraries} or \textsl{archives} with no relevant common features (be they internal or external) from a scientifically useful collection demarked by the boundaries of some homogenous notional entity.

Many authors %CITE
go further by stating that a corpus should be machine-readable, annotated with information useful to linguistic inquiry, built for a specific purpose or methodology, available for use in other studies, finite in size or even stratified to provide multiple possible analysis methods with valid data.  Whilst I do not consider many of these to be requirements for a scientifically useful corpus, many contribute greatly to [x]'s utility due to their alignment with common methodologies and uses---these will be examined in more detail later for their contributions.





% The definition of a corpus is one which has been oft-discussed with little agreement, indeed, as the field of corpus linguistics has progresssed its definition has gradually changed to suit the methodology of the day.  The Brown Corpus of Standard American English %CITE

% Examining the reasons why linguists first chose to use corpus methods reveals a number of important traits.  The first of these should be seen as its reliability---one of the major problems with assessing linguistic features by interrogation or directed research is that is it hard to establish known bounds of variability.  This inherent `fuzziness' in the method of inquiry, coupled with the context-dependent complexity of language itself, makes replication difficult.


% ----
% Brown's unique status afforded it status as a {\sl de facto} standard, and many subsequent corpora were built with similar foci.  Though the design of corpora has moved on significantly since those first efforts, this standard persists in its principles, and Brown's influence may be felt in the building and coding efforts of many general purpose corpora, to the point that many linguists are tempted to define corpora in terms of them. % TODO: move this elsewhere, but it's a good point

% Brown is widely regarded as the first of the modern age of corpora: its scale and electronic format differentiating it from previous collections and allowing automated analysis on a scale never before practical.




\subsubsection{Representativeness and Transferability}
Representativeness is, in effect, the holy grail of corpora.  It is the property that is to be maximised by adjustment of all others, and yet it is also the one that is dependent on enough factors to be poorly defined (by virtue of the difficulty of doing so).

The concept of representativeness is based not only on the definition of a population, but also the properties one wishes to generalise about, and the purpose for which one wishes to do so.  Various users of a corpus may find that, even for general-purpose corpora intending to represent the whole of a language (reference corpora), they produce unrepresentative findings where others' studies have great claims to accuracy. \td{Ahrg, this is terribly articulated}.

The concept of transferability % taken from sociology
is an alternative often applied to qualitative analyses by those in the social sciences.  It describes the likelihood that findings may apply to other members of a given, defined, population, without speculating as to the probability that a member of said population may be suitable for such a comparison.  This concept allows us to find evidence-based theories which hold true for groups specified by prior knowledge of the sample and human judgement of its relationship to the population.

It could be said that, prior to corpus methods, the best linguistics could do was the notion of qualitative transferrability.  Indeed, some may argue that the theoretically infinite population of utterances language makes possible means that any sample is incapable of being representative of language as a whole, rather that we are only able to generalise to observations of language [within finite time periods].

One approach to this is the use of monitor corpora, which grow along with their source material in order to, at any give time, represent the language used until that point.  This adds time as a piece of metadata that may be worked with in language models, though the rise in heterongeny this brings will reduce the power of the corpus when comparing to other, temporally restricted, corpora. \td{issues abound, perhaps discuss earlier/later at length instead}

\til{ Discuss representativeness in a more "linguistic" way including balance and choice of sampling frame.  Perhaps split into \textsl{GP}/\textsl{SP} corpora to discuss at more length.  Refer to web as corpus section for "we also discuss what we're sampling in WaC terms below...}

\til{Also, refer to the books more!}


\subsubsection{Size}
\til{This is the primary driver of the above, no-one really pins this down but survey some corpora and some more modern approaches (Kilgarriff's google-ology comments at the end would be good.  Link to web corpora for extreme bigness and ref to lower sections at end}


\subsubsection{Purpose}
The reason for sampling a given population is a crucial feature of corpora.  Not only does it define the level and type of metadata available (and the arbitrary definitions used therein), it has a large influence on the sampling frame used, and thus the validity of any generalisations made using said data.

\til{examples of purpose.  General/special.  Cover qualitative issues and quantitative issues of parameter estimation from biased statistics.}

Sub and re-sampling of corpora originally acquired for one purpose, though common, is likely to neglect the deep influences purposive or semi-purposive sampling is likely to have had upon the original corpus.  Correcting for these mis-matches may not be possible without extensive auxiliary data, which is often unavailable or impossible to accurately assign to existing texts.



\subsubsection{Normalised Form}
Many authors stipulate that a modern corpus should be electronic.  To generalise this position, they require that it is in some way processable by machines in a linguistically useful way.  This defines the format of not only the basic textual content, but also the availability of metadata at all levels (category/strata, document, word).

Efforts have been made %CITE EAGLES, TEI
to standardise the text type distinctions used in metadata, though there will always be the need to exceed or modify these for certain purposes\td{express arbitrary nature of these, explain the game-theoretic aspect} (see above, REF[purpose]).

To some extent, the form a corpus takes is defined by its content---any meaningful taxonomy is arbitrary and theory-laden---the best we can hope for is neutrality for the purposes of the task for which a corpus is being used (see purpose again...).  Internal text distinctions may be drawn using corpus-driven methods, however, these are often difficult to operationalise, or rely on assignment of names and descriptions that are themselves arbitrary again.  Taxonomic systems used for describing corpora and strata are designed to be maximally `purpose-neutral', and much effort is put into this.

\til{Describe taxonomy defining efforts and perhaps stratification in a way that is more tied to original corpus documentation, esp. stratification w.r.t. brown/brown-inspired corpora}


\subsubsection{Dissemination and Collaboration}
The ability for multiple researchers to access a corpus is one of the main benefits of corpus methods---corpus-based studies may be replicated and compared with absolute certainty of the empirical aspect of the research.

Historically, a number of design decisions have been made when sampling corpora in order to minimise the damage done by legal requirements from text publishers.  One of the most obvious of these is Brown's inclusion of two kiloword samples, rather than whole texts, in order to avoid many of the copyright issues associated with redistributing recently-published work.

Corpora often come bundled with restrictive licensing, something that is doubtless limiting to their scientific value.
\til{Hint at web stuff, open source corpus stuff, other scientific issues.}



\subsubsection{Summary}
\til{Define a phrase that, in a nutshell, defines what I'm using as a definition.}



\subsection{Validity Concerns in Corpus Building}
\til{Contain existing research on various aspects of validity.}
\subsubsection{Categorising Reference Corpora}
\til{Discuss the problem of meaningfully separating parts of a corpus for subsampling and description.}
\begin{itemize}
    \item Comparison to other corpora
    \item Balance, breadth and generalisability (inc. efforts to establish standards for these)
    \item Supcorpora, special-purpose-from-single-purpose
    \item Parallel and comparable corpora
\end{itemize}
\subsubsection{Dissemination}
\til{Descriptions of how to best spread corpora and the importance of sharing.  Cover the single-sample problem, open source corpora, twitter+news redactions, etc}


\subsubsection{Balance}
\til{Critiques of attempts to balance corpora against language (genres, special purpose corpus selection) and against one another (parallel corpora, brown, ae06/be06)}
\subsubsection{Replicability and Reliability}
\til{Linguistic reviews on `scientific issues' that do not centre on issues of text selection.  Include Open source corpora here, if they are not to be in the web section.}









% \til{The review of literature begins with a discussion of the current state of affairs w.r.t. sampling in corpus linguistics: how do people go about acquiring corpora, what their motivation and rationale is, and what people are currently developing in terms of sampling methodology.  I'll attempt to include some historical rationale too here.}
% 
% \subsection{Criticism of Current Data}
% \til{Here I will go on to discuss the failings of current widely-used lexical resources from the standpoint of their common use --- the section will focus on the representativeness of studies based on corpora as a method of commenting on their quality, and it's therefore necessary to draw the distinction between}
% \begin{itemize}
% 	\item General purpose corpora, and;
% 	\item Specialist corpora (and therefore some methods people use to build them).
% \end{itemize}
% 
% 
% 
% \subsection{Current Discussions of Representativeness}
% \til{Here I'll focus on work that is in the same vein as the thesis, i.e. how can we go about improving things?}




\section{Methods for Improved Web Sampling}
Based upon information from [ref: chap:attrition], and the biases/discussion identified at the end of the lit review, derive methods to fix issues identified.  Separate them into the general-purpose/specialist framework again, and their theoretical justification.

% I'm not convinced this is the best structure
\subsection{Bottom up (statistically justified)}
\begin{itemize}
	\item Social Science's methods for balancing data where population, distribution are hard to determine or where data acquisision is hard,
	\item Attempts made by others to compensate in linguistic studies.
\end{itemize}

\subsection{Top down (linguistically justified)}
\begin{itemize}
	\item Personal corpora,
	\item Subject-specific sampling methods (though these don't map well to the web).
\end{itemize}



% Moving from problem to solution again
\section{Tools Based on the Chosen Methods}
A major contribution here is to be a suite of software to allow people to put some of these principles into practice.  The idea is to use this as a separate paper, and merge it in here, so the subsections are quite fluid.

\subsection{Details of Implementation}
Outline the design choices made to avoid each of the issues.  There'll be a focus on having some kind of sampling strategy descriptor because it ties nicely to the next chapter.

\subsection{Evaluation}
Here  I'll attempt to evaluate the quality of sampling.  This will be done by comparison of results from linguistic analyses, such as keywording, as well as in comparison to the findings of search-engine document attrition studies (these will validate the findings to some extent, rather than merely stating that the corpus is different).

The big question here is how I evaluate corpus quality without using a larger (ideally more biased) corpus.  Chances are this method will become a subsection of its very own.
